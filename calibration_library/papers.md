# Calibration Metrics

## 2022
[Mitigating Bias in Calibration Error Estimation](https://arxiv.org/abs/2012.08668) - Rebecca Roelofs, Nicholas Cain, Jonathon Shlens, Michael C. Mozer (AISTATS 2022)

## 2021
[Calibration of Neural Networks using Splines](https://arxiv.org/abs/2006.12800) - Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, Richard Hartley (ICLR 2021)
* top-k calibration

[Revisiting the Calibration of Modern Neural Networks](https://arxiv.org/abs/2106.07998) - Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, Mario Lucic (NeurIPS 2021)

[Soft Calibration Objectives for Neural Networks](https://arxiv.org/abs/2108.00106) - Archit Karandikar, Nicholas Cain, Dustin Tran, Balaji Lakshminarayanan, Jonathon Shlens, Michael C. Mozer, Becca Roelofs (NeurIPS 2021)

[Calibrating Predictions to Decisions: A Novel Approach to Multi-Class Calibration](https://arxiv.org/abs/2107.05719) - Shengjia Zhao, Michael P. Kim, Roshni Sahoo, Tengyu Ma, Stefano Ermon (NeurIPS 2021)

[Uncertainty Toolbox: an Open-Source Library for Assessing, Visualizing, and Improving Uncertainty Quantification](https://arxiv.org/abs/2109.10254) - Youngseog Chung, Ian Char, Han Guo, Jeff Schneider, Willie Neiswanger

## 2020
[Measuring Calibration in Deep Learning](https://arxiv.org/abs/1904.01685) - Jeremy Nixon, Mike Dusenberry, Ghassen Jerfel, Timothy Nguyen, Jeremiah Liu, Linchuan Zhang, Dustin Tran
* SCE, ACE

[Calibrating Deep Neural Networks using Focal Loss](https://arxiv.org/abs/2002.09437) - Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip H.S. Torr, Puneet K. Dokania (NeurIPS 2020)

[Individual Calibration with Randomized Forecasting](https://arxiv.org/abs/2006.10288) - Shengjia Zhao, Tengyu Ma, Stefano Ermon (ICML 2020)

[Mix-n-Match: Ensemble and Compositional Methods for Uncertainty Calibration in Deep Learning](https://arxiv.org/abs/2003.07329) - Jize Zhang, Bhavya Kailkhura, T. Yong-Jin Han (ICML 2020)

## 2019
[Verified Uncertainty Calibration](https://arxiv.org/abs/1909.10155) - Ananya Kumar, Percy Liang, Tengyu Ma (NeurIPS 2019)

[Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration](https://arxiv.org/abs/1910.12656) - Meelis Kull, Miquel Perello-Nieto, Markus Kängsepp, Telmo Silva Filho, Hao Song, Peter Flach (NeurIPS 2019)
* Classwise-ECE

[Evaluating model calibration in classification](https://arxiv.org/abs/1902.06977) - Juozas Vaicenavicius, David Widmann, Carl Andersson, Fredrik Lindsten, Jacob Roll, Thomas B. Schön (AISTATS 2019)

[Calibration tests in multi-class classification: A unifying framework](https://arxiv.org/abs/1910.11385) - David Widmann, Fredrik Lindsten, Dave Zachariah (NeurIPS 2019)

[On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks](https://arxiv.org/abs/1905.11001) - Sunil Thulasidasan, Gopinath Chennupati, Jeff Bilmes, Tanmoy Bhattacharya, Sarah Michalak (NeurIPS 2019)

[Deep Anomaly Detection with Outlier Exposure](https://arxiv.org/abs/1812.04606) - Dan Hendrycks, Mantas Mazeika, Thomas Dietterich (ICLR 2019)
* RMS, MAD and Soft F1 Score

## 2018
[Trainable Calibration Measures for Neural Networks from Kernel Mean Embeddings](https://proceedings.mlr.press/v80/kumar18a.html) - Aviral Kumar, Sunita Sarawagi, Ujjwal Jain (ICML 2018)
* MMCE

## 2017
[On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599) - Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger (ICML 2017)

[Beta calibration: a well-founded and easily implemented improvement on logistic calibration for binary classifiers](https://proceedings.mlr.press/v54/kull17a.html) - Meelis Kull, Telmo Silva Filho, Peter Flach (AIStats 2017)

## 2015
[Obtaining Well Calibrated Probabilities Using Bayesian Binning](https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf) - Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht (AAAI 2015)
* ECE, MCE
