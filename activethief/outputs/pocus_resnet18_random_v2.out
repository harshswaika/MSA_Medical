nohup: ignoring input
[24/03/07 00:39:18] [conf.py:  298]: PyTorch Version: torch=1.11.0, cuda=11.3, cudnn=8200
[24/03/07 00:39:18] [conf.py:  300]: ACTIVE:
  ADDENDUM: 0
  ALPHA: 0.2
  AUGMENT: None
  BETA: 1.0
  BUDGET: 1000
  CUTMIX_PROB: 0.0
  CYCLES: 1
  INITIAL: 900
  LA: False
  METHOD: random
  PRETRAINED_PATH: /home/ankita/scratch/data_msa_medical/ckpts/resnet50-imagenet1k.pth
  TEMP: 1.0
  TRO: 1.0
  USE_PRETRAINED: True
  VAL: 100
AM:
  SGR: 1000
  THRESHOLD: 0.5
CUDNN_BENCHMARK: True
DS_SEED: 123
EDM:
  HASH: /home/harsh_s/scratch/msacopy/exp/hash/alpha.pt
  NUM_MODELS: 5
  PATH1: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T0.pt
  PATH2: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T1.pt
  PATH3: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T2.pt
  PATH4: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T3.pt
  PATH5: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T4.pt
GR:
  EPSILON: 0.1
  SUR_PATH: /home/harsh_s/scratch/msacopy/GRAD2/batch_training/outputs/trained_models/imagenet_cifar10_to_cifar10_surrogate_10epochs.pt
LOG_DEST: resnet18_resnet50_pocus_240307_003918.txt
LOG_TIME: 240307_003918
METHOD_NAME: v2
OUT_DIR: /home/ankita/scratch/data_msa_medical/results_ankita/
PL:
  CLASS_BLNC: 10
  ITERATIONS: 20
  KAPPA_N: 0.005
  KAPPA_P: 0.05
  NO_PROGRESS: False
  NO_UNCERTAINTY: False
  TAU_N: 0.05
  TAU_P: 0.7
  TEMP_NL: 2.0
RNG_SEED: 1
SAVE_DIR: /home/ankita/scratch/data_msa_medical/results_ankita//pocus_resnet18/butterfly_resnet50/SGD/1000_val100/random_v2
THIEF:
  ARCH: resnet50
  DATASET: butterfly
  DATA_ROOT: /home/ankita/scratch/data_msa_medical/Butte
  HARD_LABELS: True
  NUM_TRAIN: 1281167
  SUBSET: 128116
TRAIN:
  BATCH: 16
  EPOCH: 100
  GAMMA: 0.1
  LR: 0.005
  MILESTONES: [20, 40, 60, 80]
  MOMENTUM: 0.9
  OPTIMIZER: SGD
  WDECAY: 0.0005
VICTIM:
  ARCH: resnet18
  DATASET: pocus
  DATA_ROOT: /home/ankita/scratch/data_msa_medical/covid_5_fold
  HEIGHT: 224
  PATH: /home/ankita/scratch/data_msa_medical/victim_models/pocus_resnet18.pth.tar
  PATHIN: /home/harsh_s/scratch/msacopy/ckpts/cifar10-resnet32-inaccurate/checkpoint.pth.tar
  WIDTH: 224
Loaded target dataset of size 476 with 3 classes
target model keys:  122
checkpoint keys:  122

Target model acc = 0.8991596638655462
Val-Acc: 0.8992 Val-Spec: 0.9795 Val-Sens: 0.9224 Class Acc: [0.8343949  0.95402299 0.92241379]
replacing labeled set labels with victim labels
  0%|          | 0/57 [00:00<?, ?it/s]  2%|▏         | 1/57 [00:00<00:33,  1.68it/s]  9%|▉         | 5/57 [00:00<00:06,  7.82it/s] 16%|█▌        | 9/57 [00:01<00:04, 11.32it/s] 23%|██▎       | 13/57 [00:01<00:02, 15.38it/s] 28%|██▊       | 16/57 [00:01<00:02, 18.00it/s] 33%|███▎      | 19/57 [00:01<00:01, 19.39it/s] 39%|███▊      | 22/57 [00:01<00:01, 18.86it/s] 44%|████▍     | 25/57 [00:01<00:01, 18.27it/s] 51%|█████     | 29/57 [00:01<00:01, 19.44it/s] 58%|█████▊    | 33/57 [00:02<00:01, 21.03it/s] 63%|██████▎   | 36/57 [00:02<00:00, 22.77it/s] 68%|██████▊   | 39/57 [00:02<00:00, 22.96it/s] 74%|███████▎  | 42/57 [00:02<00:00, 21.42it/s] 79%|███████▉  | 45/57 [00:02<00:00, 21.02it/s] 84%|████████▍ | 48/57 [00:02<00:00, 17.98it/s] 91%|█████████ | 52/57 [00:03<00:00, 17.91it/s] 98%|█████████▊| 56/57 [00:03<00:00, 18.95it/s]100%|██████████| 57/57 [00:03<00:00, 16.40it/s]
replacing val labels with victim labels
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:00<00:03,  1.97it/s] 57%|█████▋    | 4/7 [00:00<00:00,  7.74it/s]100%|██████████| 7/7 [00:00<00:00,  8.77it/s]
Test set stats: mean/std =  (tensor([-1.2593, -1.2577, -1.2463]), tensor([0.7453, 0.7491, 0.7672]))
Train set stats: mean/std =  (tensor([-1.2243, -1.2400, -1.2166]), tensor([0.8476, 0.8431, 0.8505]))
Val set stats: mean/std =  (tensor([-1.1715, -1.1901, -1.1622]), tensor([0.8641, 0.8598, 0.8667]))
Validation set distribution: 
Number of samples  100
{0: 18, 1: 50, 2: 32}
Labeled set distribution: 
Number of samples  900
{0: 149, 1: 391, 2: 360}
Test set distribution: 
Number of samples  476
{0: 157, 1: 87, 2: 232}
pretrained state:  dict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked'])
Thief model initialized successfully
Initial model on target dataset: acc = 0.1975, agreement = 0.1933, f1 = 0.1590, spec = 0.6066, sens = 0.1595 Class Acc: [0.00636943 0.64367816 0.15948276]
Initial model on validation dataset: acc = 0.5300, agreement = 0.5300, f1 = 0.3449, spec = 0.9118, sens = 0.2812 Class Acc: [0.      0.88    0.28125]
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:04<07:07,  4.32s/it]epoch 0, loss = 10.342658996582031
epoch 1, loss = 4.999632835388184
  2%|▏         | 2/100 [00:09<07:34,  4.64s/it]  3%|▎         | 3/100 [00:12<06:49,  4.22s/it]epoch 2, loss = 4.314974784851074
epoch 3, loss = 3.675807476043701
  4%|▍         | 4/100 [00:17<07:17,  4.56s/it]epoch 4, loss = 3.1313698291778564
  5%|▌         | 5/100 [00:26<09:19,  5.89s/it]Epoch 5: Train acc/f1 = 0.9467 / 0.9300 / 0.9944 / 0.9250/ [0.93959732 0.96930946 0.925     ] 
                Val acc/f1/spec/sens = 0.8900 / 0.8502 / 0.9559 / 0.8438/ [0.66666667 1.         0.84375   ]
                Test acc/f1/spec/sens = 0.4202 / 0.4279 / 0.9303 / 0.2931/ [0.2866242  1.         0.29310345]
epoch 5, loss = 2.8689446449279785
  6%|▌         | 6/100 [00:31<08:52,  5.67s/it]  7%|▋         | 7/100 [00:35<07:45,  5.01s/it]epoch 6, loss = 2.878408670425415
epoch 7, loss = 2.2619426250457764
  8%|▊         | 8/100 [00:39<07:31,  4.91s/it]  9%|▉         | 9/100 [00:43<06:47,  4.48s/it]epoch 8, loss = 2.511268138885498
epoch 9, loss = 2.4078781604766846
 10%|█         | 10/100 [00:52<08:55,  5.95s/it]Epoch 10: Train acc/f1 = 0.9689 / 0.9566 / 0.9852 / 0.9778/ [0.89932886 0.98721228 0.97777778] 
                Val acc/f1/spec/sens = 0.9000 / 0.8567 / 0.9412 / 0.9375/ [0.61111111 0.98       0.9375    ]
                Test acc/f1/spec/sens = 0.5840 / 0.5750 / 0.8320 / 0.5948/ [0.38853503 0.90804598 0.59482759]
 11%|█         | 11/100 [00:56<07:47,  5.25s/it]epoch 10, loss = 1.4857404232025146
epoch 11, loss = 1.1963990926742554
 12%|█▏        | 12/100 [01:01<07:30,  5.12s/it] 13%|█▎        | 13/100 [01:04<06:48,  4.69s/it]epoch 12, loss = 1.121378779411316
epoch 13, loss = 1.24195396900177
 14%|█▍        | 14/100 [01:09<06:41,  4.67s/it]epoch 14, loss = 1.1960307359695435
 15%|█▌        | 15/100 [01:17<08:08,  5.74s/it]Epoch 15: Train acc/f1 = 0.9800 / 0.9712 / 0.9926 / 0.9944/ [0.89932886 0.99744246 0.99444444] 
                Val acc/f1/spec/sens = 0.8800 / 0.8493 / 0.9706 / 0.7812/ [0.83333333 0.96       0.78125   ]
                Test acc/f1/spec/sens = 0.4979 / 0.4591 / 0.9631 / 0.5733/ [0.14012739 0.94252874 0.57327586]
epoch 15, loss = 0.8779000639915466
 16%|█▌        | 16/100 [01:22<07:38,  5.46s/it] 17%|█▋        | 17/100 [01:26<06:52,  4.98s/it]epoch 16, loss = 0.6265683174133301
epoch 17, loss = 0.8651877641677856
 18%|█▊        | 18/100 [01:31<06:42,  4.91s/it] 19%|█▉        | 19/100 [01:34<06:05,  4.51s/it]epoch 18, loss = 0.7555556893348694
epoch 19, loss = 1.003068208694458
 20%|██        | 20/100 [01:43<07:50,  5.88s/it]Epoch 20: Train acc/f1 = 0.9800 / 0.9708 / 0.9722 / 1.0000/ [0.89261745 0.99488491 1.        ] 
                Val acc/f1/spec/sens = 0.8800 / 0.8416 / 0.9265 / 0.9062/ [0.66666667 0.94       0.90625   ]
                Test acc/f1/spec/sens = 0.4244 / 0.4174 / 0.8893 / 0.3922/ [0.4522293  0.45977011 0.39224138]
 21%|██        | 21/100 [01:47<07:00,  5.32s/it]epoch 20, loss = 0.5397329926490784
epoch 21, loss = 0.19084124267101288
 22%|██▏       | 22/100 [01:52<06:46,  5.22s/it] 23%|██▎       | 23/100 [01:56<06:11,  4.82s/it]epoch 22, loss = 0.23821131885051727
epoch 23, loss = 0.19327230751514435
 24%|██▍       | 24/100 [02:01<06:09,  4.87s/it]epoch 24, loss = 0.15888118743896484
 25%|██▌       | 25/100 [02:09<07:21,  5.89s/it]Epoch 25: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4349 / 0.4329 / 0.9180 / 0.4009/ [0.39490446 0.59770115 0.40086207]
epoch 25, loss = 0.148965522646904
 26%|██▌       | 26/100 [02:14<06:51,  5.57s/it] 27%|██▋       | 27/100 [02:18<06:13,  5.12s/it]epoch 26, loss = 0.092892125248909
epoch 27, loss = 0.15515486896038055
 28%|██▊       | 28/100 [02:23<06:01,  5.02s/it] 29%|██▉       | 29/100 [02:27<05:32,  4.68s/it]epoch 28, loss = 0.18554562330245972
epoch 29, loss = 0.18611852824687958
 30%|███       | 30/100 [02:36<07:07,  6.11s/it]Epoch 30: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8800 / 0.8457 / 0.9412 / 0.8750/ [0.72222222 0.94       0.875     ]
                Test acc/f1/spec/sens = 0.4559 / 0.4488 / 0.8811 / 0.4353/ [0.44585987 0.52873563 0.43534483]
 31%|███       | 31/100 [02:40<06:18,  5.48s/it]epoch 30, loss = 0.14768706262111664
epoch 31, loss = 0.08646883815526962
 32%|███▏      | 32/100 [02:45<06:02,  5.32s/it] 33%|███▎      | 33/100 [02:49<05:22,  4.81s/it]epoch 32, loss = 0.061409883201122284
epoch 33, loss = 0.08898767828941345
 34%|███▍      | 34/100 [02:54<05:14,  4.77s/it]epoch 34, loss = 0.06004573032259941
 35%|███▌      | 35/100 [03:02<06:18,  5.82s/it]Epoch 35: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4643 / 0.4609 / 0.9139 / 0.4353/ [0.42038217 0.62068966 0.43534483]
epoch 35, loss = 0.09372294694185257
 36%|███▌      | 36/100 [03:07<05:53,  5.53s/it] 37%|███▋      | 37/100 [03:11<05:15,  5.01s/it]epoch 36, loss = 0.05390701815485954
epoch 37, loss = 0.07883024215698242
 38%|███▊      | 38/100 [03:15<05:04,  4.91s/it] 39%|███▉      | 39/100 [03:19<04:39,  4.58s/it]epoch 38, loss = 0.09716885536909103
epoch 39, loss = 0.06300462037324905
 40%|████      | 40/100 [03:28<05:59,  6.00s/it]Epoch 40: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8584 / 0.9412 / 0.9062/ [0.72222222 0.94       0.90625   ]
                Test acc/f1/spec/sens = 0.4958 / 0.4771 / 0.8770 / 0.5345/ [0.43312102 0.50574713 0.53448276]
 41%|████      | 41/100 [03:32<05:13,  5.32s/it]epoch 40, loss = 0.10185407102108002
epoch 41, loss = 0.05175277590751648
 42%|████▏     | 42/100 [03:37<05:02,  5.22s/it] 43%|████▎     | 43/100 [03:41<04:30,  4.75s/it]epoch 42, loss = 0.07994648814201355
epoch 43, loss = 0.0799577459692955
 44%|████▍     | 44/100 [03:46<04:28,  4.80s/it]epoch 44, loss = 0.05676937475800514
 45%|████▌     | 45/100 [03:54<05:23,  5.88s/it]Epoch 45: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4622 / 0.4624 / 0.9344 / 0.4095/ [0.43312102 0.65517241 0.40948276]
epoch 45, loss = 0.05812489241361618
 46%|████▌     | 46/100 [03:59<05:01,  5.58s/it] 47%|████▋     | 47/100 [04:03<04:29,  5.09s/it]epoch 46, loss = 0.05329727381467819
epoch 47, loss = 0.06867080926895142
 48%|████▊     | 48/100 [04:08<04:20,  5.02s/it] 49%|████▉     | 49/100 [04:11<03:53,  4.57s/it]epoch 48, loss = 0.08066267520189285
epoch 49, loss = 0.04751552641391754
 50%|█████     | 50/100 [04:20<04:58,  5.96s/it]Epoch 50: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4685 / 0.4666 / 0.9385 / 0.4353/ [0.42675159 0.63218391 0.43534483]
 51%|█████     | 51/100 [04:24<04:21,  5.33s/it]epoch 50, loss = 0.04574435204267502
epoch 51, loss = 0.05135073885321617
 52%|█████▏    | 52/100 [04:29<04:11,  5.25s/it] 53%|█████▎    | 53/100 [04:33<03:44,  4.78s/it]epoch 52, loss = 0.05554516240954399
epoch 53, loss = 0.04725770279765129
 54%|█████▍    | 54/100 [04:38<03:43,  4.86s/it]epoch 54, loss = 0.041049689054489136
 55%|█████▌    | 55/100 [04:47<04:26,  5.93s/it]Epoch 55: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9000 / 0.8665 / 0.9559 / 0.9062/ [0.72222222 0.96       0.90625   ]
                Test acc/f1/spec/sens = 0.4664 / 0.4542 / 0.8770 / 0.4741/ [0.43312102 0.50574713 0.47413793]
epoch 55, loss = 0.05611461028456688
 56%|█████▌    | 56/100 [04:51<04:06,  5.61s/it] 57%|█████▋    | 57/100 [04:55<03:37,  5.05s/it]epoch 56, loss = 0.052046675235033035
epoch 57, loss = 0.04358029365539551
 58%|█████▊    | 58/100 [05:00<03:31,  5.04s/it] 59%|█████▉    | 59/100 [05:04<03:11,  4.66s/it]epoch 58, loss = 0.05801812559366226
epoch 59, loss = 0.07079174369573593
 60%|██████    | 60/100 [05:13<04:03,  6.09s/it]Epoch 60: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4538 / 0.4523 / 0.9098 / 0.4095/ [0.43312102 0.6091954  0.40948276]
 61%|██████    | 61/100 [05:17<03:31,  5.42s/it]epoch 60, loss = 0.052682533860206604
epoch 61, loss = 0.05101452395319939
 62%|██████▏   | 62/100 [05:22<03:19,  5.24s/it] 63%|██████▎   | 63/100 [05:26<02:57,  4.81s/it]epoch 62, loss = 0.0781085193157196
epoch 63, loss = 0.04694642871618271
 64%|██████▍   | 64/100 [05:31<02:52,  4.79s/it]epoch 64, loss = 0.06803356111049652
 65%|██████▌   | 65/100 [05:39<03:23,  5.80s/it]Epoch 65: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4685 / 0.4613 / 0.9016 / 0.4569/ [0.43312102 0.56321839 0.45689655]
epoch 65, loss = 0.04525914043188095
 66%|██████▌   | 66/100 [05:43<03:05,  5.47s/it] 67%|██████▋   | 67/100 [05:47<02:45,  5.01s/it]epoch 66, loss = 0.09273123741149902
epoch 67, loss = 0.05779015272855759
 68%|██████▊   | 68/100 [05:52<02:38,  4.96s/it] 69%|██████▉   | 69/100 [05:56<02:23,  4.62s/it]epoch 68, loss = 0.03890125826001167
epoch 69, loss = 0.06380048394203186
 70%|███████   | 70/100 [06:05<03:02,  6.07s/it]Epoch 70: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4559 / 0.4511 / 0.8975 / 0.4267/ [0.43949045 0.56321839 0.42672414]
 71%|███████   | 71/100 [06:09<02:37,  5.43s/it]epoch 70, loss = 0.04048335924744606
epoch 71, loss = 0.05342712253332138
 72%|███████▏  | 72/100 [06:14<02:26,  5.23s/it] 73%|███████▎  | 73/100 [06:18<02:08,  4.76s/it]epoch 72, loss = 0.07873183488845825
epoch 73, loss = 0.046015795320272446
 74%|███████▍  | 74/100 [06:23<02:05,  4.84s/it]epoch 74, loss = 0.043512362986803055
 75%|███████▌  | 75/100 [06:31<02:28,  5.95s/it]Epoch 75: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4538 / 0.4453 / 0.8770 / 0.4440/ [0.44585987 0.49425287 0.44396552]
epoch 75, loss = 0.03150545805692673
 76%|███████▌  | 76/100 [06:36<02:14,  5.62s/it] 77%|███████▋  | 77/100 [06:40<01:56,  5.06s/it]epoch 76, loss = 0.10225328058004379
epoch 77, loss = 0.05289803817868233
 78%|███████▊  | 78/100 [06:45<01:50,  5.03s/it] 79%|███████▉  | 79/100 [06:49<01:38,  4.68s/it]epoch 78, loss = 0.057881128042936325
epoch 79, loss = 0.04733544960618019
 80%|████████  | 80/100 [06:58<02:02,  6.11s/it]Epoch 80: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5294 / 0.5071 / 0.8893 / 0.5862/ [0.42675159 0.56321839 0.5862069 ]
 81%|████████  | 81/100 [07:02<01:43,  5.46s/it]epoch 80, loss = 0.03643195331096649
epoch 81, loss = 0.048408009111881256
 82%|████████▏ | 82/100 [07:07<01:34,  5.28s/it] 83%|████████▎ | 83/100 [07:11<01:22,  4.87s/it]epoch 82, loss = 0.033564407378435135
epoch 83, loss = 0.039380986243486404
 84%|████████▍ | 84/100 [07:16<01:17,  4.87s/it]epoch 84, loss = 0.034110188484191895
 85%|████████▌ | 85/100 [07:24<01:29,  5.96s/it]Epoch 85: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4790 / 0.4765 / 0.9303 / 0.4440/ [0.43949045 0.64367816 0.44396552]
epoch 85, loss = 0.04096376895904541
 86%|████████▌ | 86/100 [07:29<01:19,  5.71s/it] 87%|████████▋ | 87/100 [07:33<01:06,  5.10s/it]epoch 86, loss = 0.03481237590312958
epoch 87, loss = 0.05406467244029045
 88%|████████▊ | 88/100 [07:38<01:00,  5.05s/it] 89%|████████▉ | 89/100 [07:42<00:51,  4.72s/it]epoch 88, loss = 0.043780211359262466
epoch 89, loss = 0.11295302957296371
 90%|█████████ | 90/100 [07:52<01:01,  6.16s/it]Epoch 90: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5084 / 0.4879 / 0.8811 / 0.5603/ [0.41401274 0.54022989 0.56034483]
 91%|█████████ | 91/100 [07:55<00:49,  5.47s/it]epoch 90, loss = 0.040455412119627
epoch 91, loss = 0.037303365767002106
 92%|█████████▏| 92/100 [08:00<00:42,  5.27s/it] 93%|█████████▎| 93/100 [08:04<00:33,  4.85s/it]epoch 92, loss = 0.037108033895492554
epoch 93, loss = 0.1428157240152359
 94%|█████████▍| 94/100 [08:09<00:29,  4.85s/it]epoch 94, loss = 0.043013494461774826
 95%|█████████▌| 95/100 [08:17<00:29,  5.84s/it]Epoch 95: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4685 / 0.4662 / 0.9139 / 0.4310/ [0.42675159 0.64367816 0.43103448]
epoch 95, loss = 0.06646764278411865
 96%|█████████▌| 96/100 [08:22<00:22,  5.54s/it] 97%|█████████▋| 97/100 [08:26<00:15,  5.01s/it]epoch 96, loss = 0.0431215837597847
epoch 97, loss = 0.03486379608511925
 98%|█████████▊| 98/100 [08:31<00:10,  5.02s/it] 99%|█████████▉| 99/100 [08:35<00:04,  4.65s/it]epoch 98, loss = 0.077222540974617
epoch 99, loss = 0.043539419770240784
100%|██████████| 100/100 [08:44<00:00,  6.13s/it]                                                 Epoch 100: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.4727 / 0.4605 / 0.8770 / 0.4828/ [0.43949045 0.50574713 0.48275862]
Trial 1, Cycle 1
Train acc/f1/spec/sens = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.]
Val acc/f1/spec/sens = 0.8900 / 0.8539 / 0.9559 / 0.8750/ [0.72222222 0.96       0.875     ]
Test acc/f1/spec/sens = 0.4727 / 0.4605 / 0.8770 / 0.4828/ [0.43949045 0.50574713 0.48275862]
>> Finished.
Acc, agreement for latest model:  0.4726890756302521 0.5252100840336135 [0.43949045 0.50574713 0.48275862]
Load best checkpoint for thief model
Acc, agreement for best model:  0.6092436974789915 0.6449579831932774 [0.07643312 0.91954023 0.85344828]
Trial 0/1 || Cycle 1/1 || Label set size 900 || Test acc 0.6092 || Test agreement 0.6450 || Spec 0.7869 || Sens 0.8534 Class Acc: [0.07643312 0.91954023 0.85344828]
**************************************************************************************************** 

Number of samples  900
0.6449579831932774 0.0
        acc  ...                label dist
0  0.609244  ...  {0: 149, 1: 391, 2: 360}

[1 rows x 6 columns]
Results saved to  /home/ankita/scratch/data_msa_medical/results_ankita//pocus_resnet18/butterfly_resnet50/SGD/1000_val100/random_v2
