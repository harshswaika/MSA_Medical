[24/02/27 10:27:33] [conf.py:  298]: PyTorch Version: torch=2.2.1+cu121, cuda=12.1, cudnn=8902
[24/02/27 10:27:33] [conf.py:  300]: ACTIVE:
  ADDENDUM: 0
  ALPHA: 0.2
  AUGMENT: None
  BETA: 1.0
  BUDGET: 2000
  CUTMIX_PROB: 0.0
  CYCLES: 1
  INITIAL: 1800
  LA: False
  METHOD: random
  PRETRAINED_PATH: /home/deepankar/mnt/vision3_ckpts/vit_b_16-imagenet1k.pth
  TEMP: 1.0
  TRO: 1.0
  USE_PRETRAINED: True
  VAL: 200
AM:
  SGR: 1000
  THRESHOLD: 0.5
CUDNN_BENCHMARK: True
DS_SEED: 123
EDM:
  HASH: /home/harsh_s/scratch/msacopy/exp/hash/alpha.pt
  NUM_MODELS: 5
  PATH1: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T0.pt
  PATH2: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T1.pt
  PATH3: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T2.pt
  PATH4: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T3.pt
  PATH5: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T4.pt
GR:
  EPSILON: 0.1
  SUR_PATH: /home/harsh_s/scratch/msacopy/GRAD2/batch_training/outputs/trained_models/imagenet_cifar10_to_cifar10_surrogate_10epochs.pt
LOG_DEST: radformer_vitb16_1k_gbusv_240227_102733.txt
LOG_TIME: 240227_102733
METHOD_NAME: v1_0.005
OUT_DIR: /home/deepankar/scratch/MSA_Medical/results_deepankar
PL:
  CLASS_BLNC: 10
  ITERATIONS: 20
  KAPPA_N: 0.005
  KAPPA_P: 0.05
  NO_PROGRESS: False
  NO_UNCERTAINTY: False
  TAU_N: 0.05
  TAU_P: 0.7
  TEMP_NL: 2.0
RNG_SEED: 5
SAVE_DIR: /home/deepankar/scratch/MSA_Medical/results_deepankar/gbusg_radformer/GBUSV_vit_b_16_1k/SGD/2000_val200/random_v1_0.005
THIEF:
  ARCH: vit_b_16_1k
  DATASET: GBUSV
  DATA_ROOT: /home/deepankar/mnt/vision3_data_ckpts_msa_medical/GBUSV-Shared
  HARD_LABELS: True
  NUM_TRAIN: 1281167
  SUBSET: 128116
TRAIN:
  BATCH: 16
  EPOCH: 100
  GAMMA: 0.1
  LR: 0.005
  MILESTONES: [20, 40, 60, 80]
  MOMENTUM: 0.9
  OPTIMIZER: SGD
  WDECAY: 0.0005
VICTIM:
  ARCH: radformer
  DATASET: gbusg
  DATA_ROOT: /home/deepankar/mnt/vision3_data_ckpts_msa_medical/GBCU-Shared
  HEIGHT: 224
  PATH: /home/deepankar/mnt/vision3_data_ckpts_msa_medical/victim_models/radformer/radformer.pkl
  PATHIN: /home/harsh_s/scratch/msacopy/ckpts/cifar10-resnet32-inaccurate/checkpoint.pth.tar
  WIDTH: 224
Loaded target dataset of size 122 with 3 classes
target model keys:  695
checkpoint keys:  695
specificity = 72/80
sensitivity = 39/42

Target model acc = 0.9016393442622951
Val-Acc: 0.9016 Val-Spec: 0.9000 Val-Sens: 0.9286
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
replacing labeled set labels with victim labels
  0%|          | 0/113 [00:00<?, ?it/s]  1%|          | 1/113 [00:00<00:57,  1.95it/s]  2%|▏         | 2/113 [00:00<00:40,  2.76it/s]  3%|▎         | 3/113 [00:01<00:33,  3.31it/s]  4%|▎         | 4/113 [00:01<00:31,  3.45it/s]  4%|▍         | 5/113 [00:01<00:30,  3.59it/s]  5%|▌         | 6/113 [00:01<00:28,  3.78it/s]  6%|▌         | 7/113 [00:02<00:28,  3.74it/s]  7%|▋         | 8/113 [00:02<00:27,  3.82it/s]  8%|▊         | 9/113 [00:02<00:27,  3.79it/s]  9%|▉         | 10/113 [00:02<00:26,  3.82it/s] 10%|▉         | 11/113 [00:03<00:26,  3.86it/s] 11%|█         | 12/113 [00:03<00:25,  3.91it/s] 12%|█▏        | 13/113 [00:03<00:25,  3.90it/s] 12%|█▏        | 14/113 [00:03<00:27,  3.61it/s] 13%|█▎        | 15/113 [00:04<00:26,  3.68it/s] 14%|█▍        | 16/113 [00:04<00:26,  3.73it/s] 15%|█▌        | 17/113 [00:04<00:25,  3.77it/s] 16%|█▌        | 18/113 [00:04<00:26,  3.63it/s] 17%|█▋        | 19/113 [00:05<00:26,  3.50it/s] 18%|█▊        | 20/113 [00:05<00:24,  3.82it/s] 19%|█▊        | 21/113 [00:05<00:21,  4.22it/s] 19%|█▉        | 22/113 [00:05<00:20,  4.52it/s] 20%|██        | 23/113 [00:06<00:18,  4.75it/s] 21%|██        | 24/113 [00:06<00:17,  4.95it/s] 22%|██▏       | 25/113 [00:06<00:17,  5.12it/s] 23%|██▎       | 26/113 [00:06<00:16,  5.18it/s] 24%|██▍       | 27/113 [00:06<00:17,  5.01it/s] 25%|██▍       | 28/113 [00:06<00:16,  5.18it/s] 26%|██▌       | 29/113 [00:07<00:15,  5.27it/s] 27%|██▋       | 30/113 [00:07<00:15,  5.34it/s] 27%|██▋       | 31/113 [00:07<00:15,  5.40it/s] 28%|██▊       | 32/113 [00:07<00:15,  5.40it/s] 29%|██▉       | 33/113 [00:07<00:14,  5.42it/s] 30%|███       | 34/113 [00:08<00:14,  5.46it/s] 31%|███       | 35/113 [00:08<00:14,  5.49it/s] 32%|███▏      | 36/113 [00:08<00:14,  5.48it/s] 33%|███▎      | 37/113 [00:08<00:13,  5.44it/s] 34%|███▎      | 38/113 [00:08<00:14,  5.03it/s] 35%|███▍      | 39/113 [00:09<00:14,  5.00it/s] 35%|███▌      | 40/113 [00:09<00:15,  4.83it/s] 36%|███▋      | 41/113 [00:09<00:14,  4.97it/s] 37%|███▋      | 42/113 [00:09<00:14,  5.00it/s] 38%|███▊      | 43/113 [00:09<00:13,  5.10it/s] 39%|███▉      | 44/113 [00:10<00:14,  4.87it/s] 40%|███▉      | 45/113 [00:10<00:13,  4.95it/s] 41%|████      | 46/113 [00:10<00:13,  5.08it/s] 42%|████▏     | 47/113 [00:10<00:12,  5.12it/s] 42%|████▏     | 48/113 [00:10<00:13,  4.95it/s] 43%|████▎     | 49/113 [00:11<00:12,  5.01it/s] 44%|████▍     | 50/113 [00:11<00:12,  5.05it/s] 45%|████▌     | 51/113 [00:11<00:12,  5.06it/s] 46%|████▌     | 52/113 [00:11<00:12,  5.02it/s] 47%|████▋     | 53/113 [00:11<00:11,  5.08it/s] 48%|████▊     | 54/113 [00:12<00:11,  5.17it/s] 49%|████▊     | 55/113 [00:12<00:11,  5.20it/s] 50%|████▉     | 56/113 [00:12<00:10,  5.22it/s] 50%|█████     | 57/113 [00:12<00:10,  5.18it/s] 51%|█████▏    | 58/113 [00:12<00:10,  5.19it/s] 52%|█████▏    | 59/113 [00:12<00:10,  5.19it/s] 53%|█████▎    | 60/113 [00:13<00:10,  5.22it/s] 54%|█████▍    | 61/113 [00:13<00:09,  5.27it/s] 55%|█████▍    | 62/113 [00:13<00:09,  5.24it/s] 56%|█████▌    | 63/113 [00:13<00:10,  4.93it/s] 57%|█████▋    | 64/113 [00:14<00:10,  4.80it/s] 58%|█████▊    | 65/113 [00:14<00:09,  4.88it/s] 58%|█████▊    | 66/113 [00:14<00:09,  4.98it/s] 59%|█████▉    | 67/113 [00:14<00:09,  4.81it/s] 60%|██████    | 68/113 [00:14<00:09,  4.72it/s] 61%|██████    | 69/113 [00:15<00:09,  4.86it/s] 62%|██████▏   | 70/113 [00:15<00:08,  4.97it/s] 63%|██████▎   | 71/113 [00:15<00:08,  4.75it/s] 64%|██████▎   | 72/113 [00:15<00:08,  4.67it/s] 65%|██████▍   | 73/113 [00:15<00:08,  4.83it/s] 65%|██████▌   | 74/113 [00:16<00:07,  4.92it/s] 66%|██████▋   | 75/113 [00:16<00:07,  4.75it/s] 67%|██████▋   | 76/113 [00:16<00:07,  4.65it/s] 68%|██████▊   | 77/113 [00:16<00:07,  4.78it/s] 69%|██████▉   | 78/113 [00:16<00:07,  4.92it/s] 70%|██████▉   | 79/113 [00:17<00:07,  4.77it/s] 71%|███████   | 80/113 [00:17<00:07,  4.58it/s] 72%|███████▏  | 81/113 [00:17<00:06,  4.74it/s] 73%|███████▎  | 82/113 [00:17<00:06,  4.90it/s] 73%|███████▎  | 83/113 [00:17<00:06,  4.80it/s] 74%|███████▍  | 84/113 [00:18<00:06,  4.69it/s] 75%|███████▌  | 85/113 [00:18<00:05,  4.84it/s] 76%|███████▌  | 86/113 [00:18<00:05,  4.88it/s] 77%|███████▋  | 87/113 [00:18<00:05,  4.71it/s] 78%|███████▊  | 88/113 [00:19<00:05,  4.63it/s] 79%|███████▉  | 89/113 [00:19<00:05,  4.77it/s] 80%|███████▉  | 90/113 [00:19<00:04,  4.89it/s] 81%|████████  | 91/113 [00:19<00:04,  4.77it/s] 81%|████████▏ | 92/113 [00:19<00:04,  4.66it/s] 82%|████████▏ | 93/113 [00:20<00:04,  4.82it/s] 83%|████████▎ | 94/113 [00:20<00:03,  4.91it/s] 84%|████████▍ | 95/113 [00:20<00:03,  4.80it/s] 85%|████████▍ | 96/113 [00:20<00:03,  4.65it/s] 86%|████████▌ | 97/113 [00:20<00:03,  4.38it/s] 87%|████████▋ | 98/113 [00:21<00:03,  4.62it/s] 88%|████████▊ | 99/113 [00:21<00:03,  4.54it/s] 88%|████████▊ | 100/113 [00:21<00:02,  4.49it/s] 89%|████████▉ | 101/113 [00:21<00:02,  4.69it/s] 90%|█████████ | 102/113 [00:22<00:02,  4.85it/s] 91%|█████████ | 103/113 [00:22<00:02,  4.67it/s] 92%|█████████▏| 104/113 [00:22<00:01,  4.59it/s] 93%|█████████▎| 105/113 [00:22<00:01,  4.80it/s] 94%|█████████▍| 106/113 [00:22<00:01,  4.69it/s] 95%|█████████▍| 107/113 [00:23<00:01,  4.69it/s] 96%|█████████▌| 108/113 [00:23<00:01,  4.88it/s] 96%|█████████▋| 109/113 [00:23<00:00,  5.04it/s] 97%|█████████▋| 110/113 [00:23<00:00,  4.70it/s] 98%|█████████▊| 111/113 [00:23<00:00,  4.46it/s] 99%|█████████▉| 112/113 [00:24<00:00,  4.27it/s]100%|██████████| 113/113 [00:24<00:00,  3.83it/s]100%|██████████| 113/113 [00:24<00:00,  4.59it/s]
replacing val labels with victim labels
  0%|          | 0/13 [00:00<?, ?it/s]  8%|▊         | 1/13 [00:00<00:07,  1.55it/s] 15%|█▌        | 2/13 [00:00<00:04,  2.38it/s] 23%|██▎       | 3/13 [00:01<00:03,  2.89it/s] 31%|███       | 4/13 [00:01<00:02,  3.56it/s] 38%|███▊      | 5/13 [00:01<00:01,  4.08it/s] 46%|████▌     | 6/13 [00:01<00:01,  4.50it/s] 54%|█████▍    | 7/13 [00:01<00:01,  4.75it/s] 62%|██████▏   | 8/13 [00:02<00:01,  4.94it/s] 69%|██████▉   | 9/13 [00:02<00:00,  5.06it/s] 77%|███████▋  | 10/13 [00:02<00:00,  5.19it/s] 85%|████████▍ | 11/13 [00:02<00:00,  5.30it/s] 92%|█████████▏| 12/13 [00:02<00:00,  5.34it/s]100%|██████████| 13/13 [00:02<00:00,  4.40it/s]Validation set distribution: 
Number of samples  200

{0: 26, 1: 38, 2: 136}
Labeled set distribution: 
Number of samples  1800
{0: 197, 1: 443, 2: 1160}
odict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_0.mlp.0.weight', 'encoder.layers.encoder_layer_0.mlp.0.bias', 'encoder.layers.encoder_layer_0.mlp.3.weight', 'encoder.layers.encoder_layer_0.mlp.3.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_1.mlp.0.weight', 'encoder.layers.encoder_layer_1.mlp.0.bias', 'encoder.layers.encoder_layer_1.mlp.3.weight', 'encoder.layers.encoder_layer_1.mlp.3.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_2.mlp.0.weight', 'encoder.layers.encoder_layer_2.mlp.0.bias', 'encoder.layers.encoder_layer_2.mlp.3.weight', 'encoder.layers.encoder_layer_2.mlp.3.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_3.mlp.0.weight', 'encoder.layers.encoder_layer_3.mlp.0.bias', 'encoder.layers.encoder_layer_3.mlp.3.weight', 'encoder.layers.encoder_layer_3.mlp.3.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_4.mlp.0.weight', 'encoder.layers.encoder_layer_4.mlp.0.bias', 'encoder.layers.encoder_layer_4.mlp.3.weight', 'encoder.layers.encoder_layer_4.mlp.3.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_5.mlp.0.weight', 'encoder.layers.encoder_layer_5.mlp.0.bias', 'encoder.layers.encoder_layer_5.mlp.3.weight', 'encoder.layers.encoder_layer_5.mlp.3.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_6.mlp.0.weight', 'encoder.layers.encoder_layer_6.mlp.0.bias', 'encoder.layers.encoder_layer_6.mlp.3.weight', 'encoder.layers.encoder_layer_6.mlp.3.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_7.mlp.0.weight', 'encoder.layers.encoder_layer_7.mlp.0.bias', 'encoder.layers.encoder_layer_7.mlp.3.weight', 'encoder.layers.encoder_layer_7.mlp.3.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_8.mlp.0.weight', 'encoder.layers.encoder_layer_8.mlp.0.bias', 'encoder.layers.encoder_layer_8.mlp.3.weight', 'encoder.layers.encoder_layer_8.mlp.3.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_9.mlp.0.weight', 'encoder.layers.encoder_layer_9.mlp.0.bias', 'encoder.layers.encoder_layer_9.mlp.3.weight', 'encoder.layers.encoder_layer_9.mlp.3.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_10.mlp.0.weight', 'encoder.layers.encoder_layer_10.mlp.0.bias', 'encoder.layers.encoder_layer_10.mlp.3.weight', 'encoder.layers.encoder_layer_10.mlp.3.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.layers.encoder_layer_11.mlp.0.weight', 'encoder.layers.encoder_layer_11.mlp.0.bias', 'encoder.layers.encoder_layer_11.mlp.3.weight', 'encoder.layers.encoder_layer_11.mlp.3.bias', 'encoder.ln.weight', 'encoder.ln.bias', 'heads.head.weight', 'heads.head.bias'])
thief state:  None
pretrained state:  dict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.ln.weight', 'encoder.ln.bias'])
Thief model initialized successfully
specificity = 80/80
sensitivity = 0/42
Initial model on target dataset: acc = 0.2541, agreement = 0.2377, f1 = 0.1351, spec = 1.0000, sens = 0.0000
specificity = 64/64
sensitivity = 0/136
Initial model on validation dataset: acc = 0.1300, agreement = 0.1300, f1 = 0.0767, spec = 1.0000, sens = 0.0000
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:19<32:18, 19.58s/it]  2%|▏         | 2/100 [00:40<33:43, 20.65s/it]  3%|▎         | 3/100 [01:00<32:44, 20.25s/it]  4%|▍         | 4/100 [01:21<32:47, 20.50s/it]  5%|▌         | 5/100 [01:50<37:02, 23.39s/it]Epoch 5: Train acc/f1 = 0.6728 / 0.3832 / 0.2391 / 0.9440 
                Val acc/f1/spec/sens = 0.7200 / 0.4041 / 0.2031 / 0.9853
                Test acc/f1/spec/sens = 0.3361 / 0.1708 / 0.0375 / 0.9762
  6%|▌         | 6/100 [02:11<35:18, 22.54s/it]  7%|▋         | 7/100 [02:30<33:33, 21.65s/it]  8%|▊         | 8/100 [02:51<32:51, 21.43s/it]  9%|▉         | 9/100 [03:11<31:45, 20.94s/it] 10%|█         | 10/100 [03:41<35:37, 23.75s/it]Epoch 10: Train acc/f1 = 0.6711 / 0.3484 / 0.1187 / 0.9879 
                Val acc/f1/spec/sens = 0.7100 / 0.3657 / 0.0938 / 1.0000
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
 11%|█         | 11/100 [04:01<33:28, 22.57s/it] 12%|█▏        | 12/100 [04:22<32:24, 22.09s/it] 13%|█▎        | 13/100 [04:42<31:04, 21.43s/it] 14%|█▍        | 14/100 [05:03<30:30, 21.28s/it] 15%|█▌        | 15/100 [05:32<33:18, 23.51s/it]Epoch 15: Train acc/f1 = 0.6878 / 0.4095 / 0.2969 / 0.9397 
                Val acc/f1/spec/sens = 0.7100 / 0.3657 / 0.0938 / 1.0000
                Test acc/f1/spec/sens = 0.3361 / 0.1792 / 0.0250 / 0.9524
 16%|█▌        | 16/100 [05:53<31:50, 22.74s/it] 17%|█▋        | 17/100 [06:13<30:17, 21.89s/it] 18%|█▊        | 18/100 [06:33<29:32, 21.62s/it] 19%|█▉        | 19/100 [06:53<28:28, 21.09s/it] 20%|██        | 20/100 [07:24<31:48, 23.86s/it]Epoch 20: Train acc/f1 = 0.6944 / 0.4753 / 0.3797 / 0.9259 
                Val acc/f1/spec/sens = 0.7400 / 0.4869 / 0.3281 / 0.9706
                Test acc/f1/spec/sens = 0.3689 / 0.2275 / 0.0625 / 1.0000
 21%|██        | 21/100 [07:44<29:52, 22.69s/it] 22%|██▏       | 22/100 [08:05<29:06, 22.39s/it] 23%|██▎       | 23/100 [08:25<27:47, 21.65s/it] 24%|██▍       | 24/100 [08:47<27:25, 21.66s/it] 25%|██▌       | 25/100 [09:16<29:42, 23.77s/it]Epoch 25: Train acc/f1 = 0.7100 / 0.5021 / 0.4750 / 0.9129 
                Val acc/f1/spec/sens = 0.7750 / 0.5491 / 0.4219 / 0.9853
                Test acc/f1/spec/sens = 0.3689 / 0.2277 / 0.0375 / 1.0000
 26%|██▌       | 26/100 [09:37<28:32, 23.14s/it] 27%|██▋       | 27/100 [09:57<26:59, 22.18s/it] 28%|██▊       | 28/100 [10:18<26:12, 21.84s/it] 29%|██▉       | 29/100 [10:38<25:11, 21.29s/it] 30%|███       | 30/100 [11:09<28:12, 24.18s/it]Epoch 30: Train acc/f1 = 0.7072 / 0.5321 / 0.4437 / 0.9336 
                Val acc/f1/spec/sens = 0.8000 / 0.6515 / 0.5000 / 0.9853
                Test acc/f1/spec/sens = 0.3607 / 0.2132 / 0.0250 / 1.0000
 31%|███       | 31/100 [11:29<26:21, 22.92s/it] 32%|███▏      | 32/100 [11:50<25:21, 22.38s/it] 33%|███▎      | 33/100 [12:10<24:09, 21.63s/it] 34%|███▍      | 34/100 [12:31<23:37, 21.47s/it] 35%|███▌      | 35/100 [13:00<25:35, 23.62s/it]Epoch 35: Train acc/f1 = 0.7244 / 0.5284 / 0.4688 / 0.9379 
                Val acc/f1/spec/sens = 0.7600 / 0.5284 / 0.3438 / 0.9926
                Test acc/f1/spec/sens = 0.3689 / 0.2309 / 0.0625 / 0.9762
 36%|███▌      | 36/100 [13:21<24:22, 22.85s/it] 37%|███▋      | 37/100 [13:41<23:04, 21.97s/it] 38%|███▊      | 38/100 [14:03<22:38, 21.92s/it] 39%|███▉      | 39/100 [14:23<21:40, 21.32s/it] 40%|████      | 40/100 [14:53<23:58, 23.98s/it]Epoch 40: Train acc/f1 = 0.7433 / 0.5968 / 0.5687 / 0.9121 
                Val acc/f1/spec/sens = 0.8000 / 0.6471 / 0.5625 / 0.9706
                Test acc/f1/spec/sens = 0.3934 / 0.2764 / 0.1125 / 0.9762
 41%|████      | 41/100 [15:13<22:24, 22.79s/it] 42%|████▏     | 42/100 [15:34<21:33, 22.30s/it] 43%|████▎     | 43/100 [15:54<20:30, 21.59s/it] 44%|████▍     | 44/100 [16:15<20:01, 21.45s/it] 45%|████▌     | 45/100 [16:44<21:41, 23.66s/it]Epoch 45: Train acc/f1 = 0.7367 / 0.5929 / 0.6062 / 0.8974 
                Val acc/f1/spec/sens = 0.8100 / 0.6652 / 0.5781 / 0.9706
                Test acc/f1/spec/sens = 0.4098 / 0.2992 / 0.1500 / 0.9762
 46%|████▌     | 46/100 [17:05<20:34, 22.87s/it] 47%|████▋     | 47/100 [17:25<19:25, 21.99s/it] 48%|████▊     | 48/100 [17:47<19:01, 21.94s/it] 49%|████▉     | 49/100 [18:07<18:08, 21.35s/it] 50%|█████     | 50/100 [18:37<20:04, 24.09s/it]Epoch 50: Train acc/f1 = 0.7383 / 0.5788 / 0.5031 / 0.9414 
                Val acc/f1/spec/sens = 0.8150 / 0.6825 / 0.5781 / 0.9779
                Test acc/f1/spec/sens = 0.3852 / 0.2644 / 0.1000 / 0.9762
 51%|█████     | 51/100 [18:57<18:40, 22.87s/it] 52%|█████▏    | 52/100 [19:18<17:54, 22.39s/it] 53%|█████▎    | 53/100 [19:38<16:59, 21.68s/it] 54%|█████▍    | 54/100 [20:00<16:31, 21.56s/it] 55%|█████▌    | 55/100 [20:29<17:56, 23.93s/it]Epoch 55: Train acc/f1 = 0.7483 / 0.6071 / 0.6297 / 0.8922 
                Val acc/f1/spec/sens = 0.8100 / 0.6598 / 0.6250 / 0.9779
                Test acc/f1/spec/sens = 0.4098 / 0.2991 / 0.1750 / 0.9762
 56%|█████▌    | 56/100 [20:50<16:55, 23.09s/it] 57%|█████▋    | 57/100 [21:10<15:53, 22.17s/it] 58%|█████▊    | 58/100 [21:32<15:19, 21.90s/it] 59%|█████▉    | 59/100 [21:52<14:34, 21.34s/it] 60%|██████    | 60/100 [22:22<15:58, 23.96s/it]Epoch 60: Train acc/f1 = 0.7400 / 0.6012 / 0.5719 / 0.9121 
                Val acc/f1/spec/sens = 0.8150 / 0.6938 / 0.6406 / 0.9632
                Test acc/f1/spec/sens = 0.4098 / 0.2991 / 0.1625 / 0.9762
 61%|██████    | 61/100 [22:42<14:48, 22.78s/it] 62%|██████▏   | 62/100 [23:03<14:07, 22.31s/it] 63%|██████▎   | 63/100 [23:23<13:19, 21.61s/it] 64%|██████▍   | 64/100 [23:44<12:54, 21.52s/it] 65%|██████▌   | 65/100 [24:14<13:56, 23.89s/it]Epoch 65: Train acc/f1 = 0.7350 / 0.5942 / 0.5734 / 0.9026 
                Val acc/f1/spec/sens = 0.8150 / 0.6916 / 0.6406 / 0.9632
                Test acc/f1/spec/sens = 0.4098 / 0.2991 / 0.1625 / 0.9762
 66%|██████▌   | 66/100 [24:35<13:04, 23.06s/it] 67%|██████▋   | 67/100 [24:55<12:10, 22.14s/it] 68%|██████▊   | 68/100 [25:16<11:39, 21.87s/it] 69%|██████▉   | 69/100 [25:36<11:00, 21.30s/it] 70%|███████   | 70/100 [26:07<12:03, 24.10s/it]Epoch 70: Train acc/f1 = 0.7444 / 0.5957 / 0.5594 / 0.9181 
                Val acc/f1/spec/sens = 0.8150 / 0.6916 / 0.6406 / 0.9632
                Test acc/f1/spec/sens = 0.4098 / 0.2991 / 0.1625 / 0.9762
 71%|███████   | 71/100 [26:27<11:03, 22.87s/it] 72%|███████▏  | 72/100 [26:48<10:26, 22.36s/it] 73%|███████▎  | 73/100 [27:08<09:44, 21.64s/it] 74%|███████▍  | 74/100 [27:29<09:19, 21.51s/it] 75%|███████▌  | 75/100 [27:58<09:56, 23.88s/it]Epoch 75: Train acc/f1 = 0.7406 / 0.5943 / 0.5734 / 0.9147 
                Val acc/f1/spec/sens = 0.8150 / 0.6916 / 0.6406 / 0.9632
                Test acc/f1/spec/sens = 0.4016 / 0.2784 / 0.1500 / 0.9762
 76%|███████▌  | 76/100 [28:19<09:13, 23.05s/it] 77%|███████▋  | 77/100 [28:39<08:28, 22.12s/it] 78%|███████▊  | 78/100 [29:01<08:00, 21.86s/it] 79%|███████▉  | 79/100 [29:21<07:27, 21.29s/it] 80%|████████  | 80/100 [29:51<08:02, 24.13s/it]Epoch 80: Train acc/f1 = 0.7411 / 0.5874 / 0.5734 / 0.9103 
                Val acc/f1/spec/sens = 0.8100 / 0.6914 / 0.6562 / 0.9485
                Test acc/f1/spec/sens = 0.3934 / 0.2739 / 0.1500 / 0.9524
 81%|████████  | 81/100 [30:11<07:14, 22.88s/it] 82%|████████▏ | 82/100 [30:33<06:42, 22.38s/it] 83%|████████▎ | 83/100 [30:53<06:08, 21.65s/it] 84%|████████▍ | 84/100 [31:14<05:44, 21.51s/it] 85%|████████▌ | 85/100 [31:43<05:58, 23.91s/it]Epoch 85: Train acc/f1 = 0.7428 / 0.5796 / 0.5875 / 0.9172 
                Val acc/f1/spec/sens = 0.8200 / 0.6994 / 0.6562 / 0.9632
                Test acc/f1/spec/sens = 0.3934 / 0.2739 / 0.1500 / 0.9524
 86%|████████▌ | 86/100 [32:04<05:23, 23.09s/it] 87%|████████▋ | 87/100 [32:24<04:47, 22.15s/it] 88%|████████▊ | 88/100 [32:46<04:22, 21.85s/it] 89%|████████▉ | 89/100 [33:05<03:54, 21.29s/it] 90%|█████████ | 90/100 [33:36<04:00, 24.08s/it]Epoch 90: Train acc/f1 = 0.7489 / 0.6064 / 0.5750 / 0.9164 
                Val acc/f1/spec/sens = 0.8200 / 0.6994 / 0.6562 / 0.9632
                Test acc/f1/spec/sens = 0.3934 / 0.2739 / 0.1500 / 0.9524
 91%|█████████ | 91/100 [33:56<03:25, 22.83s/it] 92%|█████████▏| 92/100 [34:17<02:58, 22.34s/it] 93%|█████████▎| 93/100 [34:37<02:31, 21.62s/it] 94%|█████████▍| 94/100 [34:58<02:09, 21.53s/it] 95%|█████████▌| 95/100 [35:28<01:59, 23.91s/it]Epoch 95: Train acc/f1 = 0.7389 / 0.5971 / 0.5609 / 0.9103 
                Val acc/f1/spec/sens = 0.8150 / 0.6916 / 0.6406 / 0.9632
                Test acc/f1/spec/sens = 0.3934 / 0.2739 / 0.1500 / 0.9524
 96%|█████████▌| 96/100 [35:49<01:32, 23.08s/it] 97%|█████████▋| 97/100 [36:09<01:06, 22.13s/it] 98%|█████████▊| 98/100 [36:30<00:43, 21.90s/it] 99%|█████████▉| 99/100 [36:50<00:21, 21.35s/it]100%|██████████| 100/100 [37:23<00:00, 24.67s/it]                                                 Epoch 100: Train acc/f1 = 0.7383 / 0.5935 / 0.5641 / 0.9103 
                Val acc/f1/spec/sens = 0.8150 / 0.6916 / 0.6406 / 0.9632
                Test acc/f1/spec/sens = 0.3934 / 0.2739 / 0.1500 / 0.9524
specificity = 367/640
sensitivity = 1073/1160
specificity = 41/64
sensitivity = 131/136
specificity = 12/80
sensitivity = 40/42
Trial 1, Cycle 1
Train acc/f1/spec/sens = 0.7467 / 0.6009 / 0.5734 / 0.9250
Val acc/f1/spec/sens = 0.8150 / 0.6916 / 0.6406 / 0.9632
Test acc/f1/spec/sens = 0.3934 / 0.2739 / 0.1500 / 0.9524
>> Finished.
specificity = 12/80
sensitivity = 40/42
Acc, agreement for latest model:  0.39344262295081966 0.4426229508196721
Load best checkpoint for thief model
specificity = 8/80
sensitivity = 42/42
Acc, agreement for best model:  0.39344262295081966 0.4262295081967213
Trial 0/5 || Cycle 1/1 || Label set size 1800 || Test acc 0.3934 || Test agreement 0.4262 || Spec 0.1000 || Sens 1.0000
**************************************************************************************************** 

specificity = 8/80
sensitivity = 42/42
Number of samples  1800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
replacing labeled set labels with victim labels
  0%|          | 0/113 [00:00<?, ?it/s]  1%|          | 1/113 [00:00<01:13,  1.52it/s]  2%|▏         | 2/113 [00:00<00:49,  2.26it/s]  3%|▎         | 3/113 [00:01<00:40,  2.74it/s]  4%|▎         | 4/113 [00:01<00:35,  3.04it/s]  4%|▍         | 5/113 [00:01<00:33,  3.27it/s]  5%|▌         | 6/113 [00:02<00:30,  3.48it/s]  6%|▌         | 7/113 [00:02<00:29,  3.58it/s]  7%|▋         | 8/113 [00:02<00:28,  3.70it/s]  8%|▊         | 9/113 [00:02<00:27,  3.77it/s]  9%|▉         | 10/113 [00:03<00:27,  3.78it/s] 10%|▉         | 11/113 [00:03<00:26,  3.80it/s] 11%|█         | 12/113 [00:03<00:26,  3.84it/s] 12%|█▏        | 13/113 [00:03<00:25,  3.86it/s] 12%|█▏        | 14/113 [00:04<00:25,  3.91it/s] 13%|█▎        | 15/113 [00:04<00:24,  3.96it/s] 14%|█▍        | 16/113 [00:04<00:24,  3.94it/s] 15%|█▌        | 17/113 [00:04<00:24,  3.94it/s] 16%|█▌        | 18/113 [00:05<00:23,  3.96it/s] 17%|█▋        | 19/113 [00:05<00:23,  3.99it/s] 18%|█▊        | 20/113 [00:05<00:23,  3.94it/s] 19%|█▊        | 21/113 [00:05<00:23,  3.92it/s] 19%|█▉        | 22/113 [00:06<00:23,  3.91it/s] 20%|██        | 23/113 [00:06<00:22,  3.94it/s] 21%|██        | 24/113 [00:06<00:22,  3.95it/s] 22%|██▏       | 25/113 [00:06<00:22,  3.99it/s] 23%|██▎       | 26/113 [00:07<00:21,  4.01it/s] 24%|██▍       | 27/113 [00:07<00:21,  4.01it/s] 25%|██▍       | 28/113 [00:07<00:21,  3.93it/s] 26%|██▌       | 29/113 [00:07<00:21,  3.98it/s] 27%|██▋       | 30/113 [00:08<00:20,  3.96it/s] 27%|██▋       | 31/113 [00:08<00:20,  3.98it/s] 28%|██▊       | 32/113 [00:08<00:20,  4.03it/s] 29%|██▉       | 33/113 [00:08<00:20,  3.98it/s] 30%|███       | 34/113 [00:09<00:19,  3.99it/s] 31%|███       | 35/113 [00:09<00:19,  3.97it/s] 32%|███▏      | 36/113 [00:09<00:19,  3.90it/s] 33%|███▎      | 37/113 [00:09<00:19,  3.95it/s] 34%|███▎      | 38/113 [00:10<00:18,  3.98it/s] 35%|███▍      | 39/113 [00:10<00:18,  3.96it/s] 35%|███▌      | 40/113 [00:10<00:18,  3.99it/s] 36%|███▋      | 41/113 [00:10<00:17,  4.01it/s] 37%|███▋      | 42/113 [00:11<00:17,  4.02it/s] 38%|███▊      | 43/113 [00:11<00:17,  3.95it/s] 39%|███▉      | 44/113 [00:11<00:17,  3.95it/s] 40%|███▉      | 45/113 [00:11<00:17,  3.95it/s] 41%|████      | 46/113 [00:12<00:16,  3.96it/s] 42%|████▏     | 47/113 [00:12<00:16,  3.99it/s] 42%|████▏     | 48/113 [00:12<00:16,  3.96it/s] 43%|████▎     | 49/113 [00:12<00:15,  4.01it/s] 44%|████▍     | 50/113 [00:13<00:15,  4.04it/s] 45%|████▌     | 51/113 [00:13<00:15,  4.00it/s] 46%|████▌     | 52/113 [00:13<00:15,  3.96it/s] 47%|████▋     | 53/113 [00:13<00:15,  3.97it/s] 48%|████▊     | 54/113 [00:14<00:14,  3.97it/s] 49%|████▊     | 55/113 [00:14<00:14,  3.94it/s] 50%|████▉     | 56/113 [00:14<00:14,  4.01it/s] 50%|█████     | 57/113 [00:14<00:14,  3.98it/s] 51%|█████▏    | 58/113 [00:15<00:13,  3.98it/s] 52%|█████▏    | 59/113 [00:15<00:13,  4.00it/s] 53%|█████▎    | 60/113 [00:15<00:13,  4.01it/s] 54%|█████▍    | 61/113 [00:15<00:12,  4.04it/s] 55%|█████▍    | 62/113 [00:16<00:12,  4.04it/s] 56%|█████▌    | 63/113 [00:16<00:12,  3.99it/s] 57%|█████▋    | 64/113 [00:16<00:12,  3.97it/s] 58%|█████▊    | 65/113 [00:16<00:12,  3.94it/s] 58%|█████▊    | 66/113 [00:17<00:11,  3.97it/s] 59%|█████▉    | 67/113 [00:17<00:11,  3.99it/s] 60%|██████    | 68/113 [00:17<00:11,  4.02it/s] 61%|██████    | 69/113 [00:17<00:10,  4.03it/s] 62%|██████▏   | 70/113 [00:18<00:10,  4.02it/s] 63%|██████▎   | 71/113 [00:18<00:10,  4.05it/s] 64%|██████▎   | 72/113 [00:18<00:11,  3.72it/s] 65%|██████▍   | 73/113 [00:18<00:10,  3.66it/s] 65%|██████▌   | 74/113 [00:19<00:10,  3.76it/s] 66%|██████▋   | 75/113 [00:19<00:09,  3.80it/s] 67%|██████▋   | 76/113 [00:19<00:09,  3.87it/s] 68%|██████▊   | 77/113 [00:19<00:09,  3.89it/s] 69%|██████▉   | 78/113 [00:20<00:08,  3.90it/s] 70%|██████▉   | 79/113 [00:20<00:08,  3.93it/s] 71%|███████   | 80/113 [00:20<00:08,  3.92it/s] 72%|███████▏  | 81/113 [00:21<00:08,  3.89it/s] 73%|███████▎  | 82/113 [00:21<00:07,  3.92it/s] 73%|███████▎  | 83/113 [00:21<00:07,  3.97it/s] 74%|███████▍  | 84/113 [00:21<00:07,  3.99it/s] 75%|███████▌  | 85/113 [00:21<00:07,  3.99it/s] 76%|███████▌  | 86/113 [00:22<00:06,  3.98it/s] 77%|███████▋  | 87/113 [00:22<00:06,  3.97it/s] 78%|███████▊  | 88/113 [00:22<00:06,  3.93it/s] 79%|███████▉  | 89/113 [00:23<00:06,  3.94it/s] 80%|███████▉  | 90/113 [00:23<00:05,  3.93it/s] 81%|████████  | 91/113 [00:23<00:05,  3.93it/s] 81%|████████▏ | 92/113 [00:23<00:05,  3.96it/s] 82%|████████▏ | 93/113 [00:24<00:05,  3.96it/s] 83%|████████▎ | 94/113 [00:24<00:04,  3.96it/s] 84%|████████▍ | 95/113 [00:24<00:04,  3.97it/s] 85%|████████▍ | 96/113 [00:24<00:04,  4.02it/s] 86%|████████▌ | 97/113 [00:25<00:04,  3.96it/s] 87%|████████▋ | 98/113 [00:25<00:03,  3.95it/s] 88%|████████▊ | 99/113 [00:25<00:03,  3.92it/s] 88%|████████▊ | 100/113 [00:25<00:03,  3.92it/s] 89%|████████▉ | 101/113 [00:26<00:03,  3.97it/s] 90%|█████████ | 102/113 [00:26<00:02,  3.95it/s] 91%|█████████ | 103/113 [00:26<00:02,  3.97it/s] 92%|█████████▏| 104/113 [00:26<00:02,  3.96it/s] 93%|█████████▎| 105/113 [00:27<00:02,  3.99it/s] 94%|█████████▍| 106/113 [00:27<00:01,  4.02it/s] 95%|█████████▍| 107/113 [00:27<00:01,  4.02it/s] 96%|█████████▌| 108/113 [00:27<00:01,  4.02it/s] 96%|█████████▋| 109/113 [00:28<00:00,  4.04it/s] 97%|█████████▋| 110/113 [00:28<00:00,  3.99it/s] 98%|█████████▊| 111/113 [00:28<00:00,  3.98it/s] 99%|█████████▉| 112/113 [00:28<00:00,  4.00it/s]100%|██████████| 113/113 [00:28<00:00,  4.62it/s]100%|██████████| 113/113 [00:29<00:00,  3.89it/s]
replacing val labels with victim labels
  0%|          | 0/13 [00:00<?, ?it/s]  8%|▊         | 1/13 [00:00<00:07,  1.56it/s] 15%|█▌        | 2/13 [00:00<00:04,  2.36it/s] 23%|██▎       | 3/13 [00:01<00:03,  2.88it/s] 31%|███       | 4/13 [00:01<00:02,  3.22it/s] 38%|███▊      | 5/13 [00:01<00:02,  3.37it/s] 46%|████▌     | 6/13 [00:01<00:01,  3.55it/s] 54%|█████▍    | 7/13 [00:02<00:01,  3.68it/s] 62%|██████▏   | 8/13 [00:02<00:01,  3.67it/s] 69%|██████▉   | 9/13 [00:02<00:01,  3.77it/s] 77%|███████▋  | 10/13 [00:02<00:00,  3.86it/s] 85%|████████▍ | 11/13 [00:03<00:00,  3.91it/s] 92%|█████████▏| 12/13 [00:03<00:00,  3.94it/s]100%|██████████| 13/13 [00:03<00:00,  4.62it/s]100%|██████████| 13/13 [00:03<00:00,  3.53it/s]Validation set distribution: 
Number of samples  200

{0: 29, 1: 48, 2: 123}
Labeled set distribution: 
Number of samples  1800
{0: 219, 1: 415, 2: 1166}
odict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_0.mlp.0.weight', 'encoder.layers.encoder_layer_0.mlp.0.bias', 'encoder.layers.encoder_layer_0.mlp.3.weight', 'encoder.layers.encoder_layer_0.mlp.3.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_1.mlp.0.weight', 'encoder.layers.encoder_layer_1.mlp.0.bias', 'encoder.layers.encoder_layer_1.mlp.3.weight', 'encoder.layers.encoder_layer_1.mlp.3.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_2.mlp.0.weight', 'encoder.layers.encoder_layer_2.mlp.0.bias', 'encoder.layers.encoder_layer_2.mlp.3.weight', 'encoder.layers.encoder_layer_2.mlp.3.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_3.mlp.0.weight', 'encoder.layers.encoder_layer_3.mlp.0.bias', 'encoder.layers.encoder_layer_3.mlp.3.weight', 'encoder.layers.encoder_layer_3.mlp.3.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_4.mlp.0.weight', 'encoder.layers.encoder_layer_4.mlp.0.bias', 'encoder.layers.encoder_layer_4.mlp.3.weight', 'encoder.layers.encoder_layer_4.mlp.3.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_5.mlp.0.weight', 'encoder.layers.encoder_layer_5.mlp.0.bias', 'encoder.layers.encoder_layer_5.mlp.3.weight', 'encoder.layers.encoder_layer_5.mlp.3.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_6.mlp.0.weight', 'encoder.layers.encoder_layer_6.mlp.0.bias', 'encoder.layers.encoder_layer_6.mlp.3.weight', 'encoder.layers.encoder_layer_6.mlp.3.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_7.mlp.0.weight', 'encoder.layers.encoder_layer_7.mlp.0.bias', 'encoder.layers.encoder_layer_7.mlp.3.weight', 'encoder.layers.encoder_layer_7.mlp.3.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_8.mlp.0.weight', 'encoder.layers.encoder_layer_8.mlp.0.bias', 'encoder.layers.encoder_layer_8.mlp.3.weight', 'encoder.layers.encoder_layer_8.mlp.3.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_9.mlp.0.weight', 'encoder.layers.encoder_layer_9.mlp.0.bias', 'encoder.layers.encoder_layer_9.mlp.3.weight', 'encoder.layers.encoder_layer_9.mlp.3.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_10.mlp.0.weight', 'encoder.layers.encoder_layer_10.mlp.0.bias', 'encoder.layers.encoder_layer_10.mlp.3.weight', 'encoder.layers.encoder_layer_10.mlp.3.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.layers.encoder_layer_11.mlp.0.weight', 'encoder.layers.encoder_layer_11.mlp.0.bias', 'encoder.layers.encoder_layer_11.mlp.3.weight', 'encoder.layers.encoder_layer_11.mlp.3.bias', 'encoder.ln.weight', 'encoder.ln.bias', 'heads.head.weight', 'heads.head.bias'])
thief state:  None
pretrained state:  dict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.ln.weight', 'encoder.ln.bias'])
Thief model initialized successfully
specificity = 80/80
sensitivity = 0/42
Initial model on target dataset: acc = 0.2541, agreement = 0.2377, f1 = 0.1351, spec = 1.0000, sens = 0.0000
specificity = 77/77
sensitivity = 0/123
Initial model on validation dataset: acc = 0.1450, agreement = 0.1450, f1 = 0.0844, spec = 1.0000, sens = 0.0000
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:19<32:28, 19.68s/it]  2%|▏         | 2/100 [00:41<33:45, 20.67s/it]  3%|▎         | 3/100 [01:00<32:48, 20.29s/it]  4%|▍         | 4/100 [01:22<33:14, 20.78s/it]  5%|▌         | 5/100 [01:51<37:48, 23.88s/it]Epoch 5: Train acc/f1 = 0.6800 / 0.3784 / 0.1562 / 0.9820 
                Val acc/f1/spec/sens = 0.6450 / 0.3827 / 0.2597 / 0.9350
                Test acc/f1/spec/sens = 0.3443 / 0.1718 / 0.0125 / 1.0000
  6%|▌         | 6/100 [02:12<35:53, 22.91s/it]  7%|▋         | 7/100 [02:32<33:59, 21.93s/it]  8%|▊         | 8/100 [02:54<33:30, 21.85s/it]  9%|▉         | 9/100 [03:14<32:13, 21.25s/it] 10%|█         | 10/100 [03:44<36:02, 24.02s/it]Epoch 10: Train acc/f1 = 0.6800 / 0.4011 / 0.1703 / 0.9854 
                Val acc/f1/spec/sens = 0.6500 / 0.3796 / 0.1558 / 0.9919
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
 11%|█         | 11/100 [04:04<33:47, 22.79s/it] 12%|█▏        | 12/100 [04:25<32:38, 22.26s/it] 13%|█▎        | 13/100 [04:45<31:15, 21.56s/it] 14%|█▍        | 14/100 [05:07<31:00, 21.64s/it] 15%|█▌        | 15/100 [05:36<33:44, 23.82s/it]Epoch 15: Train acc/f1 = 0.6772 / 0.3609 / 0.1562 / 0.9820 
                Val acc/f1/spec/sens = 0.6700 / 0.4726 / 0.3377 / 0.9593
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
 16%|█▌        | 16/100 [05:57<32:11, 23.00s/it] 17%|█▋        | 17/100 [06:17<30:32, 22.08s/it] 18%|█▊        | 18/100 [06:39<30:03, 21.99s/it] 19%|█▉        | 19/100 [06:58<28:50, 21.36s/it] 20%|██        | 20/100 [07:29<32:07, 24.10s/it]Epoch 20: Train acc/f1 = 0.7011 / 0.5094 / 0.4369 / 0.9245 
                Val acc/f1/spec/sens = 0.6950 / 0.5672 / 0.4935 / 0.9024
                Test acc/f1/spec/sens = 0.3689 / 0.2284 / 0.0750 / 1.0000
 21%|██        | 21/100 [07:49<30:05, 22.86s/it] 22%|██▏       | 22/100 [08:10<29:02, 22.33s/it] 23%|██▎       | 23/100 [08:30<27:44, 21.62s/it] 24%|██▍       | 24/100 [08:52<27:27, 21.68s/it] 25%|██▌       | 25/100 [09:21<29:59, 24.00s/it]Epoch 25: Train acc/f1 = 0.7056 / 0.4803 / 0.3454 / 0.9683 
                Val acc/f1/spec/sens = 0.7300 / 0.6159 / 0.5065 / 0.9350
                Test acc/f1/spec/sens = 0.3525 / 0.1926 / 0.0125 / 1.0000
 26%|██▌       | 26/100 [09:43<28:47, 23.34s/it] 27%|██▋       | 27/100 [10:03<27:09, 22.32s/it] 28%|██▊       | 28/100 [10:24<26:21, 21.96s/it] 29%|██▉       | 29/100 [10:44<25:17, 21.37s/it] 30%|███       | 30/100 [11:14<28:05, 24.09s/it]Epoch 30: Train acc/f1 = 0.7144 / 0.5032 / 0.4006 / 0.9545 
                Val acc/f1/spec/sens = 0.7300 / 0.6072 / 0.4935 / 0.9431
                Test acc/f1/spec/sens = 0.3770 / 0.2352 / 0.0750 / 1.0000
 31%|███       | 31/100 [11:34<26:16, 22.84s/it] 32%|███▏      | 32/100 [11:56<25:17, 22.32s/it] 33%|███▎      | 33/100 [12:16<24:08, 21.62s/it] 34%|███▍      | 34/100 [12:37<23:50, 21.68s/it] 35%|███▌      | 35/100 [13:07<25:59, 24.00s/it]Epoch 35: Train acc/f1 = 0.7317 / 0.5874 / 0.5410 / 0.9211 
                Val acc/f1/spec/sens = 0.7300 / 0.6312 / 0.5455 / 0.9024
                Test acc/f1/spec/sens = 0.4262 / 0.3429 / 0.1875 / 0.9524
 36%|███▌      | 36/100 [13:28<24:39, 23.12s/it] 37%|███▋      | 37/100 [13:48<23:17, 22.18s/it] 38%|███▊      | 38/100 [14:10<22:48, 22.08s/it] 39%|███▉      | 39/100 [14:30<21:46, 21.42s/it] 40%|████      | 40/100 [15:01<24:17, 24.30s/it]Epoch 40: Train acc/f1 = 0.7378 / 0.6189 / 0.6609 / 0.8851 
                Val acc/f1/spec/sens = 0.7450 / 0.6582 / 0.7792 / 0.8699
                Test acc/f1/spec/sens = 0.4918 / 0.4086 / 0.5125 / 0.9286
 41%|████      | 41/100 [15:20<22:36, 22.99s/it] 42%|████▏     | 42/100 [15:42<21:52, 22.63s/it] 43%|████▎     | 43/100 [16:02<20:44, 21.83s/it] 44%|████▍     | 44/100 [16:24<20:22, 21.83s/it] 45%|████▌     | 45/100 [16:53<22:03, 24.07s/it]Epoch 45: Train acc/f1 = 0.7439 / 0.5967 / 0.5457 / 0.9348 
                Val acc/f1/spec/sens = 0.7650 / 0.6820 / 0.7143 / 0.9024
                Test acc/f1/spec/sens = 0.4262 / 0.3199 / 0.2625 / 0.9762
 46%|████▌     | 46/100 [17:14<20:51, 23.18s/it] 47%|████▋     | 47/100 [17:34<19:37, 22.22s/it] 48%|████▊     | 48/100 [17:56<18:57, 21.87s/it] 49%|████▉     | 49/100 [18:15<18:06, 21.30s/it] 50%|█████     | 50/100 [18:46<20:10, 24.21s/it]Epoch 50: Train acc/f1 = 0.7522 / 0.6251 / 0.6215 / 0.9177 
                Val acc/f1/spec/sens = 0.7750 / 0.6952 / 0.7403 / 0.9024
                Test acc/f1/spec/sens = 0.4754 / 0.4199 / 0.4250 / 0.9524
 51%|█████     | 51/100 [19:06<18:43, 22.93s/it] 52%|█████▏    | 52/100 [19:28<17:54, 22.38s/it] 53%|█████▎    | 53/100 [19:47<16:57, 21.66s/it] 54%|█████▍    | 54/100 [20:09<16:29, 21.51s/it] 55%|█████▌    | 55/100 [20:37<17:45, 23.67s/it]Epoch 55: Train acc/f1 = 0.7528 / 0.6076 / 0.5757 / 0.9357 
                Val acc/f1/spec/sens = 0.7750 / 0.6952 / 0.7143 / 0.9106
                Test acc/f1/spec/sens = 0.4098 / 0.2973 / 0.3250 / 0.9524
 56%|█████▌    | 56/100 [20:58<16:46, 22.88s/it] 57%|█████▋    | 57/100 [21:18<15:45, 21.99s/it] 58%|█████▊    | 58/100 [21:39<15:12, 21.73s/it] 59%|█████▉    | 59/100 [21:59<14:29, 21.20s/it] 60%|██████    | 60/100 [22:31<16:08, 24.20s/it]Epoch 60: Train acc/f1 = 0.7667 / 0.6481 / 0.6640 / 0.9142 
                Val acc/f1/spec/sens = 0.7800 / 0.7066 / 0.7532 / 0.8943
                Test acc/f1/spec/sens = 0.4508 / 0.3837 / 0.4875 / 0.8810
 61%|██████    | 61/100 [22:51<14:53, 22.92s/it] 62%|██████▏   | 62/100 [23:12<14:11, 22.40s/it] 63%|██████▎   | 63/100 [23:32<13:21, 21.65s/it] 64%|██████▍   | 64/100 [23:53<12:53, 21.50s/it] 65%|██████▌   | 65/100 [24:22<13:54, 23.84s/it]Epoch 65: Train acc/f1 = 0.7622 / 0.6474 / 0.6735 / 0.9065 
                Val acc/f1/spec/sens = 0.7800 / 0.7005 / 0.7403 / 0.9106
                Test acc/f1/spec/sens = 0.4508 / 0.3804 / 0.4750 / 0.9048
 66%|██████▌   | 66/100 [24:43<13:02, 23.03s/it] 67%|██████▋   | 67/100 [25:03<12:08, 22.09s/it] 68%|██████▊   | 68/100 [25:24<11:38, 21.82s/it] 69%|██████▉   | 69/100 [25:44<10:58, 21.25s/it] 70%|███████   | 70/100 [26:15<12:00, 24.02s/it]Epoch 70: Train acc/f1 = 0.7656 / 0.6527 / 0.6893 / 0.9005 
                Val acc/f1/spec/sens = 0.7750 / 0.6953 / 0.7662 / 0.8943
                Test acc/f1/spec/sens = 0.4590 / 0.3855 / 0.4625 / 0.9286
 71%|███████   | 71/100 [26:35<11:01, 22.80s/it] 72%|███████▏  | 72/100 [26:56<10:25, 22.32s/it] 73%|███████▎  | 73/100 [27:16<09:43, 21.63s/it] 74%|███████▍  | 74/100 [27:37<09:18, 21.49s/it] 75%|███████▌  | 75/100 [28:06<09:54, 23.77s/it]Epoch 75: Train acc/f1 = 0.7656 / 0.6478 / 0.6861 / 0.9082 
                Val acc/f1/spec/sens = 0.7700 / 0.6916 / 0.7403 / 0.8943
                Test acc/f1/spec/sens = 0.4590 / 0.3889 / 0.4750 / 0.9048
 76%|███████▌  | 76/100 [28:27<09:11, 22.98s/it] 77%|███████▋  | 77/100 [28:47<08:27, 22.06s/it] 78%|███████▊  | 78/100 [29:08<07:59, 21.79s/it] 79%|███████▉  | 79/100 [29:28<07:25, 21.22s/it] 80%|████████  | 80/100 [29:59<08:00, 24.03s/it]Epoch 80: Train acc/f1 = 0.7767 / 0.6661 / 0.6814 / 0.9160 
                Val acc/f1/spec/sens = 0.7800 / 0.6990 / 0.7922 / 0.8943
                Test acc/f1/spec/sens = 0.4590 / 0.3855 / 0.4625 / 0.9286
 81%|████████  | 81/100 [30:19<07:13, 22.80s/it] 82%|████████▏ | 82/100 [30:40<06:41, 22.33s/it] 83%|████████▎ | 83/100 [31:00<06:07, 21.62s/it] 84%|████████▍ | 84/100 [31:21<05:44, 21.51s/it] 85%|████████▌ | 85/100 [31:51<06:01, 24.11s/it]Epoch 85: Train acc/f1 = 0.7728 / 0.6595 / 0.6877 / 0.9142 
                Val acc/f1/spec/sens = 0.7750 / 0.6929 / 0.7792 / 0.8943
                Test acc/f1/spec/sens = 0.4590 / 0.3855 / 0.4625 / 0.9286
 86%|████████▌ | 86/100 [32:13<05:25, 23.24s/it] 87%|████████▋ | 87/100 [32:33<04:49, 22.26s/it] 88%|████████▊ | 88/100 [32:54<04:22, 21.90s/it] 89%|████████▉ | 89/100 [33:13<03:54, 21.29s/it] 90%|█████████ | 90/100 [33:45<04:02, 24.30s/it]Epoch 90: Train acc/f1 = 0.7783 / 0.6708 / 0.6688 / 0.9202 
                Val acc/f1/spec/sens = 0.7750 / 0.6953 / 0.7662 / 0.8943
                Test acc/f1/spec/sens = 0.4672 / 0.3905 / 0.4625 / 0.9524
 91%|█████████ | 91/100 [34:05<03:27, 23.00s/it] 92%|█████████▏| 92/100 [34:26<03:00, 22.50s/it] 93%|█████████▎| 93/100 [34:46<02:32, 21.76s/it] 94%|█████████▍| 94/100 [35:07<02:09, 21.55s/it] 95%|█████████▌| 95/100 [35:36<01:59, 23.84s/it]Epoch 95: Train acc/f1 = 0.7644 / 0.6467 / 0.6688 / 0.9091 
                Val acc/f1/spec/sens = 0.7800 / 0.7014 / 0.7792 / 0.8943
                Test acc/f1/spec/sens = 0.4508 / 0.3806 / 0.4625 / 0.9048
 96%|█████████▌| 96/100 [35:57<01:32, 23.00s/it] 97%|█████████▋| 97/100 [36:17<01:06, 22.08s/it] 98%|█████████▊| 98/100 [36:39<00:43, 21.83s/it] 99%|█████████▉| 99/100 [36:59<00:21, 21.26s/it]100%|██████████| 100/100 [37:30<00:00, 24.38s/it]                                                 Epoch 100: Train acc/f1 = 0.7644 / 0.6434 / 0.6640 / 0.9108 
                Val acc/f1/spec/sens = 0.7750 / 0.6953 / 0.7662 / 0.8943
                Test acc/f1/spec/sens = 0.4508 / 0.3806 / 0.4625 / 0.9048
specificity = 421/634
sensitivity = 1074/1166
specificity = 59/77
sensitivity = 110/123
specificity = 37/80
sensitivity = 38/42
Trial 2, Cycle 1
Train acc/f1/spec/sens = 0.7711 / 0.6504 / 0.6640 / 0.9211
Val acc/f1/spec/sens = 0.7750 / 0.6953 / 0.7662 / 0.8943
Test acc/f1/spec/sens = 0.4508 / 0.3806 / 0.4625 / 0.9048
>> Finished.
specificity = 37/80
sensitivity = 38/42
Acc, agreement for latest model:  0.45081967213114754 0.48360655737704916
Load best checkpoint for thief model
specificity = 39/80
sensitivity = 37/42
Acc, agreement for best model:  0.45081967213114754 0.48360655737704916
Trial 1/5 || Cycle 1/1 || Label set size 1800 || Test acc 0.4508 || Test agreement 0.4836 || Spec 0.4875 || Sens 0.8810
**************************************************************************************************** 

specificity = 39/80
sensitivity = 37/42
Number of samples  1800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
replacing labeled set labels with victim labels
  0%|          | 0/113 [00:00<?, ?it/s]  1%|          | 1/113 [00:00<01:04,  1.74it/s]  2%|▏         | 2/113 [00:00<00:40,  2.76it/s]  3%|▎         | 3/113 [00:01<00:33,  3.30it/s]  4%|▎         | 4/113 [00:01<00:28,  3.80it/s]  4%|▍         | 5/113 [00:01<00:26,  4.11it/s]  5%|▌         | 6/113 [00:01<00:24,  4.45it/s]  6%|▌         | 7/113 [00:01<00:22,  4.67it/s]  7%|▋         | 8/113 [00:02<00:21,  4.81it/s]  8%|▊         | 9/113 [00:02<00:21,  4.93it/s]  9%|▉         | 10/113 [00:02<00:22,  4.62it/s] 10%|▉         | 11/113 [00:02<00:23,  4.38it/s] 11%|█         | 12/113 [00:02<00:23,  4.27it/s] 12%|█▏        | 13/113 [00:03<00:22,  4.39it/s] 12%|█▏        | 14/113 [00:03<00:22,  4.38it/s] 13%|█▎        | 15/113 [00:03<00:21,  4.59it/s] 14%|█▍        | 16/113 [00:03<00:20,  4.69it/s] 15%|█▌        | 17/113 [00:04<00:21,  4.49it/s] 16%|█▌        | 18/113 [00:04<00:20,  4.64it/s] 17%|█▋        | 19/113 [00:04<00:19,  4.75it/s] 18%|█▊        | 20/113 [00:04<00:19,  4.82it/s] 19%|█▊        | 21/113 [00:04<00:19,  4.63it/s] 19%|█▉        | 22/113 [00:05<00:19,  4.70it/s] 20%|██        | 23/113 [00:05<00:19,  4.68it/s] 21%|██        | 24/113 [00:05<00:18,  4.79it/s] 22%|██▏       | 25/113 [00:05<00:18,  4.66it/s] 23%|██▎       | 26/113 [00:05<00:18,  4.80it/s] 24%|██▍       | 27/113 [00:06<00:18,  4.62it/s] 25%|██▍       | 28/113 [00:06<00:17,  4.77it/s] 26%|██▌       | 29/113 [00:06<00:17,  4.91it/s] 27%|██▋       | 30/113 [00:06<00:16,  5.00it/s] 27%|██▋       | 31/113 [00:06<00:17,  4.75it/s] 28%|██▊       | 32/113 [00:07<00:18,  4.44it/s] 29%|██▉       | 33/113 [00:07<00:17,  4.53it/s] 30%|███       | 34/113 [00:07<00:18,  4.39it/s] 31%|███       | 35/113 [00:07<00:18,  4.25it/s] 32%|███▏      | 36/113 [00:08<00:18,  4.14it/s] 33%|███▎      | 37/113 [00:08<00:19,  4.00it/s] 34%|███▎      | 38/113 [00:08<00:18,  3.96it/s] 35%|███▍      | 39/113 [00:08<00:18,  3.99it/s] 35%|███▌      | 40/113 [00:09<00:18,  4.01it/s] 36%|███▋      | 41/113 [00:09<00:18,  3.98it/s] 37%|███▋      | 42/113 [00:09<00:17,  3.95it/s] 38%|███▊      | 43/113 [00:09<00:18,  3.86it/s] 39%|███▉      | 44/113 [00:10<00:18,  3.65it/s] 40%|███▉      | 45/113 [00:10<00:19,  3.55it/s] 41%|████      | 46/113 [00:10<00:18,  3.67it/s] 42%|████▏     | 47/113 [00:11<00:17,  3.75it/s] 42%|████▏     | 48/113 [00:11<00:17,  3.81it/s] 43%|████▎     | 49/113 [00:11<00:17,  3.76it/s] 44%|████▍     | 50/113 [00:11<00:16,  3.81it/s] 45%|████▌     | 51/113 [00:12<00:16,  3.87it/s] 46%|████▌     | 52/113 [00:12<00:15,  3.85it/s] 47%|████▋     | 53/113 [00:12<00:16,  3.71it/s] 48%|████▊     | 54/113 [00:12<00:16,  3.62it/s] 49%|████▊     | 55/113 [00:13<00:16,  3.50it/s] 50%|████▉     | 56/113 [00:13<00:17,  3.33it/s] 50%|█████     | 57/113 [00:13<00:18,  3.08it/s] 51%|█████▏    | 58/113 [00:14<00:17,  3.06it/s] 52%|█████▏    | 59/113 [00:14<00:16,  3.18it/s] 53%|█████▎    | 60/113 [00:14<00:15,  3.36it/s] 54%|█████▍    | 61/113 [00:15<00:15,  3.39it/s] 55%|█████▍    | 62/113 [00:15<00:14,  3.42it/s] 56%|█████▌    | 63/113 [00:15<00:15,  3.32it/s] 57%|█████▋    | 64/113 [00:16<00:14,  3.41it/s] 58%|█████▊    | 65/113 [00:16<00:14,  3.34it/s] 58%|█████▊    | 66/113 [00:16<00:13,  3.48it/s] 59%|█████▉    | 67/113 [00:16<00:14,  3.23it/s] 60%|██████    | 68/113 [00:17<00:13,  3.31it/s] 61%|██████    | 69/113 [00:17<00:12,  3.48it/s] 62%|██████▏   | 70/113 [00:17<00:12,  3.54it/s] 63%|██████▎   | 71/113 [00:18<00:12,  3.45it/s] 64%|██████▎   | 72/113 [00:18<00:11,  3.49it/s] 65%|██████▍   | 73/113 [00:18<00:11,  3.58it/s] 65%|██████▌   | 74/113 [00:18<00:10,  3.55it/s] 66%|██████▋   | 75/113 [00:19<00:10,  3.51it/s] 67%|██████▋   | 76/113 [00:19<00:10,  3.62it/s] 68%|██████▊   | 77/113 [00:19<00:10,  3.41it/s] 69%|██████▉   | 78/113 [00:20<00:10,  3.40it/s] 70%|██████▉   | 79/113 [00:20<00:10,  3.39it/s] 71%|███████   | 80/113 [00:20<00:09,  3.51it/s] 72%|███████▏  | 81/113 [00:20<00:08,  3.57it/s] 73%|███████▎  | 82/113 [00:21<00:08,  3.53it/s] 73%|███████▎  | 83/113 [00:21<00:08,  3.45it/s] 74%|███████▍  | 84/113 [00:21<00:08,  3.61it/s] 75%|███████▌  | 85/113 [00:22<00:07,  3.56it/s] 76%|███████▌  | 86/113 [00:22<00:07,  3.56it/s] 77%|███████▋  | 87/113 [00:22<00:07,  3.28it/s] 78%|███████▊  | 88/113 [00:23<00:07,  3.15it/s] 79%|███████▉  | 89/113 [00:23<00:07,  3.09it/s] 80%|███████▉  | 90/113 [00:23<00:07,  3.08it/s] 81%|████████  | 91/113 [00:23<00:06,  3.27it/s] 81%|████████▏ | 92/113 [00:24<00:06,  3.32it/s] 82%|████████▏ | 93/113 [00:24<00:06,  3.22it/s] 83%|████████▎ | 94/113 [00:24<00:06,  3.14it/s] 84%|████████▍ | 95/113 [00:25<00:05,  3.14it/s] 85%|████████▍ | 96/113 [00:25<00:05,  3.17it/s] 86%|████████▌ | 97/113 [00:25<00:04,  3.29it/s] 87%|████████▋ | 98/113 [00:26<00:04,  3.39it/s] 88%|████████▊ | 99/113 [00:26<00:04,  3.50it/s] 88%|████████▊ | 100/113 [00:26<00:03,  3.52it/s] 89%|████████▉ | 101/113 [00:26<00:03,  3.59it/s] 90%|█████████ | 102/113 [00:27<00:03,  3.58it/s] 91%|█████████ | 103/113 [00:27<00:02,  3.56it/s] 92%|█████████▏| 104/113 [00:27<00:02,  3.61it/s] 93%|█████████▎| 105/113 [00:28<00:02,  3.66it/s] 94%|█████████▍| 106/113 [00:28<00:01,  3.72it/s] 95%|█████████▍| 107/113 [00:28<00:01,  3.80it/s] 96%|█████████▌| 108/113 [00:28<00:01,  3.82it/s] 96%|█████████▋| 109/113 [00:29<00:01,  3.89it/s] 97%|█████████▋| 110/113 [00:29<00:00,  3.86it/s] 98%|█████████▊| 111/113 [00:29<00:00,  3.87it/s] 99%|█████████▉| 112/113 [00:29<00:00,  3.90it/s]100%|██████████| 113/113 [00:29<00:00,  4.39it/s]100%|██████████| 113/113 [00:30<00:00,  3.76it/s]
replacing val labels with victim labels
  0%|          | 0/13 [00:00<?, ?it/s]  8%|▊         | 1/13 [00:00<00:08,  1.39it/s] 15%|█▌        | 2/13 [00:01<00:05,  2.08it/s] 23%|██▎       | 3/13 [00:01<00:04,  2.50it/s] 31%|███       | 4/13 [00:01<00:03,  2.52it/s] 38%|███▊      | 5/13 [00:02<00:03,  2.62it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.79it/s] 54%|█████▍    | 7/13 [00:02<00:01,  3.04it/s] 62%|██████▏   | 8/13 [00:02<00:01,  3.04it/s] 69%|██████▉   | 9/13 [00:03<00:01,  3.19it/s] 77%|███████▋  | 10/13 [00:03<00:00,  3.38it/s] 85%|████████▍ | 11/13 [00:03<00:00,  3.42it/s] 92%|█████████▏| 12/13 [00:04<00:00,  3.43it/s]100%|██████████| 13/13 [00:04<00:00,  4.08it/s]100%|██████████| 13/13 [00:04<00:00,  2.99it/s]Validation set distribution: 
Number of samples  200

{0: 28, 1: 40, 2: 132}
Labeled set distribution: 
Number of samples  1800
{0: 205, 1: 426, 2: 1169}
odict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_0.mlp.0.weight', 'encoder.layers.encoder_layer_0.mlp.0.bias', 'encoder.layers.encoder_layer_0.mlp.3.weight', 'encoder.layers.encoder_layer_0.mlp.3.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_1.mlp.0.weight', 'encoder.layers.encoder_layer_1.mlp.0.bias', 'encoder.layers.encoder_layer_1.mlp.3.weight', 'encoder.layers.encoder_layer_1.mlp.3.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_2.mlp.0.weight', 'encoder.layers.encoder_layer_2.mlp.0.bias', 'encoder.layers.encoder_layer_2.mlp.3.weight', 'encoder.layers.encoder_layer_2.mlp.3.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_3.mlp.0.weight', 'encoder.layers.encoder_layer_3.mlp.0.bias', 'encoder.layers.encoder_layer_3.mlp.3.weight', 'encoder.layers.encoder_layer_3.mlp.3.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_4.mlp.0.weight', 'encoder.layers.encoder_layer_4.mlp.0.bias', 'encoder.layers.encoder_layer_4.mlp.3.weight', 'encoder.layers.encoder_layer_4.mlp.3.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_5.mlp.0.weight', 'encoder.layers.encoder_layer_5.mlp.0.bias', 'encoder.layers.encoder_layer_5.mlp.3.weight', 'encoder.layers.encoder_layer_5.mlp.3.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_6.mlp.0.weight', 'encoder.layers.encoder_layer_6.mlp.0.bias', 'encoder.layers.encoder_layer_6.mlp.3.weight', 'encoder.layers.encoder_layer_6.mlp.3.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_7.mlp.0.weight', 'encoder.layers.encoder_layer_7.mlp.0.bias', 'encoder.layers.encoder_layer_7.mlp.3.weight', 'encoder.layers.encoder_layer_7.mlp.3.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_8.mlp.0.weight', 'encoder.layers.encoder_layer_8.mlp.0.bias', 'encoder.layers.encoder_layer_8.mlp.3.weight', 'encoder.layers.encoder_layer_8.mlp.3.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_9.mlp.0.weight', 'encoder.layers.encoder_layer_9.mlp.0.bias', 'encoder.layers.encoder_layer_9.mlp.3.weight', 'encoder.layers.encoder_layer_9.mlp.3.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_10.mlp.0.weight', 'encoder.layers.encoder_layer_10.mlp.0.bias', 'encoder.layers.encoder_layer_10.mlp.3.weight', 'encoder.layers.encoder_layer_10.mlp.3.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.layers.encoder_layer_11.mlp.0.weight', 'encoder.layers.encoder_layer_11.mlp.0.bias', 'encoder.layers.encoder_layer_11.mlp.3.weight', 'encoder.layers.encoder_layer_11.mlp.3.bias', 'encoder.ln.weight', 'encoder.ln.bias', 'heads.head.weight', 'heads.head.bias'])
thief state:  None
pretrained state:  dict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.ln.weight', 'encoder.ln.bias'])
Thief model initialized successfully
specificity = 80/80
sensitivity = 0/42
Initial model on target dataset: acc = 0.2541, agreement = 0.2377, f1 = 0.1351, spec = 1.0000, sens = 0.0000
specificity = 68/68
sensitivity = 0/132
Initial model on validation dataset: acc = 0.1400, agreement = 0.1400, f1 = 0.0819, spec = 1.0000, sens = 0.0000
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:19<32:26, 19.66s/it]  2%|▏         | 2/100 [00:41<33:43, 20.65s/it]  3%|▎         | 3/100 [01:00<32:49, 20.30s/it]  4%|▍         | 4/100 [01:22<33:22, 20.86s/it]  5%|▌         | 5/100 [01:52<37:57, 23.97s/it]Epoch 5: Train acc/f1 = 0.6567 / 0.2979 / 0.0428 / 0.9906 
                Val acc/f1/spec/sens = 0.6700 / 0.2984 / 0.0294 / 1.0000
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
  6%|▌         | 6/100 [02:13<36:21, 23.20s/it]  7%|▋         | 7/100 [02:33<34:19, 22.15s/it]  8%|▊         | 8/100 [02:54<33:26, 21.81s/it]  9%|▉         | 9/100 [03:14<32:12, 21.23s/it] 10%|█         | 10/100 [03:45<36:18, 24.20s/it]Epoch 10: Train acc/f1 = 0.6728 / 0.4120 / 0.3819 / 0.8862 
                Val acc/f1/spec/sens = 0.6950 / 0.4239 / 0.3088 / 0.9318
                Test acc/f1/spec/sens = 0.3934 / 0.2563 / 0.1375 / 0.9762
 11%|█         | 11/100 [04:05<34:00, 22.93s/it] 12%|█▏        | 12/100 [04:27<33:08, 22.60s/it] 13%|█▎        | 13/100 [04:47<31:37, 21.81s/it] 14%|█▍        | 14/100 [05:08<30:59, 21.62s/it] 15%|█▌        | 15/100 [05:38<34:01, 24.02s/it]Epoch 15: Train acc/f1 = 0.6594 / 0.3034 / 0.0507 / 0.9932 
                Val acc/f1/spec/sens = 0.6800 / 0.3684 / 0.1324 / 0.9848
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
 16%|█▌        | 16/100 [06:00<32:44, 23.38s/it] 17%|█▋        | 17/100 [06:20<30:57, 22.37s/it] 18%|█▊        | 18/100 [06:42<30:25, 22.27s/it] 19%|█▉        | 19/100 [07:02<29:09, 21.59s/it] 20%|██        | 20/100 [07:32<32:13, 24.16s/it]Epoch 20: Train acc/f1 = 0.7100 / 0.4806 / 0.4422 / 0.9341 
                Val acc/f1/spec/sens = 0.7200 / 0.5114 / 0.4265 / 0.9470
                Test acc/f1/spec/sens = 0.3525 / 0.1972 / 0.0625 / 0.9762
 21%|██        | 21/100 [07:52<30:10, 22.92s/it] 22%|██▏       | 22/100 [08:13<29:09, 22.43s/it] 23%|██▎       | 23/100 [08:33<27:52, 21.72s/it] 24%|██▍       | 24/100 [08:55<27:34, 21.76s/it] 25%|██▌       | 25/100 [09:24<29:55, 23.94s/it]Epoch 25: Train acc/f1 = 0.7178 / 0.5145 / 0.3883 / 0.9538 
                Val acc/f1/spec/sens = 0.7700 / 0.6297 / 0.5294 / 0.9470
                Test acc/f1/spec/sens = 0.3525 / 0.1966 / 0.0500 / 0.9762
 26%|██▌       | 26/100 [09:46<28:42, 23.28s/it] 27%|██▋       | 27/100 [10:06<27:06, 22.28s/it] 28%|██▊       | 28/100 [10:28<26:34, 22.15s/it] 29%|██▉       | 29/100 [10:48<25:26, 21.51s/it] 30%|███       | 30/100 [11:18<28:17, 24.25s/it]Epoch 30: Train acc/f1 = 0.7389 / 0.5857 / 0.4659 / 0.9530 
                Val acc/f1/spec/sens = 0.7750 / 0.6340 / 0.5147 / 0.9621
                Test acc/f1/spec/sens = 0.3607 / 0.2190 / 0.0750 / 0.9762
 31%|███       | 31/100 [11:38<26:25, 22.97s/it] 32%|███▏      | 32/100 [12:00<25:26, 22.44s/it] 33%|███▎      | 33/100 [12:20<24:14, 21.71s/it] 34%|███▍      | 34/100 [12:41<23:52, 21.71s/it] 35%|███▌      | 35/100 [13:10<25:55, 23.93s/it]Epoch 35: Train acc/f1 = 0.7544 / 0.6050 / 0.5784 / 0.9333 
                Val acc/f1/spec/sens = 0.7950 / 0.6830 / 0.7059 / 0.9242
                Test acc/f1/spec/sens = 0.3934 / 0.2678 / 0.1750 / 0.9762
 36%|███▌      | 36/100 [13:32<24:36, 23.08s/it] 37%|███▋      | 37/100 [13:52<23:15, 22.16s/it] 38%|███▊      | 38/100 [14:13<22:37, 21.90s/it] 39%|███▉      | 39/100 [14:33<21:41, 21.33s/it] 40%|████      | 40/100 [15:03<24:03, 24.06s/it]Epoch 40: Train acc/f1 = 0.7267 / 0.5489 / 0.3899 / 0.9649 
                Val acc/f1/spec/sens = 0.7450 / 0.5523 / 0.4265 / 0.9773
                Test acc/f1/spec/sens = 0.3443 / 0.1938 / 0.0625 / 0.9762
 41%|████      | 41/100 [15:23<22:27, 22.84s/it] 42%|████▏     | 42/100 [15:44<21:35, 22.33s/it] 43%|████▎     | 43/100 [16:04<20:33, 21.64s/it] 44%|████▍     | 44/100 [16:26<20:04, 21.51s/it] 45%|████▌     | 45/100 [16:55<21:53, 23.88s/it]Epoch 45: Train acc/f1 = 0.7528 / 0.6262 / 0.5753 / 0.9247 
                Val acc/f1/spec/sens = 0.7800 / 0.6355 / 0.6324 / 0.9394
                Test acc/f1/spec/sens = 0.4016 / 0.3012 / 0.2625 / 0.9286
 46%|████▌     | 46/100 [17:16<20:45, 23.07s/it] 47%|████▋     | 47/100 [17:36<19:34, 22.16s/it] 48%|████▊     | 48/100 [17:58<18:58, 21.90s/it]