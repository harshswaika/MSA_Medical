[24/02/27 10:26:56] [conf.py:  298]: PyTorch Version: torch=2.2.1+cu121, cuda=12.1, cudnn=8902
[24/02/27 10:26:56] [conf.py:  300]: ACTIVE:
  ADDENDUM: 0
  ALPHA: 0.2
  AUGMENT: None
  BETA: 1.0
  BUDGET: 2000
  CUTMIX_PROB: 0.0
  CYCLES: 1
  INITIAL: 1800
  LA: False
  METHOD: random
  PRETRAINED_PATH: /home/deepankar/mnt/vision3_ckpts/vit_b_16-imagenet1k.pth
  TEMP: 1.0
  TRO: 1.0
  USE_PRETRAINED: True
  VAL: 200
AM:
  SGR: 1000
  THRESHOLD: 0.5
CUDNN_BENCHMARK: True
DS_SEED: 123
EDM:
  HASH: /home/harsh_s/scratch/msacopy/exp/hash/alpha.pt
  NUM_MODELS: 5
  PATH1: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T0.pt
  PATH2: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T1.pt
  PATH3: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T2.pt
  PATH4: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T3.pt
  PATH5: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T4.pt
GR:
  EPSILON: 0.1
  SUR_PATH: /home/harsh_s/scratch/msacopy/GRAD2/batch_training/outputs/trained_models/imagenet_cifar10_to_cifar10_surrogate_10epochs.pt
LOG_DEST: radformer_vitb16_1k_gbusv_240227_102656.txt
LOG_TIME: 240227_102656
METHOD_NAME: v1
OUT_DIR: /home/deepankar/scratch/MSA_Medical/results_deepankar
PL:
  CLASS_BLNC: 10
  ITERATIONS: 20
  KAPPA_N: 0.005
  KAPPA_P: 0.05
  NO_PROGRESS: False
  NO_UNCERTAINTY: False
  TAU_N: 0.05
  TAU_P: 0.7
  TEMP_NL: 2.0
RNG_SEED: 5
SAVE_DIR: /home/deepankar/scratch/MSA_Medical/results_deepankar/gbusg_radformer/GBUSV_vit_b_16_1k/SGD/2000_val200/random_v1
THIEF:
  ARCH: vit_b_16_1k
  DATASET: GBUSV
  DATA_ROOT: /home/deepankar/mnt/vision3_data_ckpts_msa_medical/GBUSV-Shared
  HARD_LABELS: True
  NUM_TRAIN: 1281167
  SUBSET: 128116
TRAIN:
  BATCH: 16
  EPOCH: 100
  GAMMA: 0.1
  LR: 0.001
  MILESTONES: [20, 40, 60, 80]
  MOMENTUM: 0.9
  OPTIMIZER: SGD
  WDECAY: 0.0005
VICTIM:
  ARCH: radformer
  DATASET: gbusg
  DATA_ROOT: /home/deepankar/mnt/vision3_data_ckpts_msa_medical/GBCU-Shared
  HEIGHT: 224
  PATH: /home/deepankar/mnt/vision3_data_ckpts_msa_medical/victim_models/radformer/radformer.pkl
  PATHIN: /home/harsh_s/scratch/msacopy/ckpts/cifar10-resnet32-inaccurate/checkpoint.pth.tar
  WIDTH: 224
Loaded target dataset of size 122 with 3 classes
target model keys:  695
checkpoint keys:  695
specificity = 72/80
sensitivity = 39/42

Target model acc = 0.9016393442622951
Val-Acc: 0.9016 Val-Spec: 0.9000 Val-Sens: 0.9286
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
replacing labeled set labels with victim labels
  0%|          | 0/113 [00:00<?, ?it/s]  1%|          | 1/113 [00:00<00:57,  1.94it/s]  2%|▏         | 2/113 [00:00<00:36,  3.08it/s]  3%|▎         | 3/113 [00:00<00:28,  3.84it/s]  4%|▎         | 4/113 [00:01<00:27,  4.03it/s]  4%|▍         | 5/113 [00:01<00:24,  4.40it/s]  5%|▌         | 6/113 [00:01<00:22,  4.71it/s]  6%|▌         | 7/113 [00:01<00:22,  4.79it/s]  7%|▋         | 8/113 [00:01<00:21,  4.96it/s]  8%|▊         | 9/113 [00:02<00:20,  5.02it/s]  9%|▉         | 10/113 [00:02<00:20,  5.11it/s] 10%|▉         | 11/113 [00:02<00:19,  5.19it/s] 11%|█         | 12/113 [00:02<00:19,  5.26it/s] 12%|█▏        | 13/113 [00:02<00:18,  5.29it/s] 12%|█▏        | 14/113 [00:03<00:18,  5.21it/s] 13%|█▎        | 15/113 [00:03<00:18,  5.23it/s] 14%|█▍        | 16/113 [00:03<00:18,  5.24it/s] 15%|█▌        | 17/113 [00:03<00:18,  5.26it/s] 16%|█▌        | 18/113 [00:03<00:18,  5.22it/s] 17%|█▋        | 19/113 [00:03<00:17,  5.23it/s] 18%|█▊        | 20/113 [00:04<00:17,  5.19it/s] 19%|█▊        | 21/113 [00:04<00:17,  5.23it/s] 19%|█▉        | 22/113 [00:04<00:17,  5.25it/s] 20%|██        | 23/113 [00:04<00:17,  5.22it/s] 21%|██        | 24/113 [00:04<00:17,  5.22it/s] 22%|██▏       | 25/113 [00:05<00:16,  5.25it/s] 23%|██▎       | 26/113 [00:05<00:16,  5.21it/s] 24%|██▍       | 27/113 [00:05<00:16,  5.24it/s] 25%|██▍       | 28/113 [00:05<00:16,  5.26it/s] 26%|██▌       | 29/113 [00:05<00:16,  5.00it/s] 27%|██▋       | 30/113 [00:06<00:16,  5.08it/s] 27%|██▋       | 31/113 [00:06<00:15,  5.15it/s] 28%|██▊       | 32/113 [00:06<00:15,  5.16it/s] 29%|██▉       | 33/113 [00:06<00:15,  5.16it/s] 30%|███       | 34/113 [00:06<00:15,  5.20it/s] 31%|███       | 35/113 [00:07<00:14,  5.25it/s] 32%|███▏      | 36/113 [00:07<00:14,  5.28it/s] 33%|███▎      | 37/113 [00:07<00:14,  5.28it/s] 34%|███▎      | 38/113 [00:07<00:14,  5.31it/s] 35%|███▍      | 39/113 [00:07<00:13,  5.29it/s] 35%|███▌      | 40/113 [00:07<00:13,  5.29it/s] 36%|███▋      | 41/113 [00:08<00:13,  5.31it/s] 37%|███▋      | 42/113 [00:08<00:13,  5.24it/s] 38%|███▊      | 43/113 [00:08<00:13,  5.29it/s] 39%|███▉      | 44/113 [00:08<00:13,  5.28it/s] 40%|███▉      | 45/113 [00:08<00:12,  5.25it/s] 41%|████      | 46/113 [00:09<00:12,  5.31it/s] 42%|████▏     | 47/113 [00:09<00:12,  5.30it/s] 42%|████▏     | 48/113 [00:09<00:12,  5.26it/s] 43%|████▎     | 49/113 [00:09<00:12,  5.23it/s] 44%|████▍     | 50/113 [00:09<00:11,  5.27it/s] 45%|████▌     | 51/113 [00:10<00:11,  5.19it/s] 46%|████▌     | 52/113 [00:10<00:11,  5.14it/s] 47%|████▋     | 53/113 [00:10<00:11,  5.17it/s] 48%|████▊     | 54/113 [00:10<00:11,  5.27it/s] 49%|████▊     | 55/113 [00:10<00:10,  5.28it/s] 50%|████▉     | 56/113 [00:11<00:10,  5.29it/s] 50%|█████     | 57/113 [00:11<00:10,  5.23it/s] 51%|█████▏    | 58/113 [00:11<00:10,  5.25it/s] 52%|█████▏    | 59/113 [00:11<00:10,  5.25it/s] 53%|█████▎    | 60/113 [00:11<00:10,  5.28it/s] 54%|█████▍    | 61/113 [00:11<00:09,  5.32it/s] 55%|█████▍    | 62/113 [00:12<00:09,  5.31it/s] 56%|█████▌    | 63/113 [00:12<00:09,  5.28it/s] 57%|█████▋    | 64/113 [00:12<00:09,  5.34it/s] 58%|█████▊    | 65/113 [00:12<00:09,  5.28it/s] 58%|█████▊    | 66/113 [00:12<00:08,  5.28it/s] 59%|█████▉    | 67/113 [00:13<00:08,  5.31it/s] 60%|██████    | 68/113 [00:13<00:08,  5.34it/s] 61%|██████    | 69/113 [00:13<00:08,  5.30it/s] 62%|██████▏   | 70/113 [00:13<00:08,  5.31it/s] 63%|██████▎   | 71/113 [00:13<00:07,  5.28it/s] 64%|██████▎   | 72/113 [00:14<00:07,  5.32it/s] 65%|██████▍   | 73/113 [00:14<00:07,  5.30it/s] 65%|██████▌   | 74/113 [00:14<00:07,  5.27it/s] 66%|██████▋   | 75/113 [00:14<00:07,  5.27it/s] 67%|██████▋   | 76/113 [00:14<00:07,  5.28it/s] 68%|██████▊   | 77/113 [00:15<00:06,  5.27it/s] 69%|██████▉   | 78/113 [00:15<00:06,  5.29it/s] 70%|██████▉   | 79/113 [00:15<00:06,  5.33it/s] 71%|███████   | 80/113 [00:15<00:06,  5.24it/s] 72%|███████▏  | 81/113 [00:15<00:06,  5.25it/s] 73%|███████▎  | 82/113 [00:15<00:05,  5.29it/s] 73%|███████▎  | 83/113 [00:16<00:05,  5.29it/s] 74%|███████▍  | 84/113 [00:16<00:05,  4.86it/s] 75%|███████▌  | 85/113 [00:16<00:05,  4.84it/s] 76%|███████▌  | 86/113 [00:16<00:05,  4.89it/s] 77%|███████▋  | 87/113 [00:16<00:05,  4.98it/s] 78%|███████▊  | 88/113 [00:17<00:05,  4.80it/s] 79%|███████▉  | 89/113 [00:17<00:04,  4.90it/s] 80%|███████▉  | 90/113 [00:17<00:04,  5.00it/s] 81%|████████  | 91/113 [00:17<00:04,  5.13it/s] 81%|████████▏ | 92/113 [00:18<00:04,  4.92it/s] 82%|████████▏ | 93/113 [00:18<00:04,  4.98it/s] 83%|████████▎ | 94/113 [00:18<00:03,  4.93it/s] 84%|████████▍ | 95/113 [00:18<00:03,  5.08it/s] 85%|████████▍ | 96/113 [00:18<00:03,  4.84it/s] 86%|████████▌ | 97/113 [00:19<00:03,  4.97it/s] 87%|████████▋ | 98/113 [00:19<00:02,  5.10it/s] 88%|████████▊ | 99/113 [00:19<00:02,  5.17it/s] 88%|████████▊ | 100/113 [00:19<00:02,  4.92it/s] 89%|████████▉ | 101/113 [00:19<00:02,  5.03it/s] 90%|█████████ | 102/113 [00:19<00:02,  5.11it/s] 91%|█████████ | 103/113 [00:20<00:01,  5.13it/s] 92%|█████████▏| 104/113 [00:20<00:01,  4.91it/s] 93%|█████████▎| 105/113 [00:20<00:01,  5.00it/s] 94%|█████████▍| 106/113 [00:20<00:01,  5.10it/s] 95%|█████████▍| 107/113 [00:20<00:01,  5.12it/s] 96%|█████████▌| 108/113 [00:21<00:00,  5.17it/s] 96%|█████████▋| 109/113 [00:21<00:00,  5.22it/s] 97%|█████████▋| 110/113 [00:21<00:00,  5.25it/s] 98%|█████████▊| 111/113 [00:21<00:00,  5.27it/s] 99%|█████████▉| 112/113 [00:21<00:00,  5.28it/s]100%|██████████| 113/113 [00:22<00:00,  5.12it/s]100%|██████████| 113/113 [00:22<00:00,  5.10it/s]
replacing val labels with victim labels
  0%|          | 0/13 [00:00<?, ?it/s]  8%|▊         | 1/13 [00:00<00:05,  2.38it/s] 15%|█▌        | 2/13 [00:00<00:03,  3.50it/s] 23%|██▎       | 3/13 [00:00<00:02,  4.09it/s] 31%|███       | 4/13 [00:00<00:02,  4.50it/s] 38%|███▊      | 5/13 [00:01<00:01,  4.71it/s] 46%|████▌     | 6/13 [00:01<00:01,  4.89it/s] 54%|█████▍    | 7/13 [00:01<00:01,  5.01it/s] 62%|██████▏   | 8/13 [00:01<00:00,  5.06it/s] 69%|██████▉   | 9/13 [00:01<00:00,  5.10it/s] 77%|███████▋  | 10/13 [00:02<00:00,  5.16it/s] 85%|████████▍ | 11/13 [00:02<00:00,  5.21it/s] 92%|█████████▏| 12/13 [00:02<00:00,  5.24it/s]100%|██████████| 13/13 [00:02<00:00,  4.90it/s]Validation set distribution: 
Number of samples  200

{0: 26, 1: 38, 2: 136}
Labeled set distribution: 
Number of samples  1800
{0: 197, 1: 443, 2: 1160}
odict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_0.mlp.0.weight', 'encoder.layers.encoder_layer_0.mlp.0.bias', 'encoder.layers.encoder_layer_0.mlp.3.weight', 'encoder.layers.encoder_layer_0.mlp.3.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_1.mlp.0.weight', 'encoder.layers.encoder_layer_1.mlp.0.bias', 'encoder.layers.encoder_layer_1.mlp.3.weight', 'encoder.layers.encoder_layer_1.mlp.3.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_2.mlp.0.weight', 'encoder.layers.encoder_layer_2.mlp.0.bias', 'encoder.layers.encoder_layer_2.mlp.3.weight', 'encoder.layers.encoder_layer_2.mlp.3.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_3.mlp.0.weight', 'encoder.layers.encoder_layer_3.mlp.0.bias', 'encoder.layers.encoder_layer_3.mlp.3.weight', 'encoder.layers.encoder_layer_3.mlp.3.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_4.mlp.0.weight', 'encoder.layers.encoder_layer_4.mlp.0.bias', 'encoder.layers.encoder_layer_4.mlp.3.weight', 'encoder.layers.encoder_layer_4.mlp.3.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_5.mlp.0.weight', 'encoder.layers.encoder_layer_5.mlp.0.bias', 'encoder.layers.encoder_layer_5.mlp.3.weight', 'encoder.layers.encoder_layer_5.mlp.3.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_6.mlp.0.weight', 'encoder.layers.encoder_layer_6.mlp.0.bias', 'encoder.layers.encoder_layer_6.mlp.3.weight', 'encoder.layers.encoder_layer_6.mlp.3.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_7.mlp.0.weight', 'encoder.layers.encoder_layer_7.mlp.0.bias', 'encoder.layers.encoder_layer_7.mlp.3.weight', 'encoder.layers.encoder_layer_7.mlp.3.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_8.mlp.0.weight', 'encoder.layers.encoder_layer_8.mlp.0.bias', 'encoder.layers.encoder_layer_8.mlp.3.weight', 'encoder.layers.encoder_layer_8.mlp.3.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_9.mlp.0.weight', 'encoder.layers.encoder_layer_9.mlp.0.bias', 'encoder.layers.encoder_layer_9.mlp.3.weight', 'encoder.layers.encoder_layer_9.mlp.3.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_10.mlp.0.weight', 'encoder.layers.encoder_layer_10.mlp.0.bias', 'encoder.layers.encoder_layer_10.mlp.3.weight', 'encoder.layers.encoder_layer_10.mlp.3.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.layers.encoder_layer_11.mlp.0.weight', 'encoder.layers.encoder_layer_11.mlp.0.bias', 'encoder.layers.encoder_layer_11.mlp.3.weight', 'encoder.layers.encoder_layer_11.mlp.3.bias', 'encoder.ln.weight', 'encoder.ln.bias', 'heads.head.weight', 'heads.head.bias'])
thief state:  None
pretrained state:  dict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.ln.weight', 'encoder.ln.bias'])
Thief model initialized successfully
specificity = 80/80
sensitivity = 0/42
Initial model on target dataset: acc = 0.2541, agreement = 0.2377, f1 = 0.1351, spec = 1.0000, sens = 0.0000
specificity = 64/64
sensitivity = 0/136
Initial model on validation dataset: acc = 0.1300, agreement = 0.1300, f1 = 0.0767, spec = 1.0000, sens = 0.0000
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:19<32:29, 19.69s/it]  2%|▏         | 2/100 [00:41<33:45, 20.67s/it]  3%|▎         | 3/100 [01:00<32:46, 20.27s/it]  4%|▍         | 4/100 [01:21<32:48, 20.50s/it]  5%|▌         | 5/100 [01:50<36:57, 23.34s/it]Epoch 5: Train acc/f1 = 0.6806 / 0.3869 / 0.2422 / 0.9569 
                Val acc/f1/spec/sens = 0.7250 / 0.4150 / 0.2188 / 0.9853
                Test acc/f1/spec/sens = 0.3443 / 0.1728 / 0.0250 / 1.0000
  6%|▌         | 6/100 [02:10<35:13, 22.49s/it]  7%|▋         | 7/100 [02:30<33:29, 21.61s/it]  8%|▊         | 8/100 [02:51<32:46, 21.37s/it]  9%|▉         | 9/100 [03:11<31:40, 20.89s/it] 10%|█         | 10/100 [03:40<35:20, 23.56s/it]Epoch 10: Train acc/f1 = 0.6772 / 0.3823 / 0.1641 / 0.9879 
                Val acc/f1/spec/sens = 0.7150 / 0.4053 / 0.1406 / 1.0000
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
 11%|█         | 11/100 [04:00<33:13, 22.40s/it] 12%|█▏        | 12/100 [04:22<32:30, 22.17s/it] 13%|█▎        | 13/100 [04:42<31:06, 21.45s/it] 14%|█▍        | 14/100 [05:03<30:29, 21.28s/it] 15%|█▌        | 15/100 [05:31<33:11, 23.43s/it]Epoch 15: Train acc/f1 = 0.7061 / 0.4324 / 0.4266 / 0.9379 
                Val acc/f1/spec/sens = 0.7450 / 0.4439 / 0.3125 / 0.9926
                Test acc/f1/spec/sens = 0.3689 / 0.2220 / 0.0875 / 0.9762
 16%|█▌        | 16/100 [05:52<31:43, 22.66s/it] 17%|█▋        | 17/100 [06:12<30:10, 21.81s/it] 18%|█▊        | 18/100 [06:33<29:44, 21.76s/it] 19%|█▉        | 19/100 [06:53<28:35, 21.18s/it] 20%|██        | 20/100 [07:23<31:36, 23.70s/it]Epoch 20: Train acc/f1 = 0.7256 / 0.5284 / 0.4172 / 0.9500 
                Val acc/f1/spec/sens = 0.7650 / 0.5413 / 0.4062 / 0.9926
                Test acc/f1/spec/sens = 0.3525 / 0.2089 / 0.1000 / 0.9762
 21%|██        | 21/100 [07:43<29:40, 22.54s/it] 22%|██▏       | 22/100 [08:04<28:55, 22.25s/it] 23%|██▎       | 23/100 [08:24<27:36, 21.52s/it] 24%|██▍       | 24/100 [08:46<27:18, 21.56s/it] 25%|██▌       | 25/100 [09:14<29:35, 23.67s/it]Epoch 25: Train acc/f1 = 0.7361 / 0.5588 / 0.6047 / 0.9043 
                Val acc/f1/spec/sens = 0.8050 / 0.6667 / 0.6719 / 0.9412
                Test acc/f1/spec/sens = 0.4016 / 0.2669 / 0.2250 / 0.9762
 26%|██▌       | 26/100 [09:36<28:24, 23.03s/it] 27%|██▋       | 27/100 [09:56<26:50, 22.07s/it] 28%|██▊       | 28/100 [10:16<26:04, 21.73s/it] 29%|██▉       | 29/100 [10:36<25:02, 21.17s/it] 30%|███       | 30/100 [11:06<27:43, 23.77s/it]Epoch 30: Train acc/f1 = 0.7394 / 0.5845 / 0.4813 / 0.9526 
                Val acc/f1/spec/sens = 0.7900 / 0.6372 / 0.4844 / 0.9779
                Test acc/f1/spec/sens = 0.3607 / 0.2122 / 0.1000 / 1.0000
 31%|███       | 31/100 [11:26<25:59, 22.60s/it] 32%|███▏      | 32/100 [11:48<25:19, 22.35s/it] 33%|███▎      | 33/100 [12:08<24:08, 21.62s/it] 34%|███▍      | 34/100 [12:29<23:32, 21.41s/it] 35%|███▌      | 35/100 [12:57<25:31, 23.56s/it]Epoch 35: Train acc/f1 = 0.7539 / 0.6086 / 0.5813 / 0.9345 
                Val acc/f1/spec/sens = 0.8000 / 0.6519 / 0.5312 / 0.9779
                Test acc/f1/spec/sens = 0.3770 / 0.2417 / 0.1750 / 0.9524
 36%|███▌      | 36/100 [13:19<24:28, 22.95s/it] 37%|███▋      | 37/100 [13:39<23:07, 22.02s/it] 38%|███▊      | 38/100 [14:00<22:26, 21.72s/it] 39%|███▉      | 39/100 [14:20<21:31, 21.17s/it] 40%|████      | 40/100 [14:50<23:58, 23.98s/it]Epoch 40: Train acc/f1 = 0.7611 / 0.6365 / 0.6344 / 0.9147 
                Val acc/f1/spec/sens = 0.8450 / 0.7465 / 0.7031 / 0.9706
                Test acc/f1/spec/sens = 0.3689 / 0.2505 / 0.2125 / 0.9286
 41%|████      | 41/100 [15:10<22:21, 22.74s/it] 42%|████▏     | 42/100 [15:32<21:41, 22.44s/it] 43%|████▎     | 43/100 [15:51<20:34, 21.65s/it] 44%|████▍     | 44/100 [16:12<20:02, 21.47s/it] 45%|████▌     | 45/100 [16:41<21:39, 23.63s/it]Epoch 45: Train acc/f1 = 0.7739 / 0.6516 / 0.6984 / 0.9017 
                Val acc/f1/spec/sens = 0.8450 / 0.7505 / 0.7188 / 0.9559
                Test acc/f1/spec/sens = 0.3852 / 0.2768 / 0.2750 / 0.9048
 46%|████▌     | 46/100 [17:02<20:32, 22.83s/it] 47%|████▋     | 47/100 [17:22<19:22, 21.94s/it] 48%|████▊     | 48/100 [17:43<18:47, 21.68s/it] 49%|████▉     | 49/100 [18:03<17:57, 21.13s/it] 50%|█████     | 50/100 [18:33<19:51, 23.84s/it]Epoch 50: Train acc/f1 = 0.7772 / 0.6646 / 0.6484 / 0.9190 
                Val acc/f1/spec/sens = 0.8450 / 0.7576 / 0.7188 / 0.9485
                Test acc/f1/spec/sens = 0.3852 / 0.2765 / 0.2625 / 0.9048
 51%|█████     | 51/100 [18:53<18:29, 22.64s/it] 52%|█████▏    | 52/100 [19:14<17:44, 22.18s/it] 53%|█████▎    | 53/100 [19:34<16:50, 21.50s/it] 54%|█████▍    | 54/100 [19:55<16:22, 21.37s/it] 55%|█████▌    | 55/100 [20:24<17:48, 23.74s/it]Epoch 55: Train acc/f1 = 0.7711 / 0.6537 / 0.6469 / 0.9138 
                Val acc/f1/spec/sens = 0.8550 / 0.7683 / 0.7031 / 0.9632
                Test acc/f1/spec/sens = 0.3852 / 0.2636 / 0.2750 / 0.9048
 56%|█████▌    | 56/100 [20:45<16:47, 22.91s/it] 57%|█████▋    | 57/100 [21:05<15:45, 22.00s/it] 58%|█████▊    | 58/100 [21:26<15:12, 21.73s/it] 59%|█████▉    | 59/100 [21:46<14:28, 21.18s/it] 60%|██████    | 60/100 [22:17<15:58, 23.96s/it]Epoch 60: Train acc/f1 = 0.7628 / 0.6382 / 0.6266 / 0.9147 
                Val acc/f1/spec/sens = 0.8500 / 0.7661 / 0.7031 / 0.9559
                Test acc/f1/spec/sens = 0.3852 / 0.2644 / 0.2625 / 0.9048
 61%|██████    | 61/100 [22:36<14:46, 22.73s/it] 62%|██████▏   | 62/100 [22:57<14:05, 22.24s/it] 63%|██████▎   | 63/100 [23:17<13:16, 21.53s/it] 64%|██████▍   | 64/100 [23:38<12:49, 21.39s/it] 65%|██████▌   | 65/100 [24:07<13:47, 23.65s/it]Epoch 65: Train acc/f1 = 0.7572 / 0.6257 / 0.6406 / 0.9060 
                Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
                Test acc/f1/spec/sens = 0.3852 / 0.2644 / 0.2625 / 0.9048
 66%|██████▌   | 66/100 [24:28<12:56, 22.85s/it] 67%|██████▋   | 67/100 [24:48<12:04, 21.96s/it] 68%|██████▊   | 68/100 [25:09<11:34, 21.70s/it] 69%|██████▉   | 69/100 [25:29<10:55, 21.14s/it] 70%|███████   | 70/100 [26:00<11:57, 23.92s/it]Epoch 70: Train acc/f1 = 0.7722 / 0.6465 / 0.6625 / 0.9233 
                Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
                Test acc/f1/spec/sens = 0.3852 / 0.2631 / 0.2500 / 0.9048
 71%|███████   | 71/100 [26:19<10:58, 22.71s/it] 72%|███████▏  | 72/100 [26:41<10:22, 22.23s/it] 73%|███████▎  | 73/100 [27:00<09:41, 21.53s/it] 74%|███████▍  | 74/100 [27:21<09:15, 21.38s/it] 75%|███████▌  | 75/100 [27:51<09:53, 23.73s/it]Epoch 75: Train acc/f1 = 0.7644 / 0.6462 / 0.6422 / 0.9138 
                Val acc/f1/spec/sens = 0.8450 / 0.7616 / 0.7031 / 0.9485
                Test acc/f1/spec/sens = 0.3852 / 0.2630 / 0.2375 / 0.9048
 76%|███████▌  | 76/100 [28:12<09:10, 22.92s/it] 77%|███████▋  | 77/100 [28:32<08:26, 22.00s/it] 78%|███████▊  | 78/100 [28:53<07:58, 21.73s/it] 79%|███████▉  | 79/100 [29:13<07:24, 21.17s/it] 80%|████████  | 80/100 [29:43<07:59, 23.97s/it]Epoch 80: Train acc/f1 = 0.7622 / 0.6272 / 0.6531 / 0.9147 
                Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
                Test acc/f1/spec/sens = 0.3852 / 0.2631 / 0.2500 / 0.9048
 81%|████████  | 81/100 [30:03<07:12, 22.75s/it] 82%|████████▏ | 82/100 [30:24<06:40, 22.24s/it] 83%|████████▎ | 83/100 [30:44<06:06, 21.53s/it] 84%|████████▍ | 84/100 [31:05<05:42, 21.39s/it] 85%|████████▌ | 85/100 [31:34<05:56, 23.79s/it]Epoch 85: Train acc/f1 = 0.7694 / 0.6505 / 0.6531 / 0.9129 
                Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
                Test acc/f1/spec/sens = 0.3852 / 0.2631 / 0.2500 / 0.9048
 86%|████████▌ | 86/100 [31:55<05:21, 22.98s/it] 87%|████████▋ | 87/100 [32:15<04:46, 22.05s/it] 88%|████████▊ | 88/100 [32:36<04:21, 21.75s/it] 89%|████████▉ | 89/100 [32:56<03:53, 21.20s/it] 90%|█████████ | 90/100 [33:27<04:00, 24.02s/it]Epoch 90: Train acc/f1 = 0.7717 / 0.6373 / 0.6500 / 0.9241 
                Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
                Test acc/f1/spec/sens = 0.3852 / 0.2631 / 0.2500 / 0.9048
 91%|█████████ | 91/100 [33:47<03:25, 22.79s/it] 92%|█████████▏| 92/100 [34:08<02:58, 22.28s/it] 93%|█████████▎| 93/100 [34:28<02:30, 21.56s/it] 94%|█████████▍| 94/100 [34:49<02:08, 21.40s/it] 95%|█████████▌| 95/100 [35:18<01:59, 23.80s/it]Epoch 95: Train acc/f1 = 0.7756 / 0.6479 / 0.6687 / 0.9190 
                Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
                Test acc/f1/spec/sens = 0.3852 / 0.2631 / 0.2500 / 0.9048
 96%|█████████▌| 96/100 [35:39<01:31, 22.96s/it] 97%|█████████▋| 97/100 [35:59<01:06, 22.03s/it] 98%|█████████▊| 98/100 [36:20<00:43, 21.74s/it] 99%|█████████▉| 99/100 [36:40<00:21, 21.19s/it]100%|██████████| 100/100 [37:11<00:00, 24.04s/it]                                                 Epoch 100: Train acc/f1 = 0.7689 / 0.6422 / 0.6531 / 0.9138 
                Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
                Test acc/f1/spec/sens = 0.3852 / 0.2631 / 0.2500 / 0.9048
specificity = 419/640
sensitivity = 1066/1160
specificity = 45/64
sensitivity = 128/136
specificity = 20/80
sensitivity = 38/42
Trial 1, Cycle 1
Train acc/f1/spec/sens = 0.7744 / 0.6510 / 0.6547 / 0.9190
Val acc/f1/spec/sens = 0.8400 / 0.7571 / 0.7031 / 0.9412
Test acc/f1/spec/sens = 0.3852 / 0.2631 / 0.2500 / 0.9048
>> Finished.
specificity = 20/80
sensitivity = 38/42
Acc, agreement for latest model:  0.38524590163934425 0.4016393442622951
Load best checkpoint for thief model
specificity = 19/80
sensitivity = 38/42
Acc, agreement for best model:  0.3770491803278688 0.39344262295081966
Trial 0/5 || Cycle 1/1 || Label set size 1800 || Test acc 0.3770 || Test agreement 0.3934 || Spec 0.2375 || Sens 0.9048
**************************************************************************************************** 

specificity = 19/80
sensitivity = 38/42
Number of samples  1800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
replacing labeled set labels with victim labels
  0%|          | 0/113 [00:00<?, ?it/s]  1%|          | 1/113 [00:00<01:10,  1.58it/s]  2%|▏         | 2/113 [00:00<00:47,  2.33it/s]  3%|▎         | 3/113 [00:01<00:39,  2.79it/s]  4%|▎         | 4/113 [00:01<00:35,  3.06it/s]  4%|▍         | 5/113 [00:01<00:32,  3.28it/s]  5%|▌         | 6/113 [00:01<00:30,  3.48it/s]  6%|▌         | 7/113 [00:02<00:29,  3.57it/s]  7%|▋         | 8/113 [00:02<00:28,  3.67it/s]  8%|▊         | 9/113 [00:02<00:27,  3.72it/s]  9%|▉         | 10/113 [00:03<00:27,  3.73it/s] 10%|▉         | 11/113 [00:03<00:26,  3.79it/s] 11%|█         | 12/113 [00:03<00:26,  3.83it/s] 12%|█▏        | 13/113 [00:03<00:26,  3.82it/s] 12%|█▏        | 14/113 [00:04<00:25,  3.87it/s] 13%|█▎        | 15/113 [00:04<00:25,  3.91it/s] 14%|█▍        | 16/113 [00:04<00:24,  3.89it/s] 15%|█▌        | 17/113 [00:04<00:24,  3.89it/s] 16%|█▌        | 18/113 [00:05<00:24,  3.94it/s] 17%|█▋        | 19/113 [00:05<00:23,  3.97it/s] 18%|█▊        | 20/113 [00:05<00:24,  3.82it/s] 19%|█▊        | 21/113 [00:05<00:24,  3.77it/s] 19%|█▉        | 22/113 [00:06<00:24,  3.65it/s] 20%|██        | 23/113 [00:06<00:25,  3.53it/s] 21%|██        | 24/113 [00:06<00:24,  3.63it/s] 22%|██▏       | 25/113 [00:06<00:23,  3.68it/s] 23%|██▎       | 26/113 [00:07<00:23,  3.76it/s] 24%|██▍       | 27/113 [00:07<00:23,  3.73it/s] 25%|██▍       | 28/113 [00:07<00:24,  3.52it/s] 26%|██▌       | 29/113 [00:08<00:27,  3.11it/s] 27%|██▋       | 30/113 [00:08<00:27,  3.03it/s] 27%|██▋       | 31/113 [00:09<00:29,  2.79it/s] 28%|██▊       | 32/113 [00:09<00:27,  2.93it/s] 29%|██▉       | 33/113 [00:09<00:28,  2.77it/s] 30%|███       | 34/113 [00:10<00:28,  2.77it/s] 31%|███       | 35/113 [00:10<00:25,  3.02it/s] 32%|███▏      | 36/113 [00:10<00:24,  3.20it/s] 33%|███▎      | 37/113 [00:10<00:22,  3.41it/s] 34%|███▎      | 38/113 [00:11<00:21,  3.57it/s] 35%|███▍      | 39/113 [00:11<00:20,  3.64it/s] 35%|███▌      | 40/113 [00:11<00:19,  3.75it/s] 36%|███▋      | 41/113 [00:11<00:18,  3.83it/s] 37%|███▋      | 42/113 [00:12<00:18,  3.88it/s] 38%|███▊      | 43/113 [00:12<00:18,  3.84it/s] 39%|███▉      | 44/113 [00:12<00:17,  3.85it/s] 40%|███▉      | 45/113 [00:12<00:16,  4.13it/s] 41%|████      | 46/113 [00:13<00:15,  4.39it/s] 42%|████▏     | 47/113 [00:13<00:14,  4.61it/s] 42%|████▏     | 48/113 [00:13<00:13,  4.74it/s] 43%|████▎     | 49/113 [00:13<00:13,  4.90it/s] 44%|████▍     | 50/113 [00:13<00:12,  5.02it/s] 45%|████▌     | 51/113 [00:14<00:12,  5.03it/s] 46%|████▌     | 52/113 [00:14<00:12,  5.02it/s] 47%|████▋     | 53/113 [00:14<00:12,  4.95it/s] 48%|████▊     | 54/113 [00:14<00:11,  4.98it/s] 49%|████▊     | 55/113 [00:14<00:11,  5.01it/s] 50%|████▉     | 56/113 [00:15<00:11,  5.10it/s] 50%|█████     | 57/113 [00:15<00:11,  5.09it/s] 51%|█████▏    | 58/113 [00:15<00:11,  4.72it/s] 52%|█████▏    | 59/113 [00:15<00:11,  4.52it/s] 53%|█████▎    | 60/113 [00:15<00:11,  4.44it/s] 54%|█████▍    | 61/113 [00:16<00:11,  4.68it/s] 55%|█████▍    | 62/113 [00:16<00:10,  4.84it/s] 56%|█████▌    | 63/113 [00:16<00:10,  4.91it/s] 57%|█████▋    | 64/113 [00:16<00:10,  4.46it/s] 58%|█████▊    | 65/113 [00:16<00:10,  4.49it/s] 58%|█████▊    | 66/113 [00:17<00:10,  4.68it/s] 59%|█████▉    | 67/113 [00:17<00:09,  4.84it/s] 60%|██████    | 68/113 [00:17<00:09,  4.97it/s] 61%|██████    | 69/113 [00:17<00:10,  4.14it/s] 62%|██████▏   | 70/113 [00:18<00:11,  3.75it/s] 63%|██████▎   | 71/113 [00:18<00:11,  3.69it/s] 64%|██████▎   | 72/113 [00:18<00:11,  3.62it/s] 65%|██████▍   | 73/113 [00:19<00:11,  3.59it/s] 65%|██████▌   | 74/113 [00:19<00:10,  3.69it/s] 66%|██████▋   | 75/113 [00:19<00:10,  3.74it/s] 67%|██████▋   | 76/113 [00:19<00:10,  3.68it/s] 68%|██████▊   | 77/113 [00:20<00:10,  3.50it/s] 69%|██████▉   | 78/113 [00:20<00:10,  3.47it/s] 70%|██████▉   | 79/113 [00:20<00:09,  3.50it/s] 71%|███████   | 80/113 [00:21<00:10,  3.22it/s] 72%|███████▏  | 81/113 [00:21<00:09,  3.23it/s] 73%|███████▎  | 82/113 [00:21<00:09,  3.22it/s] 73%|███████▎  | 83/113 [00:22<00:09,  3.24it/s] 74%|███████▍  | 84/113 [00:22<00:08,  3.29it/s] 75%|███████▌  | 85/113 [00:22<00:08,  3.46it/s] 76%|███████▌  | 86/113 [00:22<00:07,  3.56it/s] 77%|███████▋  | 87/113 [00:23<00:07,  3.66it/s] 78%|███████▊  | 88/113 [00:23<00:06,  3.71it/s] 79%|███████▉  | 89/113 [00:23<00:06,  3.73it/s] 80%|███████▉  | 90/113 [00:23<00:06,  3.77it/s] 81%|████████  | 91/113 [00:24<00:05,  3.79it/s] 81%|████████▏ | 92/113 [00:24<00:05,  3.82it/s] 82%|████████▏ | 93/113 [00:24<00:05,  3.86it/s] 83%|████████▎ | 94/113 [00:24<00:04,  3.88it/s] 84%|████████▍ | 95/113 [00:25<00:04,  3.67it/s] 85%|████████▍ | 96/113 [00:25<00:04,  3.76it/s] 86%|████████▌ | 97/113 [00:25<00:04,  3.27it/s] 87%|████████▋ | 98/113 [00:26<00:04,  3.13it/s] 88%|████████▊ | 99/113 [00:26<00:04,  3.27it/s] 88%|████████▊ | 100/113 [00:26<00:04,  3.23it/s] 89%|████████▉ | 101/113 [00:27<00:03,  3.05it/s] 90%|█████████ | 102/113 [00:27<00:03,  3.01it/s] 91%|█████████ | 103/113 [00:27<00:03,  3.07it/s] 92%|█████████▏| 104/113 [00:28<00:03,  2.90it/s] 93%|█████████▎| 105/113 [00:28<00:02,  3.08it/s] 94%|█████████▍| 106/113 [00:28<00:02,  3.04it/s] 95%|█████████▍| 107/113 [00:29<00:01,  3.01it/s] 96%|█████████▌| 108/113 [00:29<00:01,  3.05it/s] 96%|█████████▋| 109/113 [00:29<00:01,  3.15it/s] 97%|█████████▋| 110/113 [00:30<00:00,  3.22it/s] 98%|█████████▊| 111/113 [00:30<00:00,  3.38it/s] 99%|█████████▉| 112/113 [00:30<00:00,  3.42it/s]100%|██████████| 113/113 [00:30<00:00,  3.89it/s]100%|██████████| 113/113 [00:30<00:00,  3.65it/s]
replacing val labels with victim labels
  0%|          | 0/13 [00:00<?, ?it/s]  8%|▊         | 1/13 [00:00<00:09,  1.32it/s] 15%|█▌        | 2/13 [00:01<00:05,  2.05it/s] 23%|██▎       | 3/13 [00:01<00:04,  2.38it/s] 31%|███       | 4/13 [00:01<00:03,  2.76it/s] 38%|███▊      | 5/13 [00:01<00:02,  2.86it/s] 46%|████▌     | 6/13 [00:02<00:02,  2.94it/s] 54%|█████▍    | 7/13 [00:02<00:01,  3.08it/s] 62%|██████▏   | 8/13 [00:02<00:01,  3.10it/s] 69%|██████▉   | 9/13 [00:03<00:01,  3.30it/s] 77%|███████▋  | 10/13 [00:03<00:00,  3.18it/s] 85%|████████▍ | 11/13 [00:03<00:00,  3.00it/s] 92%|█████████▏| 12/13 [00:04<00:00,  2.95it/s]100%|██████████| 13/13 [00:04<00:00,  3.61it/s]100%|██████████| 13/13 [00:04<00:00,  2.90it/s]Validation set distribution: 
Number of samples  200

{0: 29, 1: 48, 2: 123}
Labeled set distribution: 
Number of samples  1800
{0: 219, 1: 415, 2: 1166}
odict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_0.mlp.0.weight', 'encoder.layers.encoder_layer_0.mlp.0.bias', 'encoder.layers.encoder_layer_0.mlp.3.weight', 'encoder.layers.encoder_layer_0.mlp.3.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_1.mlp.0.weight', 'encoder.layers.encoder_layer_1.mlp.0.bias', 'encoder.layers.encoder_layer_1.mlp.3.weight', 'encoder.layers.encoder_layer_1.mlp.3.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_2.mlp.0.weight', 'encoder.layers.encoder_layer_2.mlp.0.bias', 'encoder.layers.encoder_layer_2.mlp.3.weight', 'encoder.layers.encoder_layer_2.mlp.3.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_3.mlp.0.weight', 'encoder.layers.encoder_layer_3.mlp.0.bias', 'encoder.layers.encoder_layer_3.mlp.3.weight', 'encoder.layers.encoder_layer_3.mlp.3.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_4.mlp.0.weight', 'encoder.layers.encoder_layer_4.mlp.0.bias', 'encoder.layers.encoder_layer_4.mlp.3.weight', 'encoder.layers.encoder_layer_4.mlp.3.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_5.mlp.0.weight', 'encoder.layers.encoder_layer_5.mlp.0.bias', 'encoder.layers.encoder_layer_5.mlp.3.weight', 'encoder.layers.encoder_layer_5.mlp.3.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_6.mlp.0.weight', 'encoder.layers.encoder_layer_6.mlp.0.bias', 'encoder.layers.encoder_layer_6.mlp.3.weight', 'encoder.layers.encoder_layer_6.mlp.3.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_7.mlp.0.weight', 'encoder.layers.encoder_layer_7.mlp.0.bias', 'encoder.layers.encoder_layer_7.mlp.3.weight', 'encoder.layers.encoder_layer_7.mlp.3.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_8.mlp.0.weight', 'encoder.layers.encoder_layer_8.mlp.0.bias', 'encoder.layers.encoder_layer_8.mlp.3.weight', 'encoder.layers.encoder_layer_8.mlp.3.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_9.mlp.0.weight', 'encoder.layers.encoder_layer_9.mlp.0.bias', 'encoder.layers.encoder_layer_9.mlp.3.weight', 'encoder.layers.encoder_layer_9.mlp.3.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_10.mlp.0.weight', 'encoder.layers.encoder_layer_10.mlp.0.bias', 'encoder.layers.encoder_layer_10.mlp.3.weight', 'encoder.layers.encoder_layer_10.mlp.3.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.layers.encoder_layer_11.mlp.0.weight', 'encoder.layers.encoder_layer_11.mlp.0.bias', 'encoder.layers.encoder_layer_11.mlp.3.weight', 'encoder.layers.encoder_layer_11.mlp.3.bias', 'encoder.ln.weight', 'encoder.ln.bias', 'heads.head.weight', 'heads.head.bias'])
thief state:  None
pretrained state:  dict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.ln.weight', 'encoder.ln.bias'])
Thief model initialized successfully
specificity = 80/80
sensitivity = 0/42
Initial model on target dataset: acc = 0.2541, agreement = 0.2377, f1 = 0.1351, spec = 1.0000, sens = 0.0000
specificity = 77/77
sensitivity = 0/123
Initial model on validation dataset: acc = 0.1450, agreement = 0.1450, f1 = 0.0844, spec = 1.0000, sens = 0.0000
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:19<32:42, 19.82s/it]  2%|▏         | 2/100 [00:41<34:13, 20.95s/it]  3%|▎         | 3/100 [01:01<33:09, 20.51s/it]  4%|▍         | 4/100 [01:23<33:33, 20.97s/it]  5%|▌         | 5/100 [01:52<37:58, 23.98s/it]Epoch 5: Train acc/f1 = 0.6794 / 0.3672 / 0.1435 / 0.9914 
                Val acc/f1/spec/sens = 0.6500 / 0.4205 / 0.3117 / 0.9593
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
  6%|▌         | 6/100 [02:13<35:59, 22.97s/it]  7%|▋         | 7/100 [02:33<34:02, 21.96s/it]  8%|▊         | 8/100 [02:55<33:33, 21.89s/it]  9%|▉         | 9/100 [03:15<32:14, 21.26s/it] 10%|█         | 10/100 [03:45<36:12, 24.14s/it]Epoch 10: Train acc/f1 = 0.7094 / 0.5585 / 0.5473 / 0.8979 
                Val acc/f1/spec/sens = 0.7150 / 0.5565 / 0.5325 / 0.9593
                Test acc/f1/spec/sens = 0.3770 / 0.2428 / 0.0750 / 1.0000
 11%|█         | 11/100 [04:05<33:51, 22.83s/it] 12%|█▏        | 12/100 [04:26<32:40, 22.28s/it] 13%|█▎        | 13/100 [04:46<31:15, 21.55s/it] 14%|█▍        | 14/100 [05:07<30:51, 21.52s/it] 15%|█▌        | 15/100 [05:37<33:46, 23.85s/it]Epoch 15: Train acc/f1 = 0.7094 / 0.4771 / 0.4196 / 0.9374 
                Val acc/f1/spec/sens = 0.7050 / 0.5797 / 0.5974 / 0.9024
                Test acc/f1/spec/sens = 0.3525 / 0.1876 / 0.0500 / 1.0000
 16%|█▌        | 16/100 [05:58<32:11, 22.99s/it] 17%|█▋        | 17/100 [06:17<30:30, 22.06s/it] 18%|█▊        | 18/100 [06:39<30:01, 21.96s/it] 19%|█▉        | 19/100 [06:59<28:48, 21.34s/it] 20%|██        | 20/100 [07:29<32:00, 24.00s/it]Epoch 20: Train acc/f1 = 0.7394 / 0.5850 / 0.5315 / 0.9383 
                Val acc/f1/spec/sens = 0.7350 / 0.5969 / 0.5455 / 0.9593
                Test acc/f1/spec/sens = 0.3607 / 0.2377 / 0.1125 / 0.9524
 21%|██        | 21/100 [07:49<29:59, 22.77s/it] 22%|██▏       | 22/100 [08:10<28:55, 22.26s/it] 23%|██▎       | 23/100 [08:30<27:40, 21.56s/it] 24%|██▍       | 24/100 [08:52<27:21, 21.60s/it] 25%|██▌       | 25/100 [09:21<29:53, 23.91s/it]Epoch 25: Train acc/f1 = 0.7433 / 0.5903 / 0.5394 / 0.9383 
                Val acc/f1/spec/sens = 0.7750 / 0.6900 / 0.7403 / 0.9106
                Test acc/f1/spec/sens = 0.3852 / 0.2576 / 0.1875 / 0.9762
 26%|██▌       | 26/100 [09:42<28:25, 23.05s/it] 27%|██▋       | 27/100 [10:02<26:53, 22.10s/it] 28%|██▊       | 28/100 [10:24<26:23, 21.99s/it] 29%|██▉       | 29/100 [10:44<25:16, 21.36s/it] 30%|███       | 30/100 [11:14<28:05, 24.08s/it]Epoch 30: Train acc/f1 = 0.7633 / 0.6345 / 0.6719 / 0.9134 
                Val acc/f1/spec/sens = 0.7900 / 0.7044 / 0.7922 / 0.9106
                Test acc/f1/spec/sens = 0.4016 / 0.3230 / 0.2625 / 0.9048
 31%|███       | 31/100 [11:34<26:14, 22.82s/it] 32%|███▏      | 32/100 [11:56<25:33, 22.55s/it] 33%|███▎      | 33/100 [12:16<24:19, 21.78s/it] 34%|███▍      | 34/100 [12:37<23:42, 21.56s/it] 35%|███▌      | 35/100 [13:06<25:50, 23.85s/it]Epoch 35: Train acc/f1 = 0.7761 / 0.6588 / 0.6546 / 0.9348 
                Val acc/f1/spec/sens = 0.7900 / 0.7141 / 0.8052 / 0.8943
                Test acc/f1/spec/sens = 0.4180 / 0.3492 / 0.2375 / 0.9286
 36%|███▌      | 36/100 [13:28<24:42, 23.16s/it] 37%|███▋      | 37/100 [13:48<23:17, 22.18s/it] 38%|███▊      | 38/100 [14:09<22:34, 21.84s/it] 39%|███▉      | 39/100 [14:29<21:35, 21.23s/it] 40%|████      | 40/100 [14:59<23:56, 23.95s/it]Epoch 40: Train acc/f1 = 0.7783 / 0.6863 / 0.7760 / 0.8825 
                Val acc/f1/spec/sens = 0.7700 / 0.7005 / 0.8571 / 0.8537
                Test acc/f1/spec/sens = 0.4262 / 0.4162 / 0.5125 / 0.5952
 41%|████      | 41/100 [15:19<22:21, 22.73s/it] 42%|████▏     | 42/100 [15:40<21:28, 22.22s/it] 43%|████▎     | 43/100 [16:00<20:26, 21.52s/it] 44%|████▍     | 44/100 [16:21<19:57, 21.38s/it] 45%|████▌     | 45/100 [16:50<21:43, 23.70s/it]Epoch 45: Train acc/f1 = 0.7828 / 0.6790 / 0.7114 / 0.9125 
                Val acc/f1/spec/sens = 0.7950 / 0.7151 / 0.8052 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.3886 / 0.3625 / 0.7619
 46%|████▌     | 46/100 [17:11<20:35, 22.88s/it] 47%|████▋     | 47/100 [17:31<19:24, 21.98s/it] 48%|████▊     | 48/100 [17:52<18:48, 21.70s/it] 49%|████▉     | 49/100 [18:12<17:58, 21.15s/it] 50%|█████     | 50/100 [18:42<19:52, 23.85s/it]Epoch 50: Train acc/f1 = 0.7883 / 0.6851 / 0.7129 / 0.9228 
                Val acc/f1/spec/sens = 0.8050 / 0.7312 / 0.8182 / 0.9106
                Test acc/f1/spec/sens = 0.4426 / 0.4248 / 0.3625 / 0.7857
 51%|█████     | 51/100 [19:02<18:30, 22.65s/it] 52%|█████▏    | 52/100 [19:23<17:44, 22.17s/it] 53%|█████▎    | 53/100 [19:42<16:49, 21.48s/it] 54%|█████▍    | 54/100 [20:04<16:22, 21.36s/it] 55%|█████▌    | 55/100 [20:32<17:40, 23.57s/it]Epoch 55: Train acc/f1 = 0.7872 / 0.6698 / 0.7177 / 0.9271 
                Val acc/f1/spec/sens = 0.8000 / 0.7271 / 0.7922 / 0.9106
                Test acc/f1/spec/sens = 0.3934 / 0.3560 / 0.3750 / 0.7143
 56%|█████▌    | 56/100 [20:53<16:43, 22.81s/it] 57%|█████▋    | 57/100 [21:13<15:42, 21.92s/it] 58%|█████▊    | 58/100 [21:34<15:10, 21.67s/it] 59%|█████▉    | 59/100 [21:54<14:25, 21.12s/it] 60%|██████    | 60/100 [22:24<15:52, 23.82s/it]Epoch 60: Train acc/f1 = 0.8011 / 0.7095 / 0.7287 / 0.9202 
                Val acc/f1/spec/sens = 0.8000 / 0.7202 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.4094 / 0.4625 / 0.6190
 61%|██████    | 61/100 [22:44<14:42, 22.63s/it] 62%|██████▏   | 62/100 [23:05<14:02, 22.17s/it] 63%|██████▎   | 63/100 [23:25<13:14, 21.47s/it] 64%|██████▍   | 64/100 [23:46<12:48, 21.35s/it] 65%|██████▌   | 65/100 [24:15<13:50, 23.72s/it]Epoch 65: Train acc/f1 = 0.7922 / 0.6931 / 0.7382 / 0.9160 
                Val acc/f1/spec/sens = 0.8000 / 0.7202 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4016 / 0.3968 / 0.4250 / 0.6429
 66%|██████▌   | 66/100 [24:36<12:59, 22.92s/it] 67%|██████▋   | 67/100 [24:56<12:05, 22.00s/it] 68%|██████▊   | 68/100 [25:17<11:35, 21.72s/it] 69%|██████▉   | 69/100 [25:37<10:56, 21.17s/it] 70%|███████   | 70/100 [26:08<11:58, 23.95s/it]Epoch 70: Train acc/f1 = 0.7956 / 0.6898 / 0.7192 / 0.9271 
                Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.3934 / 0.3912 / 0.4250 / 0.6190
 71%|███████   | 71/100 [26:27<10:58, 22.72s/it] 72%|███████▏  | 72/100 [26:49<10:22, 22.24s/it] 73%|███████▎  | 73/100 [27:08<09:41, 21.53s/it] 74%|███████▍  | 74/100 [27:30<09:16, 21.40s/it] 75%|███████▌  | 75/100 [27:59<09:53, 23.75s/it]Epoch 75: Train acc/f1 = 0.7917 / 0.6930 / 0.7429 / 0.9134 
                Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.4142 / 0.4625 / 0.6190
 76%|███████▌  | 76/100 [28:20<09:10, 22.93s/it] 77%|███████▋  | 77/100 [28:40<08:26, 22.01s/it] 78%|███████▊  | 78/100 [29:01<07:58, 21.74s/it] 79%|███████▉  | 79/100 [29:21<07:24, 21.17s/it] 80%|████████  | 80/100 [29:51<07:57, 23.90s/it]Epoch 80: Train acc/f1 = 0.7933 / 0.6917 / 0.7508 / 0.9108 
                Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.4125 / 0.4500 / 0.6429
 81%|████████  | 81/100 [30:11<07:11, 22.69s/it] 82%|████████▏ | 82/100 [30:32<06:39, 22.21s/it] 83%|████████▎ | 83/100 [30:52<06:05, 21.51s/it] 84%|████████▍ | 84/100 [31:13<05:42, 21.39s/it] 85%|████████▌ | 85/100 [31:42<05:56, 23.76s/it]Epoch 85: Train acc/f1 = 0.7928 / 0.6906 / 0.7382 / 0.9211 
                Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.4125 / 0.4500 / 0.6429
 86%|████████▌ | 86/100 [32:03<05:21, 22.93s/it] 87%|████████▋ | 87/100 [32:23<04:46, 22.00s/it] 88%|████████▊ | 88/100 [32:44<04:21, 21.83s/it] 89%|████████▉ | 89/100 [33:04<03:54, 21.28s/it] 90%|█████████ | 90/100 [33:35<04:00, 24.04s/it]Epoch 90: Train acc/f1 = 0.7989 / 0.7068 / 0.7492 / 0.9142 
                Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.4125 / 0.4500 / 0.6429
 91%|█████████ | 91/100 [33:55<03:25, 22.79s/it] 92%|█████████▏| 92/100 [34:16<02:58, 22.26s/it] 93%|█████████▎| 93/100 [34:36<02:30, 21.54s/it] 94%|█████████▍| 94/100 [34:57<02:08, 21.48s/it] 95%|█████████▌| 95/100 [35:27<01:59, 23.99s/it]Epoch 95: Train acc/f1 = 0.7894 / 0.6818 / 0.7429 / 0.9142 
                Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.4125 / 0.4500 / 0.6429
 96%|█████████▌| 96/100 [35:48<01:32, 23.13s/it] 97%|█████████▋| 97/100 [36:08<01:06, 22.16s/it] 98%|█████████▊| 98/100 [36:29<00:43, 21.82s/it] 99%|█████████▉| 99/100 [36:49<00:21, 21.23s/it]100%|██████████| 100/100 [37:19<00:00, 23.97s/it]                                                 Epoch 100: Train acc/f1 = 0.7900 / 0.6902 / 0.7445 / 0.9074 
                Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
                Test acc/f1/spec/sens = 0.4180 / 0.4125 / 0.4500 / 0.6429
specificity = 469/634
sensitivity = 1079/1166
specificity = 64/77
sensitivity = 112/123
specificity = 36/80
sensitivity = 27/42
Trial 2, Cycle 1
Train acc/f1/spec/sens = 0.7939 / 0.6840 / 0.7397 / 0.9254
Val acc/f1/spec/sens = 0.8050 / 0.7299 / 0.8312 / 0.9106
Test acc/f1/spec/sens = 0.4180 / 0.4125 / 0.4500 / 0.6429
>> Finished.
specificity = 36/80
sensitivity = 27/42
Acc, agreement for latest model:  0.4180327868852459 0.4262295081967213
Load best checkpoint for thief model
specificity = 27/80
sensitivity = 34/42
Acc, agreement for best model:  0.3770491803278688 0.39344262295081966
Trial 1/5 || Cycle 1/1 || Label set size 1800 || Test acc 0.3770 || Test agreement 0.3934 || Spec 0.3375 || Sens 0.8095
**************************************************************************************************** 

specificity = 27/80
sensitivity = 34/42
Number of samples  1800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
/home/deepankar/scratch/MSA_Medical/all_names.pkl
Num of videos 64 frames 15800
replacing labeled set labels with victim labels
  0%|          | 0/113 [00:00<?, ?it/s]  1%|          | 1/113 [00:00<01:07,  1.66it/s]  2%|▏         | 2/113 [00:00<00:41,  2.66it/s]  3%|▎         | 3/113 [00:01<00:32,  3.35it/s]  4%|▎         | 4/113 [00:01<00:28,  3.84it/s]  4%|▍         | 5/113 [00:01<00:26,  4.14it/s]  5%|▌         | 6/113 [00:01<00:24,  4.42it/s]  6%|▌         | 7/113 [00:01<00:22,  4.66it/s]  7%|▋         | 8/113 [00:02<00:22,  4.77it/s]  8%|▊         | 9/113 [00:02<00:21,  4.84it/s]  9%|▉         | 10/113 [00:02<00:20,  4.91it/s] 10%|▉         | 11/113 [00:02<00:20,  4.87it/s] 11%|█         | 12/113 [00:02<00:20,  4.89it/s] 12%|█▏        | 13/113 [00:03<00:20,  4.93it/s] 12%|█▏        | 14/113 [00:03<00:20,  4.88it/s] 13%|█▎        | 15/113 [00:03<00:19,  4.97it/s] 14%|█▍        | 16/113 [00:03<00:19,  4.97it/s] 15%|█▌        | 17/113 [00:03<00:20,  4.67it/s] 16%|█▌        | 18/113 [00:04<00:19,  4.76it/s] 17%|█▋        | 19/113 [00:04<00:19,  4.86it/s] 18%|█▊        | 20/113 [00:04<00:18,  4.95it/s] 19%|█▊        | 21/113 [00:04<00:18,  4.91it/s] 19%|█▉        | 22/113 [00:04<00:18,  4.97it/s] 20%|██        | 23/113 [00:05<00:17,  5.04it/s] 21%|██        | 24/113 [00:05<00:17,  5.08it/s] 22%|██▏       | 25/113 [00:05<00:17,  5.10it/s] 23%|██▎       | 26/113 [00:05<00:17,  5.12it/s] 24%|██▍       | 27/113 [00:05<00:16,  5.10it/s] 25%|██▍       | 28/113 [00:06<00:16,  5.11it/s] 26%|██▌       | 29/113 [00:06<00:16,  5.16it/s] 27%|██▋       | 30/113 [00:06<00:16,  5.18it/s] 27%|██▋       | 31/113 [00:06<00:15,  5.17it/s] 28%|██▊       | 32/113 [00:06<00:15,  5.22it/s] 29%|██▉       | 33/113 [00:06<00:15,  5.19it/s] 30%|███       | 34/113 [00:07<00:15,  5.23it/s] 31%|███       | 35/113 [00:07<00:14,  5.25it/s] 32%|███▏      | 36/113 [00:07<00:14,  5.22it/s] 33%|███▎      | 37/113 [00:07<00:14,  5.17it/s] 34%|███▎      | 38/113 [00:07<00:14,  5.19it/s] 35%|███▍      | 39/113 [00:08<00:14,  5.22it/s] 35%|███▌      | 40/113 [00:08<00:13,  5.25it/s] 36%|███▋      | 41/113 [00:08<00:13,  5.24it/s] 37%|███▋      | 42/113 [00:08<00:13,  5.23it/s] 38%|███▊      | 43/113 [00:08<00:13,  5.25it/s] 39%|███▉      | 44/113 [00:09<00:13,  5.19it/s] 40%|███▉      | 45/113 [00:09<00:14,  4.83it/s] 41%|████      | 46/113 [00:09<00:13,  4.96it/s] 42%|████▏     | 47/113 [00:09<00:13,  5.03it/s] 42%|████▏     | 48/113 [00:09<00:12,  5.09it/s] 43%|████▎     | 49/113 [00:10<00:12,  5.02it/s] 44%|████▍     | 50/113 [00:10<00:12,  5.00it/s] 45%|████▌     | 51/113 [00:10<00:12,  5.08it/s] 46%|████▌     | 52/113 [00:10<00:12,  5.08it/s] 47%|████▋     | 53/113 [00:10<00:13,  4.61it/s] 48%|████▊     | 54/113 [00:11<00:12,  4.70it/s] 49%|████▊     | 55/113 [00:11<00:12,  4.57it/s] 50%|████▉     | 56/113 [00:11<00:11,  4.75it/s] 50%|█████     | 57/113 [00:11<00:12,  4.47it/s] 51%|█████▏    | 58/113 [00:12<00:11,  4.65it/s] 52%|█████▏    | 59/113 [00:12<00:11,  4.81it/s] 53%|█████▎    | 60/113 [00:12<00:10,  4.94it/s] 54%|█████▍    | 61/113 [00:12<00:10,  5.03it/s] 55%|█████▍    | 62/113 [00:12<00:10,  5.04it/s] 56%|█████▌    | 63/113 [00:13<00:09,  5.08it/s] 57%|█████▋    | 64/113 [00:13<00:09,  5.13it/s] 58%|█████▊    | 65/113 [00:13<00:09,  5.13it/s] 58%|█████▊    | 66/113 [00:13<00:09,  5.16it/s] 59%|█████▉    | 67/113 [00:13<00:08,  5.15it/s] 60%|██████    | 68/113 [00:13<00:08,  5.20it/s] 61%|██████    | 69/113 [00:14<00:08,  5.03it/s] 62%|██████▏   | 70/113 [00:14<00:09,  4.46it/s] 63%|██████▎   | 71/113 [00:14<00:09,  4.33it/s] 64%|██████▎   | 72/113 [00:14<00:09,  4.27it/s] 65%|██████▍   | 73/113 [00:15<00:09,  4.13it/s] 65%|██████▌   | 74/113 [00:15<00:09,  3.93it/s] 66%|██████▋   | 75/113 [00:15<00:09,  3.90it/s] 67%|██████▋   | 76/113 [00:16<00:09,  3.92it/s] 68%|██████▊   | 77/113 [00:16<00:09,  3.94it/s] 69%|██████▉   | 78/113 [00:16<00:08,  3.95it/s] 70%|██████▉   | 79/113 [00:16<00:08,  3.96it/s] 71%|███████   | 80/113 [00:17<00:08,  3.99it/s] 72%|███████▏  | 81/113 [00:17<00:08,  3.97it/s] 73%|███████▎  | 82/113 [00:17<00:07,  3.96it/s] 73%|███████▎  | 83/113 [00:17<00:07,  3.97it/s] 74%|███████▍  | 84/113 [00:18<00:07,  4.01it/s] 75%|███████▌  | 85/113 [00:18<00:06,  4.02it/s] 76%|███████▌  | 86/113 [00:18<00:06,  4.01it/s] 77%|███████▋  | 87/113 [00:18<00:06,  3.94it/s] 78%|███████▊  | 88/113 [00:19<00:06,  3.97it/s] 79%|███████▉  | 89/113 [00:19<00:06,  3.94it/s] 80%|███████▉  | 90/113 [00:19<00:05,  3.96it/s] 81%|████████  | 91/113 [00:19<00:05,  3.98it/s] 81%|████████▏ | 92/113 [00:20<00:05,  3.95it/s] 82%|████████▏ | 93/113 [00:20<00:05,  3.97it/s] 83%|████████▎ | 94/113 [00:20<00:04,  3.96it/s] 84%|████████▍ | 95/113 [00:20<00:04,  3.98it/s] 85%|████████▍ | 96/113 [00:21<00:04,  4.03it/s] 86%|████████▌ | 97/113 [00:21<00:03,  4.04it/s] 87%|████████▋ | 98/113 [00:21<00:03,  4.00it/s] 88%|████████▊ | 99/113 [00:21<00:03,  3.96it/s] 88%|████████▊ | 100/113 [00:22<00:03,  3.98it/s] 89%|████████▉ | 101/113 [00:22<00:03,  3.97it/s] 90%|█████████ | 102/113 [00:22<00:02,  3.88it/s] 91%|█████████ | 103/113 [00:22<00:02,  3.79it/s] 92%|█████████▏| 104/113 [00:23<00:02,  3.67it/s] 93%|█████████▎| 105/113 [00:23<00:02,  3.58it/s] 94%|█████████▍| 106/113 [00:23<00:01,  3.56it/s] 95%|█████████▍| 107/113 [00:24<00:01,  3.48it/s] 96%|█████████▌| 108/113 [00:24<00:01,  3.41it/s] 96%|█████████▋| 109/113 [00:24<00:01,  3.17it/s] 97%|█████████▋| 110/113 [00:25<00:00,  3.14it/s] 98%|█████████▊| 111/113 [00:25<00:00,  3.14it/s] 99%|█████████▉| 112/113 [00:25<00:00,  3.29it/s]100%|██████████| 113/113 [00:25<00:00,  3.43it/s]100%|██████████| 113/113 [00:25<00:00,  4.35it/s]
replacing val labels with victim labels
  0%|          | 0/13 [00:00<?, ?it/s]  8%|▊         | 1/13 [00:00<00:07,  1.58it/s] 15%|█▌        | 2/13 [00:00<00:04,  2.38it/s] 23%|██▎       | 3/13 [00:01<00:03,  2.90it/s] 31%|███       | 4/13 [00:01<00:02,  3.50it/s] 38%|███▊      | 5/13 [00:01<00:02,  3.93it/s] 46%|████▌     | 6/13 [00:01<00:01,  4.24it/s] 54%|█████▍    | 7/13 [00:01<00:01,  4.49it/s] 62%|██████▏   | 8/13 [00:02<00:01,  4.56it/s] 69%|██████▉   | 9/13 [00:02<00:00,  4.75it/s] 77%|███████▋  | 10/13 [00:02<00:00,  4.91it/s] 85%|████████▍ | 11/13 [00:02<00:00,  4.94it/s] 92%|█████████▏| 12/13 [00:02<00:00,  4.74it/s]100%|██████████| 13/13 [00:03<00:00,  5.61it/s]100%|██████████| 13/13 [00:03<00:00,  4.12it/s]Validation set distribution: 
Number of samples  200

{0: 28, 1: 40, 2: 132}
Labeled set distribution: 
Number of samples  1800
{0: 205, 1: 426, 2: 1169}
odict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_0.mlp.0.weight', 'encoder.layers.encoder_layer_0.mlp.0.bias', 'encoder.layers.encoder_layer_0.mlp.3.weight', 'encoder.layers.encoder_layer_0.mlp.3.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_1.mlp.0.weight', 'encoder.layers.encoder_layer_1.mlp.0.bias', 'encoder.layers.encoder_layer_1.mlp.3.weight', 'encoder.layers.encoder_layer_1.mlp.3.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_2.mlp.0.weight', 'encoder.layers.encoder_layer_2.mlp.0.bias', 'encoder.layers.encoder_layer_2.mlp.3.weight', 'encoder.layers.encoder_layer_2.mlp.3.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_3.mlp.0.weight', 'encoder.layers.encoder_layer_3.mlp.0.bias', 'encoder.layers.encoder_layer_3.mlp.3.weight', 'encoder.layers.encoder_layer_3.mlp.3.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_4.mlp.0.weight', 'encoder.layers.encoder_layer_4.mlp.0.bias', 'encoder.layers.encoder_layer_4.mlp.3.weight', 'encoder.layers.encoder_layer_4.mlp.3.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_5.mlp.0.weight', 'encoder.layers.encoder_layer_5.mlp.0.bias', 'encoder.layers.encoder_layer_5.mlp.3.weight', 'encoder.layers.encoder_layer_5.mlp.3.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_6.mlp.0.weight', 'encoder.layers.encoder_layer_6.mlp.0.bias', 'encoder.layers.encoder_layer_6.mlp.3.weight', 'encoder.layers.encoder_layer_6.mlp.3.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_7.mlp.0.weight', 'encoder.layers.encoder_layer_7.mlp.0.bias', 'encoder.layers.encoder_layer_7.mlp.3.weight', 'encoder.layers.encoder_layer_7.mlp.3.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_8.mlp.0.weight', 'encoder.layers.encoder_layer_8.mlp.0.bias', 'encoder.layers.encoder_layer_8.mlp.3.weight', 'encoder.layers.encoder_layer_8.mlp.3.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_9.mlp.0.weight', 'encoder.layers.encoder_layer_9.mlp.0.bias', 'encoder.layers.encoder_layer_9.mlp.3.weight', 'encoder.layers.encoder_layer_9.mlp.3.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_10.mlp.0.weight', 'encoder.layers.encoder_layer_10.mlp.0.bias', 'encoder.layers.encoder_layer_10.mlp.3.weight', 'encoder.layers.encoder_layer_10.mlp.3.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.layers.encoder_layer_11.mlp.0.weight', 'encoder.layers.encoder_layer_11.mlp.0.bias', 'encoder.layers.encoder_layer_11.mlp.3.weight', 'encoder.layers.encoder_layer_11.mlp.3.bias', 'encoder.ln.weight', 'encoder.ln.bias', 'heads.head.weight', 'heads.head.bias'])
thief state:  None
pretrained state:  dict_keys(['class_token', 'conv_proj.weight', 'conv_proj.bias', 'encoder.pos_embedding', 'encoder.layers.encoder_layer_0.ln_1.weight', 'encoder.layers.encoder_layer_0.ln_1.bias', 'encoder.layers.encoder_layer_0.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_0.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_0.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_0.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_0.ln_2.weight', 'encoder.layers.encoder_layer_0.ln_2.bias', 'encoder.layers.encoder_layer_1.ln_1.weight', 'encoder.layers.encoder_layer_1.ln_1.bias', 'encoder.layers.encoder_layer_1.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_1.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_1.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_1.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_1.ln_2.weight', 'encoder.layers.encoder_layer_1.ln_2.bias', 'encoder.layers.encoder_layer_2.ln_1.weight', 'encoder.layers.encoder_layer_2.ln_1.bias', 'encoder.layers.encoder_layer_2.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_2.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_2.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_2.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_2.ln_2.weight', 'encoder.layers.encoder_layer_2.ln_2.bias', 'encoder.layers.encoder_layer_3.ln_1.weight', 'encoder.layers.encoder_layer_3.ln_1.bias', 'encoder.layers.encoder_layer_3.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_3.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_3.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_3.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_3.ln_2.weight', 'encoder.layers.encoder_layer_3.ln_2.bias', 'encoder.layers.encoder_layer_4.ln_1.weight', 'encoder.layers.encoder_layer_4.ln_1.bias', 'encoder.layers.encoder_layer_4.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_4.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_4.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_4.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_4.ln_2.weight', 'encoder.layers.encoder_layer_4.ln_2.bias', 'encoder.layers.encoder_layer_5.ln_1.weight', 'encoder.layers.encoder_layer_5.ln_1.bias', 'encoder.layers.encoder_layer_5.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_5.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_5.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_5.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_5.ln_2.weight', 'encoder.layers.encoder_layer_5.ln_2.bias', 'encoder.layers.encoder_layer_6.ln_1.weight', 'encoder.layers.encoder_layer_6.ln_1.bias', 'encoder.layers.encoder_layer_6.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_6.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_6.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_6.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_6.ln_2.weight', 'encoder.layers.encoder_layer_6.ln_2.bias', 'encoder.layers.encoder_layer_7.ln_1.weight', 'encoder.layers.encoder_layer_7.ln_1.bias', 'encoder.layers.encoder_layer_7.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_7.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_7.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_7.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_7.ln_2.weight', 'encoder.layers.encoder_layer_7.ln_2.bias', 'encoder.layers.encoder_layer_8.ln_1.weight', 'encoder.layers.encoder_layer_8.ln_1.bias', 'encoder.layers.encoder_layer_8.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_8.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_8.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_8.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_8.ln_2.weight', 'encoder.layers.encoder_layer_8.ln_2.bias', 'encoder.layers.encoder_layer_9.ln_1.weight', 'encoder.layers.encoder_layer_9.ln_1.bias', 'encoder.layers.encoder_layer_9.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_9.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_9.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_9.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_9.ln_2.weight', 'encoder.layers.encoder_layer_9.ln_2.bias', 'encoder.layers.encoder_layer_10.ln_1.weight', 'encoder.layers.encoder_layer_10.ln_1.bias', 'encoder.layers.encoder_layer_10.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_10.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_10.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_10.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_10.ln_2.weight', 'encoder.layers.encoder_layer_10.ln_2.bias', 'encoder.layers.encoder_layer_11.ln_1.weight', 'encoder.layers.encoder_layer_11.ln_1.bias', 'encoder.layers.encoder_layer_11.self_attention.in_proj_weight', 'encoder.layers.encoder_layer_11.self_attention.in_proj_bias', 'encoder.layers.encoder_layer_11.self_attention.out_proj.weight', 'encoder.layers.encoder_layer_11.self_attention.out_proj.bias', 'encoder.layers.encoder_layer_11.ln_2.weight', 'encoder.layers.encoder_layer_11.ln_2.bias', 'encoder.ln.weight', 'encoder.ln.bias'])
Thief model initialized successfully
specificity = 80/80
sensitivity = 0/42
Initial model on target dataset: acc = 0.2541, agreement = 0.2377, f1 = 0.1351, spec = 1.0000, sens = 0.0000
specificity = 68/68
sensitivity = 0/132
Initial model on validation dataset: acc = 0.1400, agreement = 0.1400, f1 = 0.0819, spec = 1.0000, sens = 0.0000
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:19<32:19, 19.59s/it]  2%|▏         | 2/100 [00:41<34:16, 20.98s/it]  3%|▎         | 3/100 [01:01<33:12, 20.54s/it]  4%|▍         | 4/100 [01:23<33:47, 21.12s/it]  5%|▌         | 5/100 [01:52<38:03, 24.04s/it]Epoch 5: Train acc/f1 = 0.6683 / 0.3321 / 0.0967 / 0.9863 
                Val acc/f1/spec/sens = 0.6850 / 0.3432 / 0.0735 / 1.0000
                Test acc/f1/spec/sens = 0.3443 / 0.1707 / 0.0000 / 1.0000
  6%|▌         | 6/100 [02:14<36:17, 23.17s/it]  7%|▋         | 7/100 [02:34<34:12, 22.07s/it]  8%|▊         | 8/100 [02:55<33:35, 21.91s/it]  9%|▉         | 9/100 [03:15<32:15, 21.27s/it] 10%|█         | 10/100 [03:46<36:22, 24.25s/it]Epoch 10: Train acc/f1 = 0.6128 / 0.4987 / 0.7338 / 0.6766 
                Val acc/f1/spec/sens = 0.6750 / 0.5715 / 0.7059 / 0.7500
                Test acc/f1/spec/sens = 0.4098 / 0.3048 / 0.3750 / 0.7857
 11%|█         | 11/100 [04:06<33:55, 22.87s/it] 12%|█▏        | 12/100 [04:27<32:39, 22.27s/it] 13%|█▎        | 13/100 [04:46<31:11, 21.51s/it] 14%|█▍        | 14/100 [05:07<30:35, 21.34s/it] 15%|█▌        | 15/100 [05:37<33:40, 23.78s/it]Epoch 15: Train acc/f1 = 0.7000 / 0.4664 / 0.2536 / 0.9795 
                Val acc/f1/spec/sens = 0.7200 / 0.4795 / 0.2647 / 0.9924
                Test acc/f1/spec/sens = 0.3525 / 0.1937 / 0.0250 / 1.0000
 16%|█▌        | 16/100 [05:58<32:06, 22.94s/it] 17%|█▋        | 17/100 [06:18<30:26, 22.01s/it] 18%|█▊        | 18/100 [06:39<29:40, 21.71s/it] 19%|█▉        | 19/100 [06:58<28:34, 21.16s/it] 20%|██        | 20/100 [07:29<31:53, 23.92s/it]Epoch 20: Train acc/f1 = 0.7117 / 0.4902 / 0.4770 / 0.9222 
                Val acc/f1/spec/sens = 0.7000 / 0.4734 / 0.4265 / 0.9242
                Test acc/f1/spec/sens = 0.3770 / 0.2411 / 0.1375 / 0.9524
 21%|██        | 21/100 [07:49<29:53, 22.71s/it] 22%|██▏       | 22/100 [08:10<28:53, 22.22s/it] 23%|██▎       | 23/100 [08:30<27:35, 21.50s/it] 24%|██▍       | 24/100 [08:51<27:03, 21.36s/it] 25%|██▌       | 25/100 [09:20<29:45, 23.81s/it]Epoch 25: Train acc/f1 = 0.7472 / 0.5891 / 0.4881 / 0.9504 
                Val acc/f1/spec/sens = 0.7400 / 0.5276 / 0.4706 / 0.9697
                Test acc/f1/spec/sens = 0.3852 / 0.2386 / 0.1375 / 1.0000
 26%|██▌       | 26/100 [09:41<28:20, 22.98s/it] 27%|██▋       | 27/100 [10:01<26:48, 22.04s/it] 28%|██▊       | 28/100 [10:22<26:03, 21.72s/it] 29%|██▉       | 29/100 [10:42<25:02, 21.16s/it] 30%|███       | 30/100 [11:13<28:09, 24.13s/it]Epoch 30: Train acc/f1 = 0.7567 / 0.6251 / 0.5642 / 0.9384 
                Val acc/f1/spec/sens = 0.7600 / 0.6089 / 0.5147 / 0.9545
                Test acc/f1/spec/sens = 0.3934 / 0.2687 / 0.2125 / 0.9762
 31%|███       | 31/100 [11:33<26:16, 22.85s/it] 32%|███▏      | 32/100 [11:54<25:29, 22.50s/it] 33%|███▎      | 33/100 [12:14<24:14, 21.71s/it] 34%|███▍      | 34/100 [12:36<23:49, 21.66s/it] 35%|███▌      | 35/100 [13:05<25:54, 23.91s/it]Epoch 35: Train acc/f1 = 0.7700 / 0.6473 / 0.6498 / 0.9179 
                Val acc/f1/spec/sens = 0.8050 / 0.7085 / 0.7206 / 0.9242
                Test acc/f1/spec/sens = 0.4836 / 0.3713 / 0.6500 / 0.7619
 36%|███▌      | 36/100 [13:26<24:33, 23.02s/it] 37%|███▋      | 37/100 [13:46<23:09, 22.05s/it] 38%|███▊      | 38/100 [14:07<22:29, 21.76s/it] 39%|███▉      | 39/100 [14:27<21:31, 21.18s/it] 40%|████      | 40/100 [14:57<23:55, 23.93s/it]Epoch 40: Train acc/f1 = 0.7528 / 0.6001 / 0.5103 / 0.9581 
                Val acc/f1/spec/sens = 0.7400 / 0.5504 / 0.4853 / 0.9621
                Test acc/f1/spec/sens = 0.4262 / 0.3242 / 0.5500 / 0.7381
 41%|████      | 41/100 [15:17<22:19, 22.70s/it] 42%|████▏     | 42/100 [15:38<21:27, 22.20s/it] 43%|████▎     | 43/100 [15:58<20:25, 21.50s/it] 44%|████▍     | 44/100 [16:19<19:56, 21.37s/it] 45%|████▌     | 45/100 [16:48<21:43, 23.70s/it]Epoch 45: Train acc/f1 = 0.7817 / 0.6704 / 0.6498 / 0.9290 
                Val acc/f1/spec/sens = 0.7850 / 0.6590 / 0.6471 / 0.9394
                Test acc/f1/spec/sens = 0.4180 / 0.3387 / 0.6375 / 0.5952
 46%|████▌     | 46/100 [17:09<20:36, 22.89s/it] 47%|████▋     | 47/100 [17:29<19:24, 21.97s/it] 48%|████▊     | 48/100 [17:50<18:47, 21.68s/it] 49%|████▉     | 49/100 [18:10<17:57, 21.12s/it] 50%|█████     | 50/100 [18:41<20:08, 24.17s/it]Epoch 50: Train acc/f1 = 0.7839 / 0.6744 / 0.6339 / 0.9358 
                Val acc/f1/spec/sens = 0.8050 / 0.6966 / 0.6618 / 0.9470
                Test acc/f1/spec/sens = 0.4344 / 0.3661 / 0.6625 / 0.5952
 51%|█████     | 51/100 [19:01<18:42, 22.91s/it]