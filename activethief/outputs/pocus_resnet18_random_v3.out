nohup: ignoring input
[24/03/07 00:52:04] [conf.py:  298]: PyTorch Version: torch=1.11.0, cuda=11.3, cudnn=8200
[24/03/07 00:52:04] [conf.py:  300]: ACTIVE:
  ADDENDUM: 0
  ALPHA: 0.2
  AUGMENT: None
  BETA: 1.0
  BUDGET: 1000
  CUTMIX_PROB: 0.0
  CYCLES: 1
  INITIAL: 900
  LA: False
  METHOD: random
  PRETRAINED_PATH: /home/ankita/scratch/data_msa_medical/ckpts/resnet50-imagenet1k.pth
  TEMP: 1.0
  TRO: 1.0
  USE_PRETRAINED: True
  VAL: 100
AM:
  SGR: 1000
  THRESHOLD: 0.5
CUDNN_BENCHMARK: True
DS_SEED: 123
EDM:
  HASH: /home/harsh_s/scratch/msacopy/exp/hash/alpha.pt
  NUM_MODELS: 5
  PATH1: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T0.pt
  PATH2: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T1.pt
  PATH3: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T2.pt
  PATH4: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T3.pt
  PATH5: /home/harsh_s/scratch/msacopy/exp/cifar10/alpha/T4.pt
GR:
  EPSILON: 0.1
  SUR_PATH: /home/harsh_s/scratch/msacopy/GRAD2/batch_training/outputs/trained_models/imagenet_cifar10_to_cifar10_surrogate_10epochs.pt
LOG_DEST: resnet18_resnet50_pocus_240307_005204.txt
LOG_TIME: 240307_005204
METHOD_NAME: v3
OUT_DIR: /home/ankita/scratch/data_msa_medical/results_ankita/
PL:
  CLASS_BLNC: 10
  ITERATIONS: 20
  KAPPA_N: 0.005
  KAPPA_P: 0.05
  NO_PROGRESS: False
  NO_UNCERTAINTY: False
  TAU_N: 0.05
  TAU_P: 0.7
  TEMP_NL: 2.0
RNG_SEED: 1
SAVE_DIR: /home/ankita/scratch/data_msa_medical/results_ankita//pocus_resnet18/butterfly_resnet50/SGD/1000_val100/random_v3
THIEF:
  ARCH: resnet50
  DATASET: butterfly
  DATA_ROOT: /home/ankita/scratch/data_msa_medical/Butte
  HARD_LABELS: True
  NUM_TRAIN: 1281167
  SUBSET: 128116
TRAIN:
  BATCH: 16
  EPOCH: 100
  GAMMA: 0.1
  LR: 0.02
  MILESTONES: [20, 40, 60, 80]
  MOMENTUM: 0.9
  OPTIMIZER: SGD
  WDECAY: 0.0005
VICTIM:
  ARCH: resnet18
  DATASET: pocus
  DATA_ROOT: /home/ankita/scratch/data_msa_medical/covid_5_fold
  HEIGHT: 224
  PATH: /home/ankita/scratch/data_msa_medical/victim_models/pocus_resnet18.pth.tar
  PATHIN: /home/harsh_s/scratch/msacopy/ckpts/cifar10-resnet32-inaccurate/checkpoint.pth.tar
  WIDTH: 224
Loaded target dataset of size 476 with 3 classes
target model keys:  122
checkpoint keys:  122

Target model acc = 0.8991596638655462
Val-Acc: 0.8992 Val-Spec: 0.9795 Val-Sens: 0.9224 Class Acc: [0.8343949  0.95402299 0.92241379]
replacing labeled set labels with victim labels
  0%|          | 0/57 [00:00<?, ?it/s]  2%|▏         | 1/57 [00:00<00:29,  1.90it/s]  9%|▉         | 5/57 [00:00<00:05,  8.73it/s] 16%|█▌        | 9/57 [00:00<00:04, 11.90it/s] 23%|██▎       | 13/57 [00:01<00:02, 15.87it/s] 28%|██▊       | 16/57 [00:01<00:02, 17.82it/s] 35%|███▌      | 20/57 [00:01<00:01, 20.29it/s] 40%|████      | 23/57 [00:01<00:01, 21.03it/s] 46%|████▌     | 26/57 [00:01<00:01, 18.73it/s] 51%|█████     | 29/57 [00:01<00:01, 18.12it/s] 56%|█████▌    | 32/57 [00:02<00:01, 19.11it/s] 63%|██████▎   | 36/57 [00:02<00:01, 20.35it/s] 70%|███████   | 40/57 [00:02<00:00, 20.88it/s] 77%|███████▋  | 44/57 [00:02<00:00, 21.61it/s] 84%|████████▍ | 48/57 [00:02<00:00, 19.93it/s] 91%|█████████ | 52/57 [00:02<00:00, 20.38it/s] 98%|█████████▊| 56/57 [00:03<00:00, 21.22it/s]100%|██████████| 57/57 [00:03<00:00, 17.38it/s]
replacing val labels with victim labels
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:00<00:02,  2.03it/s] 57%|█████▋    | 4/7 [00:00<00:00,  7.35it/s]100%|██████████| 7/7 [00:00<00:00,  8.89it/s]
Test set stats: mean/std =  (tensor([-1.2593, -1.2577, -1.2463]), tensor([0.7453, 0.7491, 0.7672]))
Train set stats: mean/std =  (tensor([-1.2243, -1.2400, -1.2166]), tensor([0.8476, 0.8431, 0.8505]))
Val set stats: mean/std =  (tensor([-1.1715, -1.1901, -1.1622]), tensor([0.8641, 0.8598, 0.8667]))
Validation set distribution: 
Number of samples  100
{0: 18, 1: 50, 2: 32}
Labeled set distribution: 
Number of samples  900
{0: 149, 1: 391, 2: 360}
Test set distribution: 
Number of samples  476
{0: 157, 1: 87, 2: 232}
pretrained state:  dict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.conv3.weight', 'layer1.0.bn3.weight', 'layer1.0.bn3.bias', 'layer1.0.bn3.running_mean', 'layer1.0.bn3.running_var', 'layer1.0.bn3.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.1.conv3.weight', 'layer1.1.bn3.weight', 'layer1.1.bn3.bias', 'layer1.1.bn3.running_mean', 'layer1.1.bn3.running_var', 'layer1.1.bn3.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.2.conv3.weight', 'layer1.2.bn3.weight', 'layer1.2.bn3.bias', 'layer1.2.bn3.running_mean', 'layer1.2.bn3.running_var', 'layer1.2.bn3.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.conv3.weight', 'layer2.0.bn3.weight', 'layer2.0.bn3.bias', 'layer2.0.bn3.running_mean', 'layer2.0.bn3.running_var', 'layer2.0.bn3.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.1.conv3.weight', 'layer2.1.bn3.weight', 'layer2.1.bn3.bias', 'layer2.1.bn3.running_mean', 'layer2.1.bn3.running_var', 'layer2.1.bn3.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.2.conv3.weight', 'layer2.2.bn3.weight', 'layer2.2.bn3.bias', 'layer2.2.bn3.running_mean', 'layer2.2.bn3.running_var', 'layer2.2.bn3.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.3.conv3.weight', 'layer2.3.bn3.weight', 'layer2.3.bn3.bias', 'layer2.3.bn3.running_mean', 'layer2.3.bn3.running_var', 'layer2.3.bn3.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.conv3.weight', 'layer3.0.bn3.weight', 'layer3.0.bn3.bias', 'layer3.0.bn3.running_mean', 'layer3.0.bn3.running_var', 'layer3.0.bn3.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.1.conv3.weight', 'layer3.1.bn3.weight', 'layer3.1.bn3.bias', 'layer3.1.bn3.running_mean', 'layer3.1.bn3.running_var', 'layer3.1.bn3.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.2.conv3.weight', 'layer3.2.bn3.weight', 'layer3.2.bn3.bias', 'layer3.2.bn3.running_mean', 'layer3.2.bn3.running_var', 'layer3.2.bn3.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.3.conv3.weight', 'layer3.3.bn3.weight', 'layer3.3.bn3.bias', 'layer3.3.bn3.running_mean', 'layer3.3.bn3.running_var', 'layer3.3.bn3.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'layer3.4.conv3.weight', 'layer3.4.bn3.weight', 'layer3.4.bn3.bias', 'layer3.4.bn3.running_mean', 'layer3.4.bn3.running_var', 'layer3.4.bn3.num_batches_tracked', 'layer3.5.conv1.weight', 'layer3.5.bn1.weight', 'layer3.5.bn1.bias', 'layer3.5.bn1.running_mean', 'layer3.5.bn1.running_var', 'layer3.5.bn1.num_batches_tracked', 'layer3.5.conv2.weight', 'layer3.5.bn2.weight', 'layer3.5.bn2.bias', 'layer3.5.bn2.running_mean', 'layer3.5.bn2.running_var', 'layer3.5.bn2.num_batches_tracked', 'layer3.5.conv3.weight', 'layer3.5.bn3.weight', 'layer3.5.bn3.bias', 'layer3.5.bn3.running_mean', 'layer3.5.bn3.running_var', 'layer3.5.bn3.num_batches_tracked', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.num_batches_tracked', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.num_batches_tracked', 'layer4.0.conv3.weight', 'layer4.0.bn3.weight', 'layer4.0.bn3.bias', 'layer4.0.bn3.running_mean', 'layer4.0.bn3.running_var', 'layer4.0.bn3.num_batches_tracked', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.num_batches_tracked', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.num_batches_tracked', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.num_batches_tracked', 'layer4.1.conv3.weight', 'layer4.1.bn3.weight', 'layer4.1.bn3.bias', 'layer4.1.bn3.running_mean', 'layer4.1.bn3.running_var', 'layer4.1.bn3.num_batches_tracked', 'layer4.2.conv1.weight', 'layer4.2.bn1.weight', 'layer4.2.bn1.bias', 'layer4.2.bn1.running_mean', 'layer4.2.bn1.running_var', 'layer4.2.bn1.num_batches_tracked', 'layer4.2.conv2.weight', 'layer4.2.bn2.weight', 'layer4.2.bn2.bias', 'layer4.2.bn2.running_mean', 'layer4.2.bn2.running_var', 'layer4.2.bn2.num_batches_tracked', 'layer4.2.conv3.weight', 'layer4.2.bn3.weight', 'layer4.2.bn3.bias', 'layer4.2.bn3.running_mean', 'layer4.2.bn3.running_var', 'layer4.2.bn3.num_batches_tracked'])
Thief model initialized successfully
Initial model on target dataset: acc = 0.1975, agreement = 0.1933, f1 = 0.1590, spec = 0.6066, sens = 0.1595 Class Acc: [0.00636943 0.64367816 0.15948276]
Initial model on validation dataset: acc = 0.5300, agreement = 0.5300, f1 = 0.3449, spec = 0.9118, sens = 0.2812 Class Acc: [0.      0.88    0.28125]
la:  None
>> Train a Model.
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:03<05:57,  3.61s/it]epoch 0, loss = 8.766222953796387
epoch 1, loss = 5.5814433097839355
  2%|▏         | 2/100 [00:08<06:54,  4.23s/it]  3%|▎         | 3/100 [00:11<06:20,  3.92s/it]epoch 2, loss = 4.521890163421631
epoch 3, loss = 3.893535614013672
  4%|▍         | 4/100 [00:16<06:48,  4.25s/it]epoch 4, loss = 3.4866533279418945
  5%|▌         | 5/100 [00:24<08:44,  5.52s/it]Epoch 5: Train acc/f1 = 0.9111 / 0.8856 / 0.9722 / 0.9389/ [0.8590604  0.90537084 0.93888889] 
                Val acc/f1/spec/sens = 0.8500 / 0.7859 / 0.8971 / 0.8750/ [0.5   0.96  0.875]
                Test acc/f1/spec/sens = 0.2563 / 0.2103 / 0.9221 / 0.1207/ [0.04458599 1.         0.12068966]
epoch 5, loss = 3.0369439125061035
  6%|▌         | 6/100 [00:29<08:21,  5.34s/it]  7%|▋         | 7/100 [00:33<07:25,  4.79s/it]epoch 6, loss = 3.149240732192993
epoch 7, loss = 3.3895046710968018
  8%|▊         | 8/100 [00:37<07:20,  4.79s/it]  9%|▉         | 9/100 [00:41<06:43,  4.43s/it]epoch 8, loss = 2.796658754348755
epoch 9, loss = 2.9328770637512207
 10%|█         | 10/100 [00:50<08:39,  5.78s/it]Epoch 10: Train acc/f1 = 0.9522 / 0.9363 / 0.9852 / 0.9639/ [0.8590604  0.9769821  0.96388889] 
                Val acc/f1/spec/sens = 0.8500 / 0.7954 / 0.9412 / 0.8438/ [0.55555556 0.96       0.84375   ]
                Test acc/f1/spec/sens = 0.6134 / 0.5927 / 0.7951 / 0.6853/ [0.32484076 0.94252874 0.68534483]
 11%|█         | 11/100 [00:53<07:33,  5.09s/it]epoch 10, loss = 2.0975162982940674
epoch 11, loss = 2.0152430534362793
 12%|█▏        | 12/100 [00:58<07:06,  4.85s/it] 13%|█▎        | 13/100 [01:01<06:28,  4.47s/it]epoch 12, loss = 1.6862980127334595
epoch 13, loss = 1.7724827527999878
 14%|█▍        | 14/100 [01:06<06:27,  4.50s/it]epoch 14, loss = 1.9533580541610718
 15%|█▌        | 15/100 [01:13<07:42,  5.44s/it]Epoch 15: Train acc/f1 = 0.9533 / 0.9406 / 0.9889 / 0.9556/ [0.85234899 0.98976982 0.95555556] 
                Val acc/f1/spec/sens = 0.8700 / 0.8228 / 0.9118 / 0.8125/ [0.66666667 0.98       0.8125    ]
                Test acc/f1/spec/sens = 0.1870 / 0.1237 / 0.8320 / 0.0043/ [0.01273885 0.98850575 0.00431034]
epoch 15, loss = 1.6386629343032837
 16%|█▌        | 16/100 [01:18<07:21,  5.25s/it] 17%|█▋        | 17/100 [01:22<06:33,  4.74s/it]epoch 16, loss = 0.9255839586257935
epoch 17, loss = 1.2514020204544067
 18%|█▊        | 18/100 [01:26<06:27,  4.72s/it] 19%|█▉        | 19/100 [01:30<05:55,  4.39s/it]epoch 18, loss = 1.2507591247558594
epoch 19, loss = 1.431302785873413
 20%|██        | 20/100 [01:39<07:32,  5.65s/it]Epoch 20: Train acc/f1 = 0.9756 / 0.9664 / 0.9759 / 0.9972/ [0.90604027 0.98209719 0.99722222] 
                Val acc/f1/spec/sens = 0.9200 / 0.8922 / 0.9559 / 0.9062/ [0.77777778 0.98       0.90625   ]
                Test acc/f1/spec/sens = 0.3487 / 0.3071 / 0.8648 / 0.2974/ [0.06369427 1.         0.29741379]
 21%|██        | 21/100 [01:42<06:37,  5.03s/it]epoch 20, loss = 0.8325724005699158
epoch 21, loss = 0.5644912719726562
 22%|██▏       | 22/100 [01:47<06:24,  4.92s/it] 23%|██▎       | 23/100 [01:51<05:50,  4.55s/it]epoch 22, loss = 0.40923410654067993
epoch 23, loss = 0.31275123357772827
 24%|██▍       | 24/100 [01:55<05:45,  4.55s/it]epoch 24, loss = 0.24883772432804108
 25%|██▌       | 25/100 [02:03<06:54,  5.52s/it]Epoch 25: Train acc/f1 = 0.9989 / 0.9985 / 1.0000 / 1.0000/ [1.         0.99744246 1.        ] 
                Val acc/f1/spec/sens = 0.9200 / 0.8995 / 0.9853 / 0.8750/ [0.88888889 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5063 / 0.4396 / 0.8238 / 0.6207/ [0.06369427 1.         0.62068966]
epoch 25, loss = 0.23773632943630219
 26%|██▌       | 26/100 [02:07<06:24,  5.20s/it] 27%|██▋       | 27/100 [02:11<05:40,  4.67s/it]epoch 26, loss = 0.1719377487897873
epoch 27, loss = 0.18321499228477478
 28%|██▊       | 28/100 [02:15<05:30,  4.59s/it] 29%|██▉       | 29/100 [02:19<05:05,  4.30s/it]epoch 28, loss = 0.18532632291316986
epoch 29, loss = 0.17386192083358765
 30%|███       | 30/100 [02:27<06:33,  5.61s/it]Epoch 30: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9100 / 0.8867 / 0.9853 / 0.8438/ [0.88888889 0.96       0.84375   ]
                Test acc/f1/spec/sens = 0.5231 / 0.4665 / 0.8115 / 0.6379/ [0.10191083 0.97701149 0.63793103]
 31%|███       | 31/100 [02:31<05:45,  5.01s/it]epoch 30, loss = 0.16028925776481628
epoch 31, loss = 0.16454564034938812
 32%|███▏      | 32/100 [02:35<05:27,  4.81s/it] 33%|███▎      | 33/100 [02:39<04:58,  4.46s/it]epoch 32, loss = 0.09383369237184525
epoch 33, loss = 0.09904628992080688
 34%|███▍      | 34/100 [02:44<04:55,  4.47s/it]epoch 34, loss = 0.11028596758842468
 35%|███▌      | 35/100 [02:51<05:57,  5.49s/it]Epoch 35: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9000 / 0.8739 / 0.9853 / 0.8125/ [0.88888889 0.96       0.8125    ]
                Test acc/f1/spec/sens = 0.5231 / 0.4642 / 0.8197 / 0.6422/ [0.08917197 0.98850575 0.64224138]
epoch 35, loss = 0.07111949473619461
 36%|███▌      | 36/100 [02:56<05:35,  5.24s/it] 37%|███▋      | 37/100 [03:00<04:57,  4.73s/it]epoch 36, loss = 0.07920462638139725
epoch 37, loss = 0.08180544525384903
 38%|███▊      | 38/100 [03:04<04:47,  4.64s/it] 39%|███▉      | 39/100 [03:08<04:26,  4.37s/it]epoch 38, loss = 0.055341169238090515
epoch 39, loss = 0.05063141509890556
 40%|████      | 40/100 [03:16<05:38,  5.64s/it]Epoch 40: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9100 / 0.8842 / 0.9706 / 0.8750/ [0.83333333 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5525 / 0.5012 / 0.7951 / 0.6724/ [0.14012739 0.97701149 0.67241379]
 41%|████      | 41/100 [03:20<04:56,  5.03s/it]epoch 40, loss = 0.04475437477231026
epoch 41, loss = 0.044209763407707214
 42%|████▏     | 42/100 [03:25<04:45,  4.91s/it] 43%|████▎     | 43/100 [03:28<04:18,  4.53s/it]epoch 42, loss = 0.0612243115901947
epoch 43, loss = 0.07521049678325653
 44%|████▍     | 44/100 [03:33<04:13,  4.53s/it]epoch 44, loss = 0.051170654594898224
 45%|████▌     | 45/100 [03:41<05:04,  5.54s/it]Epoch 45: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9200 / 0.8995 / 0.9853 / 0.8750/ [0.88888889 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5147 / 0.4512 / 0.8238 / 0.6379/ [0.07006369 0.98850575 0.63793103]
epoch 45, loss = 0.04811030998826027
 46%|████▌     | 46/100 [03:45<04:41,  5.22s/it] 47%|████▋     | 47/100 [03:49<04:14,  4.80s/it]epoch 46, loss = 0.04941176623106003
epoch 47, loss = 0.05013219267129898
 48%|████▊     | 48/100 [03:53<04:03,  4.68s/it] 49%|████▉     | 49/100 [03:57<03:42,  4.36s/it]epoch 48, loss = 0.07393017411231995
epoch 49, loss = 0.058429036289453506
 50%|█████     | 50/100 [04:06<04:45,  5.71s/it]Epoch 50: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9300 / 0.9101 / 0.9853 / 0.8750/ [0.88888889 0.98       0.875     ]
                Test acc/f1/spec/sens = 0.5126 / 0.4514 / 0.8238 / 0.6336/ [0.07006369 0.98850575 0.63362069]
 51%|█████     | 51/100 [04:09<04:07,  5.04s/it]epoch 50, loss = 0.036645710468292236
epoch 51, loss = 0.0820559710264206
 52%|█████▏    | 52/100 [04:14<03:52,  4.85s/it] 53%|█████▎    | 53/100 [04:18<03:33,  4.54s/it]epoch 52, loss = 0.058038875460624695
epoch 53, loss = 0.043932005763053894
 54%|█████▍    | 54/100 [04:22<03:29,  4.55s/it]epoch 54, loss = 0.0464758574962616
 55%|█████▌    | 55/100 [04:30<04:10,  5.58s/it]Epoch 55: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9200 / 0.8922 / 0.9559 / 0.9062/ [0.77777778 0.98       0.90625   ]
                Test acc/f1/spec/sens = 0.5357 / 0.4738 / 0.8074 / 0.6681/ [0.08917197 0.98850575 0.66810345]
epoch 55, loss = 0.05787378549575806
 56%|█████▌    | 56/100 [04:34<03:49,  5.21s/it] 57%|█████▋    | 57/100 [04:38<03:22,  4.71s/it]epoch 56, loss = 0.0428737998008728
epoch 57, loss = 0.03968331590294838
 58%|█████▊    | 58/100 [04:42<03:14,  4.63s/it] 59%|█████▉    | 59/100 [04:46<02:56,  4.31s/it]epoch 58, loss = 0.0485941506922245
epoch 59, loss = 0.05237684026360512
 60%|██████    | 60/100 [04:55<03:43,  5.59s/it]Epoch 60: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9200 / 0.8995 / 0.9853 / 0.8750/ [0.88888889 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5189 / 0.4586 / 0.8238 / 0.6422/ [0.07643312 0.98850575 0.64224138]
 61%|██████    | 61/100 [04:58<03:13,  4.97s/it]epoch 60, loss = 0.05022810772061348
epoch 61, loss = 0.04811407998204231
 62%|██████▏   | 62/100 [05:03<03:02,  4.81s/it] 63%|██████▎   | 63/100 [05:06<02:44,  4.44s/it]epoch 62, loss = 0.061897873878479004
epoch 63, loss = 0.04006305709481239
 64%|██████▍   | 64/100 [05:11<02:40,  4.47s/it]epoch 64, loss = 0.04323043301701546
 65%|██████▌   | 65/100 [05:19<03:12,  5.50s/it]Epoch 65: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9100 / 0.8842 / 0.9706 / 0.8750/ [0.83333333 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5294 / 0.4726 / 0.8033 / 0.6466/ [0.10191083 0.98850575 0.64655172]
epoch 65, loss = 0.046439461410045624
 66%|██████▌   | 66/100 [05:23<02:57,  5.21s/it] 67%|██████▋   | 67/100 [05:27<02:36,  4.74s/it]epoch 66, loss = 0.10296503454446793
epoch 67, loss = 0.05214895308017731
 68%|██████▊   | 68/100 [05:31<02:29,  4.68s/it] 69%|██████▉   | 69/100 [05:35<02:16,  4.41s/it]epoch 68, loss = 0.03637217357754707
epoch 69, loss = 0.03802625462412834
 70%|███████   | 70/100 [05:44<02:51,  5.70s/it]Epoch 70: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9200 / 0.8995 / 0.9853 / 0.8750/ [0.88888889 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5189 / 0.4604 / 0.8197 / 0.6379/ [0.08280255 0.98850575 0.63793103]
 71%|███████   | 71/100 [05:47<02:27,  5.09s/it]epoch 70, loss = 0.049329161643981934
epoch 71, loss = 0.044515594840049744
 72%|███████▏  | 72/100 [05:52<02:17,  4.91s/it] 73%|███████▎  | 73/100 [05:55<02:01,  4.49s/it]epoch 72, loss = 0.051801275461912155
epoch 73, loss = 0.06286114454269409
 74%|███████▍  | 74/100 [06:00<01:56,  4.47s/it]epoch 74, loss = 0.047979071736335754
 75%|███████▌  | 75/100 [06:08<02:17,  5.51s/it]Epoch 75: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9200 / 0.8948 / 0.9706 / 0.8750/ [0.83333333 0.98       0.875     ]
                Test acc/f1/spec/sens = 0.5336 / 0.4868 / 0.7992 / 0.6379/ [0.13375796 0.97701149 0.63793103]
epoch 75, loss = 0.04941381886601448
 76%|███████▌  | 76/100 [06:13<02:06,  5.27s/it] 77%|███████▋  | 77/100 [06:16<01:49,  4.75s/it]epoch 76, loss = 0.04639429971575737
epoch 77, loss = 0.07833198457956314
 78%|███████▊  | 78/100 [06:20<01:42,  4.65s/it] 79%|███████▉  | 79/100 [06:24<01:31,  4.34s/it]epoch 78, loss = 0.07194581627845764
epoch 79, loss = 0.0446050688624382
 80%|████████  | 80/100 [06:33<01:54,  5.72s/it]Epoch 80: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9000 / 0.8685 / 0.9559 / 0.8750/ [0.77777778 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5420 / 0.4784 / 0.8033 / 0.6810/ [0.08917197 0.98850575 0.68103448]
 81%|████████  | 81/100 [06:37<01:36,  5.09s/it]epoch 80, loss = 0.03993691876530647
epoch 81, loss = 0.0380527600646019
 82%|████████▏ | 82/100 [06:41<01:28,  4.90s/it] 83%|████████▎ | 83/100 [06:45<01:17,  4.56s/it]epoch 82, loss = 0.026472002267837524
epoch 83, loss = 0.0401218943297863
 84%|████████▍ | 84/100 [06:49<01:13,  4.57s/it]epoch 84, loss = 0.035593532025814056
 85%|████████▌ | 85/100 [06:58<01:24,  5.66s/it]Epoch 85: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9100 / 0.8842 / 0.9706 / 0.8750/ [0.83333333 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5168 / 0.4529 / 0.8238 / 0.6422/ [0.07006369 0.98850575 0.64224138]
epoch 85, loss = 0.03511318564414978
 86%|████████▌ | 86/100 [07:02<01:13,  5.28s/it] 87%|████████▋ | 87/100 [07:06<01:01,  4.77s/it]epoch 86, loss = 0.037894051522016525
epoch 87, loss = 0.049941301345825195
 88%|████████▊ | 88/100 [07:10<00:56,  4.67s/it] 89%|████████▉ | 89/100 [07:14<00:47,  4.31s/it]epoch 88, loss = 0.04435638338327408
epoch 89, loss = 0.05068408325314522
 90%|█████████ | 90/100 [07:22<00:56,  5.62s/it]Epoch 90: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9200 / 0.8922 / 0.9559 / 0.9062/ [0.77777778 0.98       0.90625   ]
                Test acc/f1/spec/sens = 0.5378 / 0.4685 / 0.8033 / 0.6853/ [0.07006369 0.98850575 0.68534483]
 91%|█████████ | 91/100 [07:26<00:45,  5.03s/it]epoch 90, loss = 0.03256413713097572
epoch 91, loss = 0.038316115736961365
 92%|█████████▏| 92/100 [07:30<00:39,  4.90s/it] 93%|█████████▎| 93/100 [07:34<00:31,  4.54s/it]epoch 92, loss = 0.03940891474485397
epoch 93, loss = 0.14518773555755615
 94%|█████████▍| 94/100 [07:39<00:27,  4.52s/it]epoch 94, loss = 0.03627144172787666
 95%|█████████▌| 95/100 [07:46<00:27,  5.49s/it]Epoch 95: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9100 / 0.8842 / 0.9706 / 0.8750/ [0.83333333 0.96       0.875     ]
                Test acc/f1/spec/sens = 0.5168 / 0.4524 / 0.8238 / 0.6422/ [0.07006369 0.98850575 0.64224138]
epoch 95, loss = 0.0448782742023468
 96%|█████████▌| 96/100 [07:51<00:20,  5.19s/it] 97%|█████████▋| 97/100 [07:55<00:14,  4.74s/it]epoch 96, loss = 0.06050094962120056
epoch 97, loss = 0.039594873785972595
 98%|█████████▊| 98/100 [07:59<00:09,  4.70s/it] 99%|█████████▉| 99/100 [08:03<00:04,  4.34s/it]epoch 98, loss = 0.058578602969646454
epoch 99, loss = 0.042194850742816925
100%|██████████| 100/100 [08:12<00:00,  5.76s/it]                                                 Epoch 100: Train acc/f1 = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.] 
                Val acc/f1/spec/sens = 0.9200 / 0.8948 / 0.9706 / 0.8750/ [0.83333333 0.98       0.875     ]
                Test acc/f1/spec/sens = 0.5399 / 0.4873 / 0.7992 / 0.6595/ [0.12101911 0.97701149 0.65948276]
Trial 1, Cycle 1
Train acc/f1/spec/sens = 1.0000 / 1.0000 / 1.0000 / 1.0000/ [1. 1. 1.]
Val acc/f1/spec/sens = 0.9200 / 0.8948 / 0.9706 / 0.8750/ [0.83333333 0.98       0.875     ]
Test acc/f1/spec/sens = 0.5399 / 0.4873 / 0.7992 / 0.6595/ [0.12101911 0.97701149 0.65948276]
>> Finished.
Acc, agreement for latest model:  0.5399159663865546 0.592436974789916 [0.12101911 0.97701149 0.65948276]
Load best checkpoint for thief model
Acc, agreement for best model:  0.4852941176470588 0.5378151260504201 [0.24203822 0.97701149 0.46551724]
Trial 0/1 || Cycle 1/1 || Label set size 900 || Test acc 0.4853 || Test agreement 0.5378 || Spec 0.8156 || Sens 0.4655 Class Acc: [0.24203822 0.97701149 0.46551724]
**************************************************************************************************** 

Number of samples  900
0.5378151260504201 0.0
        acc  ...                label dist
0  0.485294  ...  {0: 149, 1: 391, 2: 360}

[1 rows x 6 columns]
Results saved to  /home/ankita/scratch/data_msa_medical/results_ankita//pocus_resnet18/butterfly_resnet50/SGD/1000_val100/random_v3
